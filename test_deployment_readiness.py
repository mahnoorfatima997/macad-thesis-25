#!/usr/bin/env python3
"""
Comprehensive Deployment Readiness Test Suite
Tests all critical components of the ArchMentor application
"""

import os
import sys
import time
import json
import traceback
from typing import Dict, List, Any, Tuple
from dotenv import load_dotenv

# Add project root to path
sys.path.append('.')
# Add thesis-agents to path (handle hyphen in directory name)
sys.path.append('thesis-agents')

class DeploymentTester:
    def __init__(self):
        self.results = {
            'passed': 0,
            'failed': 0,
            'warnings': 0,
            'tests': []
        }
        self.critical_failures = []
        
    def log_test(self, test_name: str, status: str, message: str, is_critical: bool = False):
        """Log test result"""
        result = {
            'test': test_name,
            'status': status,
            'message': message,
            'critical': is_critical,
            'timestamp': time.strftime('%H:%M:%S')
        }
        self.results['tests'].append(result)
        
        if status == 'PASS':
            self.results['passed'] += 1
            print(f"‚úÖ {test_name}: {message}")
        elif status == 'FAIL':
            self.results['failed'] += 1
            print(f"‚ùå {test_name}: {message}")
            if is_critical:
                self.critical_failures.append(test_name)
        elif status == 'WARN':
            self.results['warnings'] += 1
            print(f"‚ö†Ô∏è  {test_name}: {message}")

    def test_environment_setup(self):
        """Test 1: Environment and Dependencies"""
        print("\nüîß TESTING ENVIRONMENT SETUP...")
        
        # Test .env file loading
        try:
            load_dotenv()
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key and len(api_key) > 50:
                self.log_test("Environment - API Key", "PASS", f"OpenAI API key loaded ({len(api_key)} chars)", True)
            else:
                self.log_test("Environment - API Key", "FAIL", "OpenAI API key missing or invalid", True)
        except Exception as e:
            self.log_test("Environment - API Key", "FAIL", f"Failed to load .env: {e}", True)
        
        # Test critical imports (fixed paths and class names)
        critical_imports = [
            ('streamlit', None, 'streamlit'),  # Just test streamlit module
            ('dashboard.ui.enhanced_gamification', 'EnhancedGamificationRenderer'),
            ('agents.domain_expert.adapter', 'DomainExpertAgent'),
            ('agents.cognitive_enhancement.processors.challenge_generator', 'ChallengeGeneratorProcessor'),
            ('dashboard.ui.chat_components', 'render_chat_interface'),
        ]

        for import_info in critical_imports:
            if len(import_info) == 3:  # Special case for streamlit
                module_path, _, display_name = import_info
                try:
                    __import__(module_path)
                    self.log_test(f"Import - {display_name}", "PASS", f"Successfully imported {display_name}")
                except Exception as e:
                    self.log_test(f"Import - {display_name}", "FAIL", f"Failed to import {display_name}: {e}", True)
            else:
                module_path, class_name = import_info
                try:
                    # Fix path separators for thesis-agents
                    fixed_path = module_path.replace('-', '_')
                    module = __import__(fixed_path, fromlist=[class_name])
                    getattr(module, class_name)
                    self.log_test(f"Import - {class_name}", "PASS", f"Successfully imported {class_name}")
                except Exception as e:
                    self.log_test(f"Import - {class_name}", "FAIL", f"Failed to import {class_name}: {e}", True)

    def test_gamification_system(self):
        """Test 2: Gamification System"""
        print("\nüéÆ TESTING GAMIFICATION SYSTEM...")
        
        try:
            from dashboard.ui.enhanced_gamification import EnhancedGamificationRenderer
            renderer = EnhancedGamificationRenderer()
            
            # Test transformation name generation (without API calls)
            try:
                # Just test that the method exists and is callable
                if hasattr(renderer.content_generator, 'generate_transformations_from_context'):
                    method = getattr(renderer.content_generator, 'generate_transformations_from_context')
                    if callable(method):
                        self.log_test("Gamification - Transformation Names", "PASS", "Transformation generation method available")
                    else:
                        self.log_test("Gamification - Transformation Names", "FAIL", "Transformation method not callable", True)
                else:
                    self.log_test("Gamification - Transformation Names", "FAIL", "Transformation generation method missing", True)

            except Exception as e:
                self.log_test("Gamification - Transformation Names", "FAIL", f"Transformation generation test failed: {e}", True)

            # Test other game types (method existence only)
            game_types = ['personas', 'constraints', 'perspectives', 'mystery', 'story_chapters', 'time_periods']
            for game_type in game_types:
                try:
                    method_name = f'generate_{game_type}_from_context'
                    if hasattr(renderer.content_generator, method_name):
                        method = getattr(renderer.content_generator, method_name)
                        if callable(method):
                            self.log_test(f"Gamification - {game_type.title()}", "PASS", f"Method {method_name} available")
                        else:
                            self.log_test(f"Gamification - {game_type.title()}", "WARN", f"Method {method_name} not callable")
                    else:
                        self.log_test(f"Gamification - {game_type.title()}", "WARN", f"Method {method_name} not found")
                except Exception as e:
                    self.log_test(f"Gamification - {game_type.title()}", "FAIL", f"Method test failed: {e}")
                    
        except Exception as e:
            self.log_test("Gamification - System", "FAIL", f"Gamification system failed to initialize: {e}", True)

    def test_frequency_control(self):
        """Test 3: Frequency Control Logic"""
        print("\n‚è∞ TESTING FREQUENCY CONTROL...")
        
        try:
            # Test frequency control logic by checking the code directly
            frequency_file = 'thesis-agents/agents/cognitive_enhancement/processors/challenge_generator.py'
            if os.path.exists(frequency_file):
                with open(frequency_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Check for frequency control implementation
                if 'total_user_messages % 3' in content:
                    self.log_test("Frequency - Logic", "PASS", "Frequency control logic found (every 3 messages)")
                elif 'total_user_messages %' in content:
                    self.log_test("Frequency - Logic", "WARN", "Frequency control found but not set to 3 messages")
                else:
                    self.log_test("Frequency - Logic", "FAIL", "No frequency control logic found", True)

                # Check if frequency control is enabled (not commented out)
                if 'return False' in content and 'total_user_messages %' in content:
                    self.log_test("Frequency - Enabled", "PASS", "Frequency control is enabled")
                else:
                    self.log_test("Frequency - Enabled", "WARN", "Frequency control may be disabled")
            else:
                self.log_test("Frequency - File", "FAIL", f"Challenge generator file not found: {frequency_file}", True)

        except Exception as e:
            self.log_test("Frequency - System", "FAIL", f"Frequency control test failed: {e}")

    def test_web_search_integration(self):
        """Test 4: Web Search Integration"""
        print("\nüåê TESTING WEB SEARCH INTEGRATION...")
        
        try:
            # Test web search detection logic (without imports)
            test_queries = [
                ("give example projects for community centers", True, "Should trigger web search"),
                ("example projects for adaptive reuse", True, "Should trigger web search"),
                ("what are design principles", False, "Should not trigger web search"),
                ("how to design circulation", False, "Should not trigger web search"),
            ]

            for query, should_search, description in test_queries:
                # Check if query contains project example patterns
                has_project_patterns = any(pattern in query.lower() for pattern in ['example projects', 'project examples'])

                if has_project_patterns == should_search:
                    self.log_test(f"Web Search - Query Detection", "PASS", f"'{query[:30]}...' - {description}")
                else:
                    self.log_test(f"Web Search - Query Detection", "FAIL", f"'{query[:30]}...' - Detection failed")

            # Test Tavily API availability (without making actual calls)
            tavily_key = os.getenv('TAVILY_API_KEY')
            if tavily_key:
                self.log_test("Web Search - API Key", "PASS", "Tavily API key available")
            else:
                self.log_test("Web Search - API Key", "WARN", "Tavily API key not found - web search may not work")

            # Check if web search logic exists in domain expert
            domain_expert_file = 'thesis-agents/agents/domain_expert/adapter.py'
            if os.path.exists(domain_expert_file):
                with open(domain_expert_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                if 'web_search' in content.lower() or 'tavily' in content.lower():
                    self.log_test("Web Search - Implementation", "PASS", "Web search logic found in domain expert")
                else:
                    self.log_test("Web Search - Implementation", "WARN", "Web search logic not found in domain expert")
            else:
                self.log_test("Web Search - Implementation", "FAIL", "Domain expert file not found")

        except Exception as e:
            self.log_test("Web Search - System", "FAIL", f"Web search system test failed: {e}")

    def test_ui_components(self):
        """Test 5: UI Components"""
        print("\nüé® TESTING UI COMPONENTS...")
        
        # Test critical UI imports
        ui_components = [
            ('dashboard.ui.chat_components', 'render_chat_interface'),
            ('dashboard.ui.chat_components', 'get_chat_input'),
            ('dashboard.ui.gamification_components', 'render_gamified_challenge'),
            ('dashboard.ui.enhanced_gamification', 'render_enhanced_gamified_challenge'),
        ]
        
        for module_path, component_name in ui_components:
            try:
                module = __import__(module_path, fromlist=[component_name])
                component = getattr(module, component_name)
                if callable(component):
                    self.log_test(f"UI - {component_name}", "PASS", f"Component {component_name} is callable")
                else:
                    self.log_test(f"UI - {component_name}", "FAIL", f"Component {component_name} is not callable")
            except Exception as e:
                self.log_test(f"UI - {component_name}", "FAIL", f"Failed to load {component_name}: {e}", True)
        
        # Test HTML structure (check for common issues)
        try:
            from dashboard.ui.enhanced_gamification import EnhancedGamificationRenderer
            renderer = EnhancedGamificationRenderer()
            
            # Test game description generation
            game_desc = "Design your community_center to adapt and transform for different uses, times, and seasons. Explore how spaces can change to serve multiple functions while maintaining their architectural integrity."
            
            # Check for HTML tags in description
            has_html_tags = any(tag in game_desc for tag in ['<div>', '</div>', '<span>', '</span>'])
            if not has_html_tags:
                self.log_test("UI - HTML Structure", "PASS", "Game descriptions are clean (no HTML tags)")
            else:
                self.log_test("UI - HTML Structure", "FAIL", "Game descriptions contain HTML tags", True)
                
        except Exception as e:
            self.log_test("UI - HTML Structure", "FAIL", f"HTML structure test failed: {e}")

    def test_database_integration(self):
        """Test 6: Database Integration"""
        print("\nüóÑÔ∏è TESTING DATABASE INTEGRATION...")
        
        try:
            # Test database files existence
            db_files = [
                'thesis-agents/data/knowledge_base',
                'thesis-agents/data',
                'data'
            ]

            db_found = False
            for db_path in db_files:
                if os.path.exists(db_path):
                    db_found = True
                    self.log_test("Database - Files", "PASS", f"Database directory found: {db_path}")
                    break

            if not db_found:
                self.log_test("Database - Files", "WARN", "No database directory found")

            # Test ChromaDB dependency
            try:
                import chromadb
                self.log_test("Database - ChromaDB", "PASS", "ChromaDB library available")
            except ImportError:
                self.log_test("Database - ChromaDB", "FAIL", "ChromaDB library not installed", True)

        except Exception as e:
            self.log_test("Database - System", "FAIL", f"Database system test failed: {e}")

    def test_agent_orchestration(self):
        """Test 7: Agent Orchestration"""
        print("\nü§ñ TESTING AGENT ORCHESTRATION...")
        
        try:
            # Test agent file existence
            agent_files = [
                ('thesis-agents/agents/domain_expert/adapter.py', 'DomainExpertAgent'),
                ('thesis-agents/agents/socratic_tutor/adapter.py', 'SocraticTutorAgent'),
                ('thesis-agents/agents/cognitive_enhancement/adapter.py', 'CognitiveEnhancementAgent'),
                ('thesis-agents/agents/analysis_agent/adapter.py', 'AnalysisAgent'),
            ]

            for file_path, agent_class in agent_files:
                if os.path.exists(file_path):
                    # Check if the class is defined in the file
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    if f'class {agent_class}' in content:
                        self.log_test(f"Agent - {agent_class}", "PASS", f"{agent_class} class found in {file_path}")
                    else:
                        self.log_test(f"Agent - {agent_class}", "WARN", f"{agent_class} class not found in {file_path}")
                else:
                    self.log_test(f"Agent - {agent_class}", "FAIL", f"Agent file not found: {file_path}", True)

            # Test orchestrator file
            orchestrator_file = 'orchestration/orchestrator/architecture.py'
            if os.path.exists(orchestrator_file):
                self.log_test("Agent - Orchestrator", "PASS", "Orchestrator file found")
            else:
                self.log_test("Agent - Orchestrator", "WARN", "Orchestrator file not found")

        except Exception as e:
            self.log_test("Agent - System", "FAIL", f"Agent system test failed: {e}")

    def test_performance_optimizations(self):
        """Test 8: Performance Optimizations"""
        print("\n‚ö° TESTING PERFORMANCE OPTIMIZATIONS...")
        
        # Test debug prints are disabled
        debug_files = [
            'dashboard/ui/chat_components.py',
            'dashboard/ui/enhanced_gamification.py',
            'thesis-agents/agents/cognitive_enhancement/processors/challenge_generator.py'
        ]
        
        for file_path in debug_files:
            try:
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Count active debug prints (not commented out)
                    lines = content.split('\n')
                    active_prints = 0
                    for line in lines:
                        stripped = line.strip()
                        if (stripped.startswith('print(') and 'DEBUG' in stripped and 
                            not stripped.startswith('#') and not stripped.startswith('//')):
                            active_prints += 1
                    
                    if active_prints == 0:
                        self.log_test(f"Performance - {os.path.basename(file_path)}", "PASS", "No active debug prints found")
                    else:
                        self.log_test(f"Performance - {os.path.basename(file_path)}", "WARN", f"Found {active_prints} active debug prints")
                else:
                    self.log_test(f"Performance - {os.path.basename(file_path)}", "WARN", f"File not found: {file_path}")
            except Exception as e:
                self.log_test(f"Performance - {os.path.basename(file_path)}", "FAIL", f"Failed to check file: {e}")

    def run_all_tests(self):
        """Run all deployment readiness tests"""
        print("üöÄ STARTING COMPREHENSIVE DEPLOYMENT READINESS TEST")
        print("=" * 60)
        
        start_time = time.time()
        
        # Run all test suites
        self.test_environment_setup()
        self.test_gamification_system()
        self.test_frequency_control()
        self.test_web_search_integration()
        self.test_ui_components()
        self.test_database_integration()
        self.test_agent_orchestration()
        self.test_performance_optimizations()
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Generate final report
        self.generate_final_report(duration)
        
    def generate_final_report(self, duration: float):
        """Generate final deployment readiness report"""
        print("\n" + "=" * 60)
        print("üìä DEPLOYMENT READINESS REPORT")
        print("=" * 60)
        
        total_tests = self.results['passed'] + self.results['failed'] + self.results['warnings']
        
        print(f"‚è±Ô∏è  Test Duration: {duration:.2f} seconds")
        print(f"üìà Total Tests: {total_tests}")
        print(f"‚úÖ Passed: {self.results['passed']}")
        print(f"‚ùå Failed: {self.results['failed']}")
        print(f"‚ö†Ô∏è  Warnings: {self.results['warnings']}")
        
        # Calculate readiness score
        if total_tests > 0:
            readiness_score = (self.results['passed'] / total_tests) * 100
            print(f"üéØ Readiness Score: {readiness_score:.1f}%")
        else:
            readiness_score = 0
            print("üéØ Readiness Score: 0% (No tests completed)")
        
        # Deployment recommendation
        print("\nüöÄ DEPLOYMENT RECOMMENDATION:")
        if len(self.critical_failures) == 0 and readiness_score >= 80:
            print("‚úÖ READY FOR DEPLOYMENT")
            print("   All critical systems are functional.")
        elif len(self.critical_failures) == 0 and readiness_score >= 60:
            print("‚ö†Ô∏è  DEPLOYMENT WITH CAUTION")
            print("   Core systems work but some issues need attention.")
        else:
            print("‚ùå NOT READY FOR DEPLOYMENT")
            print("   Critical issues must be resolved first.")
            if self.critical_failures:
                print(f"   Critical failures: {', '.join(self.critical_failures)}")
        
        # Save detailed report
        self.save_detailed_report()
        
    def save_detailed_report(self):
        """Save detailed test report to file"""
        try:
            report_data = {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'summary': {
                    'passed': self.results['passed'],
                    'failed': self.results['failed'],
                    'warnings': self.results['warnings'],
                    'critical_failures': self.critical_failures
                },
                'tests': self.results['tests']
            }
            
            with open('deployment_test_report.json', 'w') as f:
                json.dump(report_data, f, indent=2)
            
            print(f"\nüìÑ Detailed report saved to: deployment_test_report.json")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to save detailed report: {e}")

if __name__ == "__main__":
    tester = DeploymentTester()
    tester.run_all_tests()
