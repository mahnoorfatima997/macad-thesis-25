# MEGA Architectural Mentor App - Complete Functionality Overview

## ğŸ—ï¸ App Purpose and Logic

The MEGA (Multi-agent Enhanced Generative Architecture) system is a research platform designed to study how different AI interaction modes affect architectural design learning. The app guides users through three phases of architectural design while comparing three distinct AI interaction approaches.

### Core Research Question
**How do different levels of AI scaffolding affect design learning outcomes in architectural education?**

The app tests three conditions:
1. **Mentor Mode**: Multi-agent socratic scaffolding system
2. **Raw GPT Mode**: Direct ChatGPT-style interaction
3. **No AI Mode**: Hardcoded questions only (control group)

## ğŸ”„ Three-Phase Design Process

### Phase Structure
All users progress through the same three phases regardless of AI mode:

1. **ğŸ§  Ideation Phase** (25% weight)
   - Conceptual thinking and program development
   - Site analysis and community understanding
   - Initial design concepts and parti diagrams

2. **ğŸ‘ï¸ Visualization Phase** (35% weight)
   - Spatial organization and form development
   - Circulation and experience design
   - Visual representation and sketching

3. **ğŸ”¨ Materialization Phase** (40% weight)
   - Material selection and construction details
   - Technical feasibility and building systems
   - Final design refinement

## ğŸ“Š Phase Calculation System

### How Phases Are Determined

The system uses a sophisticated algorithm that analyzes conversation content to automatically detect the current design phase:

#### Calculation Method
```
Phase Detection = f(message_count, keyword_analysis, conversation_depth)
```

#### Phase Thresholds
- **Minimum Messages for Visualization**: 6 user messages
- **Minimum Messages for Materialization**: 10 user messages
- **Keyword Threshold for Visualization**: 3+ spatial keywords
- **Keyword Threshold for Materialization**: 3+ technical keywords

#### Example Phase Calculation
```
ğŸ”„ PHASE_CALCULATION: Analyzing 8 messages for session_abc123
   ğŸ“Š Keyword counts: ideation=2, visualization=4, materialization=1
   ğŸ¯ Phase determination: VISUALIZATION (6+ messages, 4 viz keywords)
   ğŸ“ˆ Progression: 45.2% (based on message count and keyword density)
   ğŸ” Confidence: 78% (strong keyword signals)
```

### Phase Progression Formula
```
Progression = min(1.0, current_messages / phase_threshold_messages)

For Ideation: progression = messages / 6
For Visualization: progression = (messages - 6) / 4  
For Materialization: progression = (messages - 10) / ongoing
```

### Terminal Debug Output Examples
When running the app, you'll see detailed phase analysis in the terminal:

```
ğŸ¦ PHASE_SYSTEM: Dynamic question bank initialized
   ğŸ¤– LLM-powered question generation enabled
   
ğŸ”„ PHASE_CALCULATION: Session unified_session_20250119_143022
   ğŸ“ Total messages: 12
   ğŸ” Analyzing keywords...
   ğŸ“Š Ideation keywords: ['concept', 'program'] (count: 2)
   ğŸ“Š Visualization keywords: ['space', 'form', 'circulation', 'layout'] (count: 4)  
   ğŸ“Š Materialization keywords: ['material', 'construction'] (count: 3)
   
   ğŸ¯ Phase Logic:
   âœ… Messages >= 10 (materialization threshold)
   âœ… Material keywords >= 3
   âœ… Material count (3) > Visualization count (4)? No
   
   ğŸ“ RESULT: VISUALIZATION phase (67% complete)
   ğŸ” Confidence: 82% (strong keyword signals)
```

## ğŸ¤– Three AI Interaction Modes

### 1. Mentor Mode (Multi-Agent System)
**Purpose**: Sophisticated socratic scaffolding using multiple AI agents

#### Agent Architecture
```
User Input â†’ Analysis Agent â†’ Routing Decision â†’ Agent Orchestration
                â†“
    [Domain Expert] + [Socratic Tutor] + [Cognitive Enhancement]
                â†“
         Response Synthesis â†’ Final Output
```

#### Example Terminal Output
```
ğŸ¤– ORCHESTRATOR: Processing student input...
   ğŸ” Analysis Agent: Classifying interaction type
   ğŸ¯ Routing Decision: socratic_focus + cognitive_enhancement
   ğŸ§  Socratic Agent: Generating guiding questions
   ğŸ’¡ Cognitive Agent: Assessing learning state
   âš¡ Response Synthesis: Combining agent outputs
   
âœ… Multi-agent response generated (1.2s)
```

#### Example Response Style
```
User: "I think a glass facade would work well here."

Mentor Response: "Glass facades can indeed create interesting visual effects. 
What specific qualities are you hoping to achieve with transparency in this 
context? Consider how the interior activities might relate to the street 
life outside. What time of day will this space be most active, and how 
might that influence your material choice?"
```

### 2. Raw GPT Mode (Direct AI)
**Purpose**: Unfiltered ChatGPT-style interaction without scaffolding

#### Processing Method
```
User Input â†’ Direct GPT-4 API Call â†’ Unfiltered Response
```

#### Terminal Output
```
ğŸ”„ RAW_GPT: Processing unfiltered request
   ğŸ“ Input: "I think a glass facade would work well here."
   ğŸ¤– Model: gpt-4o (temperature: 0.7)
   âš¡ Response generated (0.8s)
   
âœ… Raw GPT response: no_conditioning=True, no_socratic_elements=True
```

#### Example Response Style
```
User: "I think a glass facade would work well here."

Raw GPT Response: "A glass facade is a great choice! Glass can provide 
excellent natural light, create visual connections between interior and 
exterior spaces, and give your building a modern, sleek appearance. You 
might want to consider energy efficiency with double or triple glazing, 
and think about privacy needs for different areas of the building."
```

### 3. No AI Mode (Control Group)
**Purpose**: Hardcoded questions only, simulating traditional learning

#### Processing Method
```
User Input â†’ Phase Detection â†’ Hardcoded Question Bank â†’ Next Question
```

#### Terminal Output
```
ğŸ”„ NO_AI: Control group processing
   ğŸ“Š Phase calculation: VISUALIZATION (45% complete)
   ğŸ“ Question bank: visualization_questions
   ğŸ¯ Next question: #3 of 8 visualization questions
   
âœ… Hardcoded response delivered
```

#### Example Response Style
```
User: "I think a glass facade would work well here."

No AI Response: "Thank you for sharing your thoughts. How do you envision 
the relationship between the interior spaces and the surrounding context? 
What specific views or connections do you want to emphasize or screen?"
```

## ğŸ® Gaming System Integration

### Gamification Triggers
The system detects specific behavioral patterns and responds with gamified elements:

#### Terminal Debug Output
```
ğŸ® DEBUG: Gamification analysis
   ğŸ” Analyzing user confidence level...
   âš ï¸ Overconfidence detected: "This design is perfect"
   ğŸ¯ Trigger: cognitive_challenge
   ğŸ¨ Display type: enhanced_visual
   
ğŸ® GAMIFICATION: Challenge activated
   ğŸ’ª Challenge type: "Design Critique Challenge"
   ğŸ¯ Goal: Consider alternative perspectives
   â­ Reward: Critical thinking badge
```

### Gaming Elements
- **Challenge Cards**: Pop up when overconfidence is detected
- **Progress Badges**: Awarded for phase completion
- **Reflection Prompts**: Triggered by specific conversation patterns
- **Peer Comparison**: Anonymous benchmarking against other users

## ğŸ“ˆ Data Collection and Analysis

### Metrics Tracked
The system continuously monitors and logs:

#### Interaction Metrics
```
ğŸ“Š LOGGING: Interaction recorded
   â±ï¸ Response time: 1.2s
   ğŸ“ Message length: 156 characters
   ğŸ¯ Phase: VISUALIZATION (67% complete)
   ğŸ” Engagement level: HIGH
   ğŸ’­ Design moves extracted: 3
```

#### Linkography Analysis
```
ğŸ”— LINKOGRAPHY: Design move analysis
   ğŸ“ Move #12: "Consider natural ventilation"
   ğŸ”— Links to: Move #8 (sustainability), Move #15 (climate)
   ğŸ“Š Link density: 0.73
   ğŸ¯ Critical move: YES (high connectivity)
```

### Export Functionality
```
ğŸ’¾ DATA_EXPORT: Session complete
   ğŸ“ Local save: /thesis_data/session_abc123.json
   â˜ï¸ Dropbox sync: /MEGA_Study/participant_data/
   ğŸ“Š Metrics: 47 interactions, 23 design moves, 3 phases
   
âœ… Data exported successfully
```

## ğŸ”§ Technical Architecture

### File Structure
```
mentor.py                    # Main entry point
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ unified_dashboard.py # Main dashboard controller
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ mode_processors.py    # Routes between AI modes
â”‚   â”‚   â”œâ”€â”€ phase_calculator.py   # Phase detection system
â”‚   â”‚   â”œâ”€â”€ raw_gpt_processor.py  # Raw GPT mode
â”‚   â”‚   â””â”€â”€ no_ai_processor.py    # No AI mode
â”‚   â””â”€â”€ ui/                       # UI components
â”œâ”€â”€ thesis-agents/               # Multi-agent system (Mentor mode)
â”‚   â”œâ”€â”€ orchestration/
â”‚   â”‚   â””â”€â”€ orchestrator.py     # Agent coordination
â”‚   â””â”€â”€ agents/                 # Individual AI agents
â””â”€â”€ thesis_tests/               # Research testing framework
    â”œâ”€â”€ test_dashboard.py       # Test environment
    â”œâ”€â”€ mentor_environment.py   # Mentor mode testing
    â”œâ”€â”€ generic_ai_environment.py # Raw GPT testing
    â””â”€â”€ control_environment.py  # No AI testing
```

### Session Management
```
ğŸ”„ SESSION: unified_session_20250119_143022
   ğŸ‘¤ Participant: anonymous_user_47
   ğŸ¯ Test group: MENTOR
   ğŸ“Š Current phase: VISUALIZATION (67%)
   â±ï¸ Session duration: 23 minutes
   ğŸ’¬ Total interactions: 15
```

## ğŸš€ Running the Application

### Launch Commands
```bash
# Main mentor interface
python mentor.py

# Research testing dashboard  
python launch_test_dashboard.py

# Benchmarking system
python benchmarking/run_benchmarking.py
```

### Expected Terminal Output on Startup
```
âœ… Fallback to dotenv successful
ğŸ—ï¸ MEGA Architectural Mentor System Starting...
ğŸ”§ Initializing components...
   âœ… API keys loaded
   âœ… Session state initialized  
   âœ… Phase calculator ready
   âœ… Multi-agent system loaded
   
ğŸŒ Dashboard available at: http://localhost:8501
ğŸ¯ Ready for architectural design mentoring!
```

This comprehensive system enables rigorous comparison of AI interaction modes while providing meaningful architectural design learning experiences across all three conditions.
