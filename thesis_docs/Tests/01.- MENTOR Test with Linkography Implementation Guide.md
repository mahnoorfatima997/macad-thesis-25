Linkography Implementation

The current session data structure doesn't contain the specific information needed for linkography analysis. Here's what's missing and what needs to be tracked:

  Current Issue

The linkography analyzer is trying to extract "design moves" from your session data, but the current interaction logging format doesn't capture the necessary details for proper linkography analysis.

  What Linkography Needs

  1\. Design Moves as Discrete Units  
    \- Currently: Our sessions log general interactions (user\_message \+ ai\_response)  
    \- Needed: Each design-related action/thought should be captured as a separate "move"  
    \- Example: "I want to add a window here" → "Let me consider the lighting" → "This affects the facade composition"  
  2\. Temporal Sequencing  
    \- Currently: Interactions have timestamps but aren't fine-grained enough  
    \- Needed: Precise timestamps for each design decision/action  
    \- The order and timing between moves is crucial for link formation  
  3\. Design Phase Identification  
    \- Currently: The analyzer tries to guess phases from keywords  
    \- Needed: Explicit tracking of which design phase the user is in:  
      \- Ideation: Conceptual exploration, brainstorming  
      \- Visualization: Sketching, form development  
      \- Materialization: Technical details, construction  
  4\. Move Type Classification  
    \- Currently: Not tracked  
    \- Needed: Each move should be classified as:  
      \- Analysis (examining existing conditions)  
      \- Synthesis (combining ideas)  
      \- Evaluation (judging solutions)  
      \- Transformation (modifying designs)  
      \- Reflection (thinking about process)  
  5\. Modality Information  
    \- Currently: Only text interactions  
    \- Needed: Track whether the move came from:  
       \- Text input  
      \- Sketch/drawing action  
      \- Image upload/annotation  
      \- Voice command

  What Should Be Logged

  Our interaction logger should capture:

  {  
      "design\_move": {  
          "content": "The actual design action/decision",  
          "timestamp": precise\_timestamp,  
          "phase": "ideation|visualization|materialization",  
          "move\_type": "analysis|synthesis|evaluation|transformation|reflection",  
          "modality": "text|sketch|image|voice",  
          "context": {  
              "previous\_move": "what came before",  
              "tool\_used": "which feature was active",  
              "cognitive\_load": estimated\_value  
          }  
      }  
  }

  Integration Points

  For linkography to work with our MEGA system:

  1\. When using GPT-4 Vision \+ SAM: Each identified architectural element or design decision should be logged as a move  
  2\. During Multi-Agent conversations: Break down responses into discrete design-relevant statements  
  3\. In Socratic dialogue: Each question-answer pair might contain multiple moves  
  4\. Visual annotations: Each markup or selection is a potential move

  Example Session Structure for Linkography

  Instead of:  
  User: "I want to design a sustainable building"  
  AI: "Let's explore sustainable features..."

  We need:  
  Move 1: "Identify sustainability as design goal" (synthesis, ideation)  
  Move 2: "Consider solar orientation" (analysis, ideation)  
  Move 3: "Sketch south-facing facade" (transformation, visualization)  
  Move 4: "Evaluate daylight penetration" (evaluation, visualization)

The linkography engine then analyzes these moves for semantic similarity and creates links between related concepts, revealing the design thinking process.

# **Claude Code Implementation Guide for MENTOR B-Tests**

## **Overview for Claude Code Integration**

This guide provides specific instructions for implementing the MENTOR B-Tests using Claude Code in Visual Studio Code. The implementation focuses on creating a robust, scalable testing platform that integrates seamlessly with your cognitive benchmarking system.

---

## **Project Initialization Commands**

### **1\. Initial Setup**

\# Initialize project structure  
mkdir mentor-btests-platform  
cd mentor-btests-platform

\# Create Python virtual environment  
python \-m venv venv  
source venv/bin/activate  \# On Windows: venv\\Scripts\\activate

\# Initialize git repository  
git init  
echo "\# MENTOR B-Tests Platform" \> README.md  
git add README.md  
git commit \-m "Initial commit"

\# Create core directory structure  
mkdir \-p {src/{api,ui,benchmarking,assessment,data\_processing,utils},tests,data/{raw,processed,exports},docs,config,scripts}

### **2\. Dependencies Installation**

\# Core dependencies  
pip install fastapi uvicorn sqlalchemy psycopg2-binary redis pandas numpy scipy scikit-learn  
pip install anthropic openai torch transformers sentence-transformers  
pip install streamlit plotly seaborn matplotlib opencv-python Pillow  
pip install pytest pytest-asyncio black flake8 mypy  
pip install python-dotenv pydantic alembic asyncpg

\# Create requirements.txt  
pip freeze \> requirements.txt

---

## **Core Implementation Files**

### **1\. Main Application Entry Point**

**File: `src/main.py`**

"""  
MENTOR B-Tests Platform \- Main Application Entry Point  
Integrates cognitive benchmarking with multi-phase testing  
"""

from fastapi import FastAPI, HTTPException, Depends, WebSocket  
from fastapi.middleware.cors import CORSMiddleware  
from fastapi.staticfiles import StaticFiles  
import asyncio  
import logging  
from typing import List, Dict, Any  
import uuid  
from datetime import datetime

from .api.routes import router as api\_router  
from .benchmarking.cognitive\_metrics import CognitiveMetricsEngine  
from .data\_processing.session\_manager import SessionManager  
from .utils.database import get\_db\_session  
from .utils.config import settings

\# Configure logging  
logging.basicConfig(level=logging.INFO)  
logger \= logging.getLogger(\_\_name\_\_)

app \= FastAPI(  
    title="MENTOR B-Tests Platform",  
    description="Cognitive Benchmarking for Multimodal AI Mentor",  
    version="1.0.0"  
)

\# CORS middleware  
app.add\_middleware(  
    CORSMiddleware,  
    allow\_origins=\["http://localhost:3000", "http://localhost:8501"\],  
    allow\_credentials=True,  
    allow\_methods=\["\*"\],  
    allow\_headers=\["\*"\],  
)

\# Static files  
app.mount("/static", StaticFiles(directory="static"), name="static")

\# Include API routes  
app.include\_router(api\_router, prefix="/api/v1")

\# Global instances  
cognitive\_engine \= CognitiveMetricsEngine()  
session\_manager \= SessionManager()

@app.on\_event("startup")  
async def startup\_event():  
    """Initialize application components"""  
    logger.info("Starting MENTOR B-Tests Platform")  
    await cognitive\_engine.initialize()  
    await session\_manager.initialize()  
    logger.info("Platform initialization complete")

@app.on\_event("shutdown")  
async def shutdown\_event():  
    """Cleanup on shutdown"""  
    logger.info("Shutting down MENTOR B-Tests Platform")  
    await cognitive\_engine.cleanup()  
    await session\_manager.cleanup()

@app.websocket("/ws/{session\_id}")  
async def websocket\_endpoint(websocket: WebSocket, session\_id: str):  
    """WebSocket endpoint for real-time interaction tracking"""  
    await websocket.accept()  
      
    try:  
        while True:  
            data \= await websocket.receive\_json()  
              
            \# Process interaction through cognitive engine  
            result \= await cognitive\_engine.process\_interaction(  
                session\_id=session\_id,  
                interaction\_data=data  
            )  
              
            \# Send real-time metrics back  
            await websocket.send\_json({  
                "type": "metrics\_update",  
                "data": result  
            })  
              
    except Exception as e:  
        logger.error(f"WebSocket error: {e}")  
    finally:  
        await websocket.close()

if \_\_name\_\_ \== "\_\_main\_\_":  
    import uvicorn  
    uvicorn.run(app, host="0.0.0.0", port=8000, log\_level="info")

### **2\. Enhanced Session Manager with Linkography Integration**

**File: `src/data_processing/session_manager.py`**

"""  
Session Manager \- Enhanced with Linkography data capture  
Handles design move extraction and linkographic analysis  
"""

import asyncio  
import uuid  
from datetime import datetime, timedelta  
from typing import Dict, List, Optional, Any  
import json  
import logging  
from dataclasses import dataclass, asdict  
from sqlalchemy.ext.asyncio import AsyncSession  
from sqlalchemy import select, insert, update  
import re  
import spacy  
from sentence\_transformers import SentenceTransformer

from ..utils.database import get\_db\_session  
from ..utils.models import Session, Interaction, Assessment, DesignOutput, DesignMove, LinkographyLink

logger \= logging.getLogger(\_\_name\_\_)

@dataclass  
class DesignMoveData:  
    move\_id: str  
    session\_id: str  
    timestamp: datetime  
    sequence\_number: int  
    content: str  
    move\_type: str  \# analysis|synthesis|evaluation|transformation|reflection  
    phase: str     \# ideation|visualization|materialization  
    modality: str  \# text|sketch|image|voice|upload  
    cognitive\_operation: str  \# proposal|clarification|assessment|support|reference  
    design\_focus: str  \# function|form|structure|material|environment|culture  
    context: Dict\[str, Any\]  
    linkography\_metadata: Dict\[str, Any\]

class DesignMoveExtractor:  
    """Extracts discrete design moves from user interactions and AI responses"""  
      
    def \_\_init\_\_(self):  
        self.nlp \= spacy.load("en\_core\_web\_sm")  
        self.sentence\_transformer \= SentenceTransformer('all-MiniLM-L6-v2')  
          
        \# Move type classification patterns  
        self.move\_patterns \= {  
            'analysis': \[  
                r'analyze', r'examine', r'consider', r'look at', r'study',  
                r'what is', r'how does', r'why does', r'identify', r'recognize'  
            \],  
            'synthesis': \[  
                r'combine', r'integrate', r'merge', r'connect', r'link',  
                r'bring together', r'synthesize', r'unify', r'blend'  
            \],  
            'evaluation': \[  
                r'assess', r'evaluate', r'judge', r'rate', r'compare',  
                r'better', r'worse', r'good', r'bad', r'effective', r'ineffective'  
            \],  
            'transformation': \[  
                r'change', r'modify', r'alter', r'transform', r'adapt',  
                r'adjust', r'revise', r'update', r'improve', r'enhance'  
            \],  
            'reflection': \[  
                r'think about', r'reflect', r'consider', r'ponder',  
                r'looking back', r'in retrospect', r'realize', r'understand'  
            \]  
        }  
          
        \# Design focus classification  
        self.design\_focus\_patterns \= {  
            'function': \[  
                r'program', r'use', r'activity', r'purpose', r'function',  
                r'community needs', r'accessibility', r'circulation'  
            \],  
            'form': \[  
                r'shape', r'geometry', r'proportion', r'scale', r'composition',  
                r'facade', r'elevation', r'massing', r'volume'  
            \],  
            'structure': \[  
                r'structural', r'frame', r'support', r'column', r'beam',  
                r'foundation', r'load', r'engineering', r'construction'  
            \],  
            'material': \[  
                r'material', r'concrete', r'steel', r'wood', r'glass',  
                r'brick', r'finish', r'texture', r'surface'  
            \],  
            'environment': \[  
                r'lighting', r'ventilation', r'climate', r'energy', r'sustainable',  
                r'solar', r'thermal', r'acoustic', r'environmental'  
            \],  
            'culture': \[  
                r'community', r'cultural', r'social', r'identity', r'tradition',  
                r'neighborhood', r'inclusive', r'diversity', r'heritage'  
            \]  
        }  
      
    async def extract\_moves\_from\_interaction(self,   
                                          user\_input: str,   
                                          ai\_response: str,  
                                          session\_id: str,  
                                          phase: str,  
                                          interaction\_timestamp: datetime) \-\> List\[DesignMoveData\]:  
        """Extract discrete design moves from an interaction"""  
          
        moves \= \[\]  
        move\_counter \= 0  
          
        \# Extract moves from user input  
        user\_moves \= await self.\_parse\_text\_into\_moves(  
            user\_input, 'user', session\_id, phase, interaction\_timestamp, move\_counter  
        )  
        moves.extend(user\_moves)  
        move\_counter \+= len(user\_moves)  
          
        \# Extract moves from AI response  
        ai\_moves \= await self.\_parse\_text\_into\_moves(  
            ai\_response, 'ai', session\_id, phase, interaction\_timestamp, move\_counter  
        )  
        moves.extend(ai\_moves)  
          
        return moves  
      
    async def \_parse\_text\_into\_moves(self,   
                                   text: str,   
                                   source: str,  
                                   session\_id: str,  
                                   phase: str,  
                                   base\_timestamp: datetime,  
                                   start\_counter: int) \-\> List\[DesignMoveData\]:  
        """Parse text into discrete design moves"""  
          
        moves \= \[\]  
          
        \# Split into sentences for initial parsing  
        doc \= self.nlp(text)  
        sentences \= \[sent.text.strip() for sent in doc.sents if len(sent.text.strip()) \> 10\]  
          
        for i, sentence in enumerate(sentences):  
            \# Check if sentence contains design-relevant content  
            if not await self.\_is\_design\_relevant(sentence):  
                continue  
              
            move\_id \= str(uuid.uuid4())  
            timestamp \= base\_timestamp \+ timedelta(microseconds=i\*1000)  \# Microsecond precision  
              
            move\_data \= DesignMoveData(  
                move\_id=move\_id,  
                session\_id=session\_id,  
                timestamp=timestamp,  
                sequence\_number=start\_counter \+ i,  
                content=sentence,  
                move\_type=await self.\_classify\_move\_type(sentence),  
                phase=phase,  
                modality='text',  
                cognitive\_operation=await self.\_classify\_cognitive\_operation(sentence),  
                design\_focus=await self.\_classify\_design\_focus(sentence),  
                context={  
                    'source': source,  
                    'sentence\_index': i,  
                    'total\_sentences': len(sentences),  
                    'preceding\_context': sentences\[max(0, i-1)\] if i \> 0 else '',  
                    'following\_context': sentences\[min(len(sentences)-1, i+1)\] if i \< len(sentences)-1 else ''  
                },  
                linkography\_metadata={  
                    'semantic\_embedding': self.sentence\_transformer.encode(sentence).tolist(),  
                    'word\_count': len(sentence.split()),  
                    'complexity\_score': await self.\_calculate\_linguistic\_complexity(sentence),  
                    'design\_entity\_mentions': await self.\_extract\_design\_entities(sentence)  
                }  
            )  
              
            moves.append(move\_data)  
          
        return moves  
      
    async def \_is\_design\_relevant(self, sentence: str) \-\> bool:  
        """Check if sentence contains design-relevant content"""  
          
        design\_keywords \= \[  
            'design', 'space', 'building', 'architecture', 'structure',  
            'material', 'light', 'form', 'function', 'community', 'user',  
            'program', 'circulation', 'facade', 'interior', 'exterior'  
        \]  
          
        sentence\_lower \= sentence.lower()  
        return any(keyword in sentence\_lower for keyword in design\_keywords)  
      
    async def \_classify\_move\_type(self, sentence: str) \-\> str:  
        """Classify the type of design move"""  
          
        sentence\_lower \= sentence.lower()  
          
        for move\_type, patterns in self.move\_patterns.items():  
            if any(re.search(pattern, sentence\_lower) for pattern in patterns):  
                return move\_type  
          
        return 'synthesis'  \# Default to synthesis if unclear  
      
    async def \_classify\_cognitive\_operation(self, sentence: str) \-\> str:  
        """Classify cognitive operation type"""  
          
        sentence\_lower \= sentence.lower()  
          
        if any(word in sentence\_lower for word in \['propose', 'suggest', 'idea', 'concept'\]):  
            return 'proposal'  
        elif any(word in sentence\_lower for word in \['clarify', 'explain', 'define', 'specify'\]):  
            return 'clarification'  
        elif any(word in sentence\_lower for word in \['assess', 'evaluate', 'judge', 'rate'\]):  
            return 'assessment'  
        elif any(word in sentence\_lower for word in \['support', 'agree', 'confirm', 'validate'\]):  
            return 'support'  
        elif any(word in sentence\_lower for word in \['refer', 'mention', 'cite', 'example'\]):  
            return 'reference'  
        else:  
            return 'proposal'  
      
    async def \_classify\_design\_focus(self, sentence: str) \-\> str:  
        """Classify the design focus area"""  
          
        sentence\_lower \= sentence.lower()  
          
        for focus, patterns in self.design\_focus\_patterns.items():  
            if any(re.search(pattern, sentence\_lower) for pattern in patterns):  
                return focus  
          
        return 'function'  \# Default to function if unclear  
      
    async def \_calculate\_linguistic\_complexity(self, sentence: str) \-\> float:  
        """Calculate linguistic complexity score"""  
          
        doc \= self.nlp(sentence)  
          
        \# Factors contributing to complexity  
        word\_count \= len(\[token for token in doc if not token.is\_punct\])  
        avg\_word\_length \= sum(len(token.text) for token in doc if not token.is\_punct) / max(word\_count, 1\)  
        dependency\_depth \= max(\[token.depth for token in doc\], default=0)  
        technical\_terms \= sum(1 for token in doc if token.pos\_ in \['NOUN', 'ADJ'\] and len(token.text) \> 6\)  
          
        complexity \= (  
            (word\_count / 20\) \* 0.3 \+  
            (avg\_word\_length / 10\) \* 0.2 \+  
            (dependency\_depth / 5\) \* 0.3 \+  
            (technical\_terms / word\_count) \* 0.2  
        )  
          
        return min(1.0, complexity)  
      
    async def \_extract\_design\_entities(self, sentence: str) \-\> List\[str\]:  
        """Extract design-related entities from sentence"""  
          
        doc \= self.nlp(sentence)  
        entities \= \[\]  
          
        \# Extract architectural terms  
        architectural\_terms \= \[  
            'window', 'door', 'wall', 'roof', 'floor', 'ceiling',  
            'entrance', 'lobby', 'corridor', 'room', 'space', 'hall',  
            'facade', 'elevation', 'section', 'plan', 'site'  
        \]  
          
        for token in doc:  
            if token.lemma\_.lower() in architectural\_terms:  
                entities.append(token.text)  
          
        \# Extract named entities  
        for ent in doc.ents:  
            if ent.label\_ in \['ORG', 'GPE', 'PRODUCT'\]:  \# Organizations, places, products  
                entities.append(ent.text)  
          
        return list(set(entities))  \# Remove duplicates

class LinkographyAnalyzer:  
    """Analyzes design moves to create linkographic connections"""  
      
    def \_\_init\_\_(self):  
        self.sentence\_transformer \= SentenceTransformer('all-MiniLM-L6-v2')  
      
    async def create\_linkographic\_connections(self, moves: List\[DesignMoveData\]) \-\> List\[Dict\[str, Any\]\]:  
        """Create linkographic connections between design moves"""  
          
        connections \= \[\]  
          
        for i, move\_a in enumerate(moves):  
            for j, move\_b in enumerate(moves\[i+1:\], i+1):  
                  
                \# Calculate semantic similarity  
                semantic\_similarity \= await self.\_calculate\_semantic\_similarity(move\_a, move\_b)  
                  
                \# Calculate temporal proximity  
                temporal\_proximity \= await self.\_calculate\_temporal\_proximity(move\_a, move\_b)  
                  
                \# Calculate design focus overlap  
                focus\_overlap \= await self.\_calculate\_focus\_overlap(move\_a, move\_b)  
                  
                \# Calculate overall link strength  
                link\_strength \= (  
                    semantic\_similarity \* 0.5 \+  
                    temporal\_proximity \* 0.2 \+  
                    focus\_overlap \* 0.3  
                )  
                  
                \# Create link if strength exceeds threshold  
                if link\_strength \> 0.3:  
                    connection \= {  
                        'source\_move\_id': move\_a.move\_id,  
                        'target\_move\_id': move\_b.move\_id,  
                        'link\_strength': link\_strength,  
                        'link\_type': await self.\_classify\_link\_type(move\_a, move\_b),  
                        'semantic\_similarity': semantic\_similarity,  
                        'temporal\_distance': abs((move\_b.timestamp \- move\_a.timestamp).total\_seconds()),  
                        'phase\_transition': move\_a.phase \!= move\_b.phase,  
                        'move\_type\_transition': move\_a.move\_type \!= move\_b.move\_type  
                    }  
                    connections.append(connection)  
          
        return connections  
      
    async def \_calculate\_semantic\_similarity(self, move\_a: DesignMoveData, move\_b: DesignMoveData) \-\> float:  
        """Calculate semantic similarity between two moves"""  
          
        embedding\_a \= move\_a.linkography\_metadata\['semantic\_embedding'\]  
        embedding\_b \= move\_b.linkography\_metadata\['semantic\_embedding'\]  
          
        \# Cosine similarity  
        import numpy as np  
        similarity \= np.dot(embedding\_a, embedding\_b) / (  
            np.linalg.norm(embedding\_a) \* np.linalg.norm(embedding\_b)  
        )  
          
        return max(0, similarity)  \# Ensure non-negative  
      
    async def \_calculate\_temporal\_proximity(self, move\_a: DesignMoveData, move\_b: DesignMoveData) \-\> float:  
        """Calculate temporal proximity score"""  
          
        time\_diff \= abs((move\_b.timestamp \- move\_a.timestamp).total\_seconds())  
          
        \# Decay function \- closer moves have higher proximity  
        proximity \= 1 / (1 \+ time\_diff / 60\)  \# Normalize by minute  
          
        return proximity  
      
    async def \_calculate\_focus\_overlap(self, move\_a: DesignMoveData, move\_b: DesignMoveData) \-\> float:  
        """Calculate design focus overlap"""  
          
        if move\_a.design\_focus \== move\_b.design\_focus:  
            return 1.0  
          
        \# Related focus areas  
        focus\_relationships \= {  
            'function': \['form', 'structure'\],  
            'form': \['function', 'material'\],  
            'structure': \['function', 'material'\],  
            'material': \['form', 'structure', 'environment'\],  
            'environment': \['material', 'culture'\],  
            'culture': \['function', 'environment'\]  
        }  
          
        if move\_b.design\_focus in focus\_relationships.get(move\_a.design\_focus, \[\]):  
            return 0.5  
          
        return 0.0  
      
    async def \_classify\_link\_type(self, move\_a: DesignMoveData, move\_b: DesignMoveData) \-\> str:  
        """Classify the type of link between moves"""  
          
        if move\_a.phase \!= move\_b.phase:  
            return 'phase\_transition'  
        elif move\_a.design\_focus \== move\_b.design\_focus:  
            return 'focus\_development'  
        elif move\_a.move\_type \== 'analysis' and move\_b.move\_type \== 'synthesis':  
            return 'analysis\_synthesis'  
        elif move\_a.move\_type \== 'synthesis' and move\_b.move\_type \== 'evaluation':  
            return 'synthesis\_evaluation'  
        else:  
            return 'conceptual'

class EnhancedSessionManager(SessionManager):  
    """Session manager enhanced with linkography capabilities"""  
      
    def \_\_init\_\_(self):  
        super().\_\_init\_\_()  
        self.move\_extractor \= DesignMoveExtractor()  
        self.linkography\_analyzer \= LinkographyAnalyzer()  
      
    async def process\_interaction\_with\_linkography(self,   
                                                 session\_id: str,  
                                                 user\_input: str,  
                                                 ai\_response: str,  
                                                 phase: str,  
                                                 interaction\_metadata: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Process interaction with full linkography analysis"""  
          
        interaction\_timestamp \= datetime.now()  
          
        \# Extract design moves  
        moves \= await self.move\_extractor.extract\_moves\_from\_interaction(  
            user\_input, ai\_response, session\_id, phase, interaction\_timestamp  
        )  
          
        \# Save design moves to database  
        for move in moves:  
            await self.\_save\_design\_move(move)  
          
        \# Get all session moves for linkography analysis  
        all\_session\_moves \= await self.\_get\_session\_moves(session\_id)  
          
        \# Create linkographic connections  
        connections \= await self.linkography\_analyzer.create\_linkographic\_connections(all\_session\_moves)  
          
        \# Save linkographic connections  
        for connection in connections:  
            await self.\_save\_linkography\_connection(connection)  
          
        \# Calculate linkography metrics  
        linkography\_metrics \= await self.\_calculate\_linkography\_metrics(session\_id)  
          
        \# Process through original interaction logging  
        standard\_result \= await self.save\_interaction(session\_id, {  
            'user\_input': user\_input,  
            'ai\_response': ai\_response,  
            'type': 'dialogue',  
            'phase': phase,  
            'metadata': {  
                \*\*interaction\_metadata,  
                'design\_moves\_count': len(moves),  
                'new\_connections\_count': len(connections),  
                'linkography\_metrics': linkography\_metrics  
            }  
        })  
          
        return {  
            'interaction\_id': standard\_result,  
            'design\_moves': \[asdict(move) for move in moves\],  
            'linkographic\_connections': connections,  
            'linkography\_metrics': linkography\_metrics,  
            'move\_analysis': await self.\_analyze\_move\_patterns(moves)  
        }  
      
    async def \_save\_design\_move(self, move\_data: DesignMoveData):  
        """Save design move to database"""  
          
        async with get\_db\_session() as db:  
            design\_move \= DesignMove(  
                id=move\_data.move\_id,  
                session\_id=move\_data.session\_id,  
                timestamp=move\_data.timestamp,  
                sequence\_number=move\_data.sequence\_number,  
                content=move\_data.content,  
                move\_type=move\_data.move\_type,  
                phase=move\_data.phase,  
                modality=move\_data.modality,  
                cognitive\_operation=move\_data.cognitive\_operation,  
                design\_focus=move\_data.design\_focus,  
                context=move\_data.context,  
                linkography\_metadata=move\_data.linkography\_metadata  
            )  
              
            db.add(design\_move)  
            await db.commit()  
      
    async def \_save\_linkography\_connection(self, connection: Dict\[str, Any\]):  
        """Save linkographic connection to database"""  
          
        async with get\_db\_session() as db:  
            link \= LinkographyLink(  
                id=str(uuid.uuid4()),  
                source\_move\_id=connection\['source\_move\_id'\],  
                target\_move\_id=connection\['target\_move\_id'\],  
                link\_strength=connection\['link\_strength'\],  
                link\_type=connection\['link\_type'\],  
                semantic\_similarity=connection\['semantic\_similarity'\],  
                temporal\_distance=connection\['temporal\_distance'\],  
                phase\_transition=connection\['phase\_transition'\],  
                move\_type\_transition=connection\['move\_type\_transition'\]  
            )  
              
            db.add(link)  
            await db.commit()  
      
    async def \_get\_session\_moves(self, session\_id: str) \-\> List\[DesignMoveData\]:  
        """Retrieve all design moves for a session"""  
          
        async with get\_db\_session() as db:  
            result \= await db.execute(  
                select(DesignMove).where(DesignMove.session\_id \== session\_id)  
                .order\_by(DesignMove.sequence\_number)  
            )  
            moves \= result.scalars().all()  
          
        return \[  
            DesignMoveData(  
                move\_id=move.id,  
                session\_id=move.session\_id,  
                timestamp=move.timestamp,  
                sequence\_number=move.sequence\_number,  
                content=move.content,  
                move\_type=move.move\_type,  
                phase=move.phase,  
                modality=move.modality,  
                cognitive\_operation=move.cognitive\_operation,  
                design\_focus=move.design\_focus,  
                context=move.context,  
                linkography\_metadata=move.linkography\_metadata  
            )  
            for move in moves  
        \]  
      
    async def \_calculate\_linkography\_metrics(self, session\_id: str) \-\> Dict\[str, Any\]:  
        """Calculate linkography metrics for session"""  
          
        async with get\_db\_session() as db:  
            \# Get moves count  
            moves\_result \= await db.execute(  
                select(DesignMove).where(DesignMove.session\_id \== session\_id)  
            )  
            moves \= moves\_result.scalars().all()  
            move\_count \= len(moves)  
              
            \# Get links count  
            links\_result \= await db.execute(  
                select(LinkographyLink).join(DesignMove, LinkographyLink.source\_move\_id \== DesignMove.id)  
                .where(DesignMove.session\_id \== session\_id)  
            )  
            links \= links\_result.scalars().all()  
            link\_count \= len(links)  
          
        \# Calculate Link Index (L.I.)  
        link\_index \= link\_count / max(move\_count, 1\)  
          
        \# Calculate phase-specific metrics  
        phase\_metrics \= {}  
        for phase in \['ideation', 'visualization', 'materialization'\]:  
            phase\_moves \= \[m for m in moves if m.phase \== phase\]  
            phase\_links \= \[l for l in links if any(m.id \== l.source\_move\_id and m.phase \== phase for m in moves)\]  
              
            if phase\_moves:  
                phase\_metrics\[phase\] \= {  
                    'move\_count': len(phase\_moves),  
                    'link\_count': len(phase\_links),  
                    'link\_index': len(phase\_links) / len(phase\_moves),  
                    'critical\_moves': await self.\_identify\_critical\_moves(phase\_moves, phase\_links)  
                }  
          
        return {  
            'total\_moves': move\_count,  
            'total\_links': link\_count,  
            'link\_index': link\_index,  
            'phase\_metrics': phase\_metrics,  
            'move\_type\_distribution': await self.\_calculate\_move\_type\_distribution(moves),  
            'design\_focus\_distribution': await self.\_calculate\_design\_focus\_distribution(moves)  
        }  
      
    async def \_identify\_critical\_moves(self, moves: List, links: List) \-\> List\[str\]:  
        """Identify critical moves with high link density"""  
          
        move\_link\_counts \= {}  
          
        for link in links:  
            \# Count forward links (outgoing)  
            move\_link\_counts\[link.source\_move\_id\] \= move\_link\_counts.get(link.source\_move\_id, 0\) \+ 1  
            \# Count backward links (incoming)   
            move\_link\_counts\[link.target\_move\_id\] \= move\_link\_counts.get(link.target\_move\_id, 0\) \+ 1  
          
        \# Identify moves with link count above threshold (e.g., top 20%)  
        if not move\_link\_counts:  
            return \[\]  
          
        threshold \= sorted(move\_link\_counts.values())\[-max(1, len(move\_link\_counts) // 5)\]  
        critical\_moves \= \[move\_id for move\_id, count in move\_link\_counts.items() if count \>= threshold\]  
          
        return critical\_moves  
      
    async def \_calculate\_move\_type\_distribution(self, moves: List) \-\> Dict\[str, float\]:  
        """Calculate distribution of move types"""  
          
        if not moves:  
            return {}  
          
        type\_counts \= {}  
        for move in moves:  
            type\_counts\[move.move\_type\] \= type\_counts.get(move.move\_type, 0\) \+ 1  
          
        total \= len(moves)  
        return {move\_type: count / total for move\_type, count in type\_counts.items()}  
      
    async def \_calculate\_design\_focus\_distribution(self, moves: List) \-\> Dict\[str, float\]:  
        """Calculate distribution of design focus areas"""  
          
        if not moves:  
            return {}  
          
        focus\_counts \= {}  
        for move in moves:  
            focus\_counts\[move.design\_focus\] \= focus\_counts.get(move.design\_focus, 0\) \+ 1  
          
        total \= len(moves)  
        return {focus: count / total for focus, count in focus\_counts.items()}  
      
    async def \_analyze\_move\_patterns(self, moves: List\[DesignMoveData\]) \-\> Dict\[str, Any\]:  
        """Analyze patterns in design moves"""  
          
        return {  
            'temporal\_clustering': await self.\_analyze\_temporal\_clustering(moves),  
            'conceptual\_progression': await self.\_analyze\_conceptual\_progression(moves),  
            'complexity\_evolution': await self.\_analyze\_complexity\_evolution(moves),  
            'phase\_transitions': await self.\_analyze\_phase\_transitions(moves)  
        }  
      
    async def \_analyze\_temporal\_clustering(self, moves: List\[DesignMoveData\]) \-\> Dict\[str, Any\]:  
        """Analyze temporal clustering of moves"""  
          
        if len(moves) \< 2:  
            return {'clusters': 0, 'avg\_cluster\_size': 0}  
          
        \# Simple clustering based on time gaps  
        clusters \= \[\]  
        current\_cluster \= \[moves\[0\]\]  
          
        for i in range(1, len(moves)):  
            time\_gap \= (moves\[i\].timestamp \- moves\[i-1\].timestamp).total\_seconds()  
              
            if time\_gap \> 30:  \# 30-second threshold for new cluster  
                clusters.append(current\_cluster)  
                current\_cluster \= \[moves\[i\]\]  
            else:  
                current\_cluster.append(moves\[i\])  
          
        clusters.append(current\_cluster)  
          
        return {  
            'clusters': len(clusters),  
            'avg\_cluster\_size': sum(len(cluster) for cluster in clusters) / len(clusters),  
            'cluster\_sizes': \[len(cluster) for cluster in clusters\]  
        }  
      
    async def \_analyze\_conceptual\_progression(self, moves: List\[DesignMoveData\]) \-\> Dict\[str, Any\]:  
        """Analyze progression of conceptual development"""  
          
        \# Track evolution of design focus over time  
        focus\_sequence \= \[move.design\_focus for move in moves\]  
        focus\_transitions \= \[\]  
          
        for i in range(1, len(focus\_sequence)):  
            if focus\_sequence\[i\] \!= focus\_sequence\[i-1\]:  
                focus\_transitions.append({  
                    'from': focus\_sequence\[i-1\],  
                    'to': focus\_sequence\[i\],  
                    'position': i / len(focus\_sequence)  
                })  
          
        return {  
            'focus\_transitions': len(focus\_transitions),  
            'focus\_stability': 1 \- (len(focus\_transitions) / max(len(moves), 1)),  
            'transition\_details': focus\_transitions  
        }  
      
    async def \_analyze\_complexity\_evolution(self, moves: List\[DesignMoveData\]) \-\> Dict\[str, Any\]:  
        """Analyze evolution of move complexity"""  
          
        complexities \= \[  
            move.linkography\_metadata.get('complexity\_score', 0.5)   
            for move in moves  
        \]  
          
        if not complexities:  
            return {'trend': 'none', 'progression': 0}  
          
        \# Simple linear trend analysis  
        x\_values \= list(range(len(complexities)))  
        if len(x\_values) \> 1:  
            import numpy as np  
            trend \= np.polyfit(x\_values, complexities, 1)\[0\]  \# Linear coefficient  
              
            return {  
                'trend': 'increasing' if trend \> 0.01 else 'decreasing' if trend \< \-0.01 else 'stable',  
                'progression': trend,  
                'complexity\_range': \[min(complexities), max(complexities)\],  
                'average\_complexity': sum(complexities) / len(complexities)  
            }  
        else:  
            return {'trend': 'single\_move', 'progression': 0}  
      
    async def \_analyze\_phase\_transitions(self, moves: List\[DesignMoveData\]) \-\> Dict\[str, Any\]:  
        """Analyze transitions between design phases"""  
          
        phase\_sequence \= \[move.phase for move in moves\]  
        transitions \= \[\]  
          
        for i in range(1, len(phase\_sequence)):  
            if phase\_sequence\[i\] \!= phase\_sequence\[i-1\]:  
                transitions.append({  
                    'from': phase\_sequence\[i-1\],  
                    'to': phase\_sequence\[i\],  
                    'move\_index': i,  
                    'timestamp': moves\[i\].timestamp.isoformat()  
                })  
          
        return {  
            'total\_transitions': len(transitions),  
            'transition\_details': transitions,  
            'phase\_progression': await self.\_validate\_phase\_progression(transitions)  
        }  
      
    async def \_validate\_phase\_progression(self, transitions: List\[Dict\]) \-\> Dict\[str, Any\]:  
        """Validate if phase progression follows expected pattern"""  
          
        expected\_progression \= \['ideation', 'visualization', 'materialization'\]  
          
        if not transitions:  
            return {'valid': True, 'pattern': 'single\_phase'}  
          
        \# Check if transitions follow expected order  
        valid\_progression \= True  
        pattern\_violations \= \[\]  
          
        for transition in transitions:  
            from\_phase \= transition\['from'\]  
            to\_phase \= transition\['to'\]  
              
            try:  
                from\_index \= expected\_progression.index(from\_phase)  
                to\_index \= expected\_progression.index(to\_phase)  
                  
                if to\_index \< from\_index:  \# Moving backward  
                    valid\_progression \= False  
                    pattern\_violations.append(transition)  
            except ValueError:  
                \# Unknown phase  
                valid\_progression \= False  
                pattern\_violations.append(transition)  
          
        return {  
            'valid': valid\_progression,  
            'pattern': 'sequential' if valid\_progression else 'non\_sequential',  
            'violations': pattern\_violations  
        }

"""  
Cognitive Metrics Engine \- Core benchmarking implementation  
Implements the six-dimensional assessment framework  
"""

import asyncio  
import numpy as np  
import pandas as pd  
from typing import Dict, List, Any, Optional  
from datetime import datetime, timedelta  
import logging  
from dataclasses import dataclass  
from sentence\_transformers import SentenceTransformer  
import re  
import json

logger \= logging.getLogger(\_\_name\_\_)

@dataclass  
class InteractionData:  
    session\_id: str  
    timestamp: datetime  
    user\_input: str  
    ai\_response: str  
    phase: str  
    interaction\_type: str  
    metadata: Dict\[str, Any\]

@dataclass  
class CognitiveMetrics:  
    cop\_score: float  \# Cognitive Offloading Prevention  
    dte\_score: float  \# Deep Thinking Engagement  
    se\_score: float   \# Scaffolding Effectiveness  
    ki\_score: float   \# Knowledge Integration  
    lp\_score: float   \# Learning Progression  
    ma\_score: float   \# Metacognitive Awareness  
    composite\_score: float  
    timestamp: datetime

class CognitiveMetricsEngine:  
    """Main engine for calculating cognitive benchmarking metrics"""  
      
    def \_\_init\_\_(self):  
        self.nlp\_model \= None  
        self.session\_histories \= {}  
        self.baseline\_metrics \= {}  
        self.weights \= {  
            'cop': 0.20,  
            'dte': 0.20,  
            'se': 0.15,  
            'ki': 0.15,  
            'lp': 0.15,  
            'ma': 0.15  
        }  
      
    async def initialize(self):  
        """Initialize the metrics engine"""  
        logger.info("Initializing Cognitive Metrics Engine")  
        self.nlp\_model \= SentenceTransformer('all-MiniLM-L6-v2')  
        logger.info("NLP model loaded successfully")  
      
    async def process\_interaction(self, session\_id: str, interaction\_data: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Process a single interaction and calculate real-time metrics"""  
          
        interaction \= InteractionData(  
            session\_id=session\_id,  
            timestamp=datetime.now(),  
            user\_input=interaction\_data.get('user\_input', ''),  
            ai\_response=interaction\_data.get('ai\_response', ''),  
            phase=interaction\_data.get('phase', 'ideation'),  
            interaction\_type=interaction\_data.get('type', 'dialogue'),  
            metadata=interaction\_data.get('metadata', {})  
        )  
          
        \# Store interaction in session history  
        if session\_id not in self.session\_histories:  
            self.session\_histories\[session\_id\] \= \[\]  
        self.session\_histories\[session\_id\].append(interaction)  
          
        \# Calculate cognitive metrics  
        metrics \= await self.\_calculate\_metrics(session\_id, interaction)  
          
        return {  
            'metrics': metrics,  
            'interaction\_id': str(uuid.uuid4()),  
            'recommendations': await self.\_generate\_recommendations(metrics),  
            'alerts': await self.\_check\_alerts(session\_id, metrics)  
        }  
      
    async def \_calculate\_metrics(self, session\_id: str, current\_interaction: InteractionData) \-\> CognitiveMetrics:  
        """Calculate all cognitive metrics for current session state"""  
          
        session\_history \= self.session\_histories\[session\_id\]  
          
        \# Calculate individual metrics  
        cop\_score \= await self.\_calculate\_cop\_score(session\_history, current\_interaction)  
        dte\_score \= await self.\_calculate\_dte\_score(session\_history, current\_interaction)  
        se\_score \= await self.\_calculate\_se\_score(session\_history, current\_interaction)  
        ki\_score \= await self.\_calculate\_ki\_score(session\_history, current\_interaction)  
        lp\_score \= await self.\_calculate\_lp\_score(session\_history, current\_interaction)  
        ma\_score \= await self.\_calculate\_ma\_score(session\_history, current\_interaction)  
          
        \# Calculate composite score  
        composite\_score \= (  
            cop\_score \* self.weights\['cop'\] \+  
            dte\_score \* self.weights\['dte'\] \+  
            se\_score \* self.weights\['se'\] \+  
            ki\_score \* self.weights\['ki'\] \+  
            lp\_score \* self.weights\['lp'\] \+  
            ma\_score \* self.weights\['ma'\]  
        )  
          
        return CognitiveMetrics(  
            cop\_score=cop\_score,  
            dte\_score=dte\_score,  
            se\_score=se\_score,  
            ki\_score=ki\_score,  
            lp\_score=lp\_score,  
            ma\_score=ma\_score,  
            composite\_score=composite\_score,  
            timestamp=datetime.now()  
        )  
      
    async def \_calculate\_cop\_score(self, session\_history: List\[InteractionData\], current: InteractionData) \-\> float:  
        """Calculate Cognitive Offloading Prevention score"""  
          
        \# Analyze question patterns in user inputs  
        direct\_answer\_patterns \= \[  
            r'^what is\\s+',  
            r'^give me\\s+',  
            r'^tell me\\s+',  
            r'^show me\\s+',  
            r'^can you\\s+.\*\\s+for me',  
        \]  
          
        exploratory\_patterns \= \[  
            r'^how might\\s+',  
            r'^what if\\s+',  
            r'^why do you think\\s+',  
            r'^what are the implications of\\s+',  
            r'^how does this relate to\\s+',  
        \]  
          
        direct\_queries \= 0  
        exploratory\_queries \= 0  
          
        for interaction in session\_history\[-10:\]:  \# Last 10 interactions  
            user\_input \= interaction.user\_input.lower()  
              
            if any(re.search(pattern, user\_input) for pattern in direct\_answer\_patterns):  
                direct\_queries \+= 1  
            elif any(re.search(pattern, user\_input) for pattern in exploratory\_patterns):  
                exploratory\_queries \+= 1  
          
        total\_queries \= direct\_queries \+ exploratory\_queries  
        if total\_queries \== 0:  
            return 0.5  \# Neutral score  
          
        \# Calculate inquiry depth  
        inquiry\_depth \= await self.\_calculate\_inquiry\_depth(session\_history)  
          
        \# COP score formula  
        cop\_ratio \= exploratory\_queries / total\_queries  
        cop\_score \= cop\_ratio \* inquiry\_depth  
          
        return min(1.0, max(0.0, cop\_score))  
      
    async def \_calculate\_dte\_score(self, session\_history: List\[InteractionData\], current: InteractionData) \-\> float:  
        """Calculate Deep Thinking Engagement score"""  
          
        if not session\_history:  
            return 0.0  
          
        \# Analyze response complexity  
        response\_complexity \= await self.\_analyze\_response\_complexity(session\_history)  
          
        \# Extract reasoning patterns  
        reasoning\_chains \= await self.\_extract\_reasoning\_patterns(session\_history)  
          
        \# Count reflection indicators  
        reflection\_markers \= await self.\_count\_reflection\_language(session\_history)  
          
        \# Analyze thinking pauses (from interaction timing)  
        pause\_patterns \= await self.\_analyze\_thinking\_pauses(session\_history)  
          
        \# DTE score calculation  
        dte\_score \= (  
            response\_complexity \* 0.3 \+  
            reasoning\_chains \* 0.3 \+  
            reflection\_markers \* 0.2 \+  
            pause\_patterns \* 0.2  
        )  
          
        return min(1.0, max(0.0, dte\_score))  
      
    async def \_calculate\_se\_score(self, session\_history: List\[InteractionData\], current: InteractionData) \-\> float:  
        """Calculate Scaffolding Effectiveness score"""  
          
        \# This would integrate with user proficiency model  
        \# For now, using simplified heuristics  
          
        guidance\_appropriateness \= 0.7  \# Would be calculated based on user profile  
          
        \# Measure progress indicators  
        if len(session\_history) \< 2:  
            return 0.5  
          
        \# Analyze improvement in response quality over time  
        recent\_responses \= \[i.user\_input for i in session\_history\[-5:\]\]  
        earlier\_responses \= \[i.user\_input for i in session\_history\[-10:-5\]\] if len(session\_history) \>= 10 else \[\]  
          
        if not earlier\_responses:  
            return 0.5  
          
        recent\_complexity \= np.mean(\[len(r.split()) for r in recent\_responses\])  
        earlier\_complexity \= np.mean(\[len(r.split()) for r in earlier\_responses\])  
          
        progress\_delta \= (recent\_complexity \- earlier\_complexity) / max(earlier\_complexity, 1\)  
        progress\_score \= 1 / (1 \+ np.exp(-progress\_delta))  \# Sigmoid function  
          
        se\_score \= guidance\_appropriateness \* progress\_score  
          
        return min(1.0, max(0.0, se\_score))  
      
    async def \_calculate\_ki\_score(self, session\_history: List\[InteractionData\], current: InteractionData) \-\> float:  
        """Calculate Knowledge Integration score"""  
          
        \# Analyze concept connections in user responses  
        architectural\_concepts \= \[  
            'proportion', 'scale', 'circulation', 'adjacency', 'hierarchy',  
            'light', 'orientation', 'sustainability', 'community', 'culture',  
            'structure', 'material', 'space', 'program', 'context'  
        \]  
          
        concept\_usage \= {}  
        cross\_connections \= 0  
          
        for interaction in session\_history:  
            user\_text \= interaction.user\_input.lower()  
            for concept in architectural\_concepts:  
                if concept in user\_text:  
                    concept\_usage\[concept\] \= concept\_usage.get(concept, 0\) \+ 1  
          
        \# Count concepts that appear together in responses  
        for interaction in session\_history:  
            user\_text \= interaction.user\_input.lower()  
            concepts\_in\_response \= \[c for c in architectural\_concepts if c in user\_text\]  
            if len(concepts\_in\_response) \> 1:  
                cross\_connections \+= len(concepts\_in\_response) \- 1  
          
        total\_concepts \= len(\[c for c in concept\_usage.values() if c \> 0\])  
        if total\_concepts \== 0:  
            return 0.0  
          
        integration\_depth \= cross\_connections / max(total\_concepts, 1\)  
        ki\_score \= (total\_concepts / len(architectural\_concepts)) \* integration\_depth  
          
        return min(1.0, max(0.0, ki\_score))  
      
    async def \_calculate\_lp\_score(self, session\_history: List\[InteractionData\], current: InteractionData) \-\> float:  
        """Calculate Learning Progression score"""  
          
        if len(session\_history) \< 3:  
            return 0.5  
          
        \# Analyze progression in question sophistication  
        question\_sophistication\_scores \= \[\]  
          
        for interaction in session\_history:  
            sophistication \= await self.\_calculate\_question\_sophistication(interaction.user\_input)  
            question\_sophistication\_scores.append(sophistication)  
          
        if len(question\_sophistication\_scores) \< 2:  
            return 0.5  
          
        \# Calculate learning velocity (improvement rate)  
        time\_points \= list(range(len(question\_sophistication\_scores)))  
        if len(time\_points) \> 1:  
            slope \= np.polyfit(time\_points, question\_sophistication\_scores, 1)\[0\]  
            velocity \= max(0, slope)  \# Only positive learning velocity  
        else:  
            velocity \= 0  
          
        \# Calculate consistency factor  
        consistency \= 1 \- np.std(question\_sophistication\_scores) / max(np.mean(question\_sophistication\_scores), 0.1)  
          
        lp\_score \= velocity \* consistency  
          
        return min(1.0, max(0.0, lp\_score))  
      
    async def \_calculate\_ma\_score(self, session\_history: List\[InteractionData\], current: InteractionData) \-\> float:  
        """Calculate Metacognitive Awareness score"""  
          
        metacognitive\_indicators \= \[  
            r'i think', r'i believe', r'i assume', r'i realize',  
            r'looking back', r'reflecting on', r'considering',  
            r'my approach', r'my strategy', r'i would change',  
            r'i learned', r'i understand better', r'i see now'  
        \]  
          
        self\_corrections \= 0  
        strategy\_changes \= 0  
        reflection\_depth \= 0  
          
        for interaction in session\_history:  
            user\_text \= interaction.user\_input.lower()  
              
            \# Count metacognitive language  
            for indicator in metacognitive\_indicators:  
                if re.search(indicator, user\_text):  
                    reflection\_depth \+= 1  
              
            \# Detect self-corrections  
            if re.search(r'actually|correction|i meant|let me rephrase', user\_text):  
                self\_corrections \+= 1  
              
            \# Detect strategy changes  
            if re.search(r'different approach|new strategy|change.\*method', user\_text):  
                strategy\_changes \+= 1  
          
        total\_interactions \= max(len(session\_history), 1\)  
          
        ma\_score \= (  
            (self\_corrections / total\_interactions) \* 0.3 \+  
            (strategy\_changes / total\_interactions) \* 0.3 \+  
            (reflection\_depth / total\_interactions) \* 0.4  
        )  
          
        return min(1.0, max(0.0, ma\_score))  
      
    \# Helper methods  
    async def \_calculate\_inquiry\_depth(self, session\_history: List\[InteractionData\]) \-\> float:  
        """Calculate depth of inquiry based on question chains"""  
        \# Implementation for analyzing follow-up question patterns  
        return 0.7  \# Placeholder  
      
    async def \_analyze\_response\_complexity(self, session\_history: List\[InteractionData\]) \-\> float:  
        """Analyze linguistic complexity of user responses"""  
        if not session\_history:  
            return 0.0  
          
        complexities \= \[\]  
        for interaction in session\_history:  
            text \= interaction.user\_input  
            words \= len(text.split())  
            sentences \= len(\[s for s in text.split('.') if s.strip()\])  
            avg\_words\_per\_sentence \= words / max(sentences, 1\)  
            complexity \= min(1.0, avg\_words\_per\_sentence / 20\)  \# Normalize to 0-1  
            complexities.append(complexity)  
          
        return np.mean(complexities)  
      
    async def \_extract\_reasoning\_patterns(self, session\_history: List\[InteractionData\]) \-\> float:  
        """Extract and analyze reasoning patterns"""  
        reasoning\_indicators \= \[  
            r'because', r'therefore', r'consequently', r'as a result',  
            r'this means', r'which leads to', r'given that', r'since'  
        \]  
          
        reasoning\_count \= 0  
        total\_interactions \= max(len(session\_history), 1\)  
          
        for interaction in session\_history:  
            text \= interaction.user\_input.lower()  
            for indicator in reasoning\_indicators:  
                if indicator in text:  
                    reasoning\_count \+= 1  
                    break  \# Count once per interaction  
          
        return reasoning\_count / total\_interactions  
      
    async def \_count\_reflection\_language(self, session\_history: List\[InteractionData\]) \-\> float:  
        """Count reflection and metacognitive language markers"""  
        \# Implementation for reflection language analysis  
        return 0.6  \# Placeholder  
      
    async def \_analyze\_thinking\_pauses(self, session\_history: List\[InteractionData\]) \-\> float:  
        """Analyze pause patterns between interactions"""  
        if len(session\_history) \< 2:  
            return 0.5  
          
        pause\_durations \= \[\]  
        for i in range(1, len(session\_history)):  
            duration \= (session\_history\[i\].timestamp \- session\_history\[i-1\].timestamp).total\_seconds()  
            pause\_durations.append(duration)  
          
        \# Longer pauses might indicate deeper thinking  
        avg\_pause \= np.mean(pause\_durations)  
        thinking\_score \= min(1.0, avg\_pause / 60\)  \# Normalize to 1 minute max  
          
        return thinking\_score  
      
    async def \_calculate\_question\_sophistication(self, question: str) \-\> float:  
        """Calculate sophistication level of a question"""  
        sophistication\_factors \= {  
            'length': len(question.split()) / 50,  \# Longer questions might be more sophisticated  
            'question\_words': len(\[w for w in question.lower().split() if w in \['how', 'why', 'what', 'when', 'where', 'which'\]\]) / 5,  
            'complexity\_words': len(\[w for w in question.lower().split() if w in \['integrate', 'synthesize', 'evaluate', 'analyze', 'compare'\]\]) / 3  
        }  
          
        sophistication \= sum(sophistication\_factors.values()) / len(sophistication\_factors)  
        return min(1.0, sophistication)  
      
    async def \_generate\_recommendations(self, metrics: CognitiveMetrics) \-\> List\[str\]:  
        """Generate recommendations based on current metrics"""  
        recommendations \= \[\]  
          
        if metrics.cop\_score \< 0.5:  
            recommendations.append("Encourage more exploratory questioning rather than seeking direct answers")  
          
        if metrics.dte\_score \< 0.5:  
            recommendations.append("Promote deeper reflection and analysis of design decisions")  
          
        if metrics.ki\_score \< 0.5:  
            recommendations.append("Guide user to connect concepts across different architectural domains")  
          
        return recommendations  
      
    async def \_check\_alerts(self, session\_id: str, metrics: CognitiveMetrics) \-\> List\[str\]:  
        """Check for alerts that require intervention"""  
        alerts \= \[\]  
          
        if metrics.composite\_score \< 0.3:  
            alerts.append("LOW\_ENGAGEMENT")  
          
        if metrics.cop\_score \< 0.2:  
            alerts.append("HIGH\_COGNITIVE\_OFFLOADING")  
          
        return alerts  
      
    async def cleanup(self):  
        """Cleanup resources"""  
        logger.info("Cleaning up Cognitive Metrics Engine")

### **3\. Session Management**

**File: `src/data_processing/session_manager.py`**

"""  
Session Manager \- Handles test session lifecycle and data persistence  
"""

import asyncio  
import uuid  
from datetime import datetime, timedelta  
from typing import Dict, List, Optional, Any  
import json  
import logging  
from dataclasses import dataclass, asdict  
from sqlalchemy.ext.asyncio import AsyncSession  
from sqlalchemy import select, insert, update

from ..utils.database import get\_db\_session  
from ..utils.models import Session, Interaction, Assessment, DesignOutput

logger \= logging.getLogger(\_\_name\_\_)

@dataclass  
class SessionConfig:  
    participant\_id: str  
    group\_assignment: str  \# 'mentor', 'generic\_ai', 'control'  
    phase: str  
    test\_configuration: Dict\[str, Any\]

class SessionManager:  
    """Manages test sessions and data persistence"""  
      
    def \_\_init\_\_(self):  
        self.active\_sessions \= {}  
        self.session\_configs \= {}  
      
    async def initialize(self):  
        """Initialize session manager"""  
        logger.info("Initializing Session Manager")  
      
    async def create\_session(self, config: SessionConfig) \-\> str:  
        """Create a new test session"""  
        session\_id \= str(uuid.uuid4())  
          
        async with get\_db\_session() as db:  
            session \= Session(  
                id=session\_id,  
                participant\_id=config.participant\_id,  
                group\_assignment=config.group\_assignment,  
                phase=config.phase,  
                start\_time=datetime.now(),  
                configuration=asdict(config.test\_configuration)  
            )  
              
            db.add(session)  
            await db.commit()  
          
        self.active\_sessions\[session\_id\] \= config  
          
        logger.info(f"Created session {session\_id} for participant {config.participant\_id}")  
        return session\_id  
      
    async def save\_interaction(self, session\_id: str, interaction\_data: Dict\[str, Any\]) \-\> str:  
        """Save interaction to database"""  
        interaction\_id \= str(uuid.uuid4())  
          
        async with get\_db\_session() as db:  
            interaction \= Interaction(  
                id=interaction\_id,  
                session\_id=session\_id,  
                timestamp=datetime.now(),  
                interaction\_type=interaction\_data.get('type', 'dialogue'),  
                user\_input=interaction\_data.get('user\_input', ''),  
                ai\_response=interaction\_data.get('ai\_response', ''),  
                cognitive\_metrics=interaction\_data.get('cognitive\_metrics', {}),  
                metadata=interaction\_data.get('metadata', {})  
            )  
              
            db.add(interaction)  
            await db.commit()  
          
        return interaction\_id  
      
    async def save\_assessment(self, session\_id: str, assessment\_data: Dict\[str, Any\]) \-\> str:  
        """Save assessment results"""  
        assessment\_id \= str(uuid.uuid4())  
          
        async with get\_db\_session() as db:  
            assessment \= Assessment(  
                id=assessment\_id,  
                session\_id=session\_id,  
                assessment\_type=assessment\_data.get('type', 'critical\_thinking'),  
                questions=assessment\_data.get('questions', \[\]),  
                responses=assessment\_data.get('responses', {}),  
                scores=assessment\_data.get('scores', {}),  
                completion\_time=assessment\_data.get('completion\_time', 0),  
                timestamp=datetime.now()  
            )  
              
            db.add(assessment)  
            await db.commit()  
          
        return assessment\_id  
      
    async def get\_session\_data(self, session\_id: str) \-\> Dict\[str, Any\]:  
        """Retrieve complete session data"""  
        async with get\_db\_session() as db:  
            \# Get session  
            session\_result \= await db.execute(  
                select(Session).where(Session.id \== session\_id)  
            )  
            session \= session\_result.scalar\_one\_or\_none()  
              
            if not session:  
                raise ValueError(f"Session {session\_id} not found")  
              
            \# Get interactions  
            interactions\_result \= await db.execute(  
                select(Interaction).where(Interaction.session\_id \== session\_id)  
                .order\_by(Interaction.timestamp)  
            )  
            interactions \= interactions\_result.scalars().all()  
              
            \# Get assessments  
            assessments\_result \= await db.execute(  
                select(Assessment).where(Assessment.session\_id \== session\_id)  
            )  
            assessments \= assessments\_result.scalars().all()  
              
            \# Get design outputs  
            outputs\_result \= await db.execute(  
                select(DesignOutput).where(DesignOutput.session\_id \== session\_id)  
            )  
            outputs \= outputs\_result.scalars().all()  
          
        return {  
            'session': {  
                'id': session.id,  
                'participant\_id': session.participant\_id,  
                'group\_assignment': session.group\_assignment,  
                'phase': session.phase,  
                'start\_time': session.start\_time.isoformat(),  
                'end\_time': session.end\_time.isoformat() if session.end\_time else None,  
                'configuration': session.configuration  
            },  
            'interactions': \[  
                {  
                    'id': i.id,  
                    'timestamp': i.timestamp.isoformat(),  
                    'type': i.interaction\_type,  
                    'user\_input': i.user\_input,  
                    'ai\_response': i.ai\_response,  
                    'cognitive\_metrics': i.cognitive\_metrics,  
                    'metadata': i.metadata  
                }  
                for i in interactions  
            \],  
            'assessments': \[  
                {  
                    'id': a.id,  
                    'type': a.assessment\_type,  
                    'questions': a.questions,  
                    'responses': a.responses,  
                    'scores': a.scores,  
                    'completion\_time': a.completion\_time,  
                    'timestamp': a.timestamp.isoformat()  
                }  
                for a in assessments  
            \],  
            'design\_outputs': \[  
                {  
                    'id': o.id,  
                    'type': o.output\_type,  
                    'content': o.content,  
                    'file\_path': o.file\_path,  
                    'expert\_scores': o.expert\_scores,  
                    'timestamp': o.created\_at.isoformat()  
                }  
                for o in outputs  
            \]  
        }  
      
    async def close\_session(self, session\_id: str):  
        """Close and finalize session"""  
        async with get\_db\_session() as db:  
            await db.execute(  
                update(Session)  
                .where(Session.id \== session\_id)  
                .values(end\_time=datetime.now(), completed=True)  
            )  
            await db.commit()  
          
        if session\_id in self.active\_sessions:  
            del self.active\_sessions\[session\_id\]  
          
        logger.info(f"Closed session {session\_id}")  
      
    async def cleanup(self):  
        """Cleanup session manager"""  
        logger.info("Cleaning up Session Manager")

### **4\. Assessment Tools**

**File: `src/assessment/critical_thinking_test.py`**

"""  
Critical Thinking Assessment \- Adapted for architectural design context  
"""

import json  
import random  
from typing import Dict, List, Any  
from dataclasses import dataclass  
import uuid

@dataclass  
class Question:  
    id: str  
    type: str  
    content: str  
    options: List\[str\]  
    correct\_answer: Any  
    skill\_area: str  
    difficulty: str

class CriticalThinkingAssessment:  
    """Critical thinking assessment adapted for architectural design"""  
      
    def \_\_init\_\_(self):  
        self.questions \= self.\_load\_questions()  
      
    def \_load\_questions(self) \-\> List\[Question\]:  
        """Load critical thinking questions specific to architectural design"""  
          
        questions \= \[  
            Question(  
                id="ct\_001",  
                type="argument\_analysis",  
                content="A community center design includes a large open space that can be divided. The architect argues: 'This flexibility is important because community centers need to accommodate varying group sizes.' Evaluate this argument's strength and identify any assumptions.",  
                options=\[\],  
                correct\_answer="Strong argument with valid assumption about variable community needs",  
                skill\_area="argument\_evaluation",  
                difficulty="intermediate"  
            ),  
              
            Question(  
                id="ct\_002",   
                type="problem\_solving",  
                content="A community group wants both a quiet library space and an active children's area in limited square footage. What's your primary approach to resolving this conflict?",  
                options=\[\],  
                correct\_answer="Multi-step approach involving stakeholder consultation, flexible design solutions, and prioritization matrix",  
                skill\_area="problem\_decomposition",  
                difficulty="advanced"  
            ),  
              
            Question(  
                id="ct\_003",  
                type="assumption\_identification",  
                content="An architect states: 'The entrance should be monumental because community centers need to make a strong civic statement.' What key assumption underlies this statement?",  
                options=\[  
                    "A) Community centers should represent civic importance",  
                    "B) Monumental entrances effectively communicate civic values",  
                    "C) The community desires a prominent architectural statement",  
                    "D) All of the above assumptions are present"  
                \],  
                correct\_answer="D",  
                skill\_area="assumption\_analysis",  
                difficulty="intermediate"  
            ),  
              
            Question(  
                id="ct\_004",  
                type="hypothesis\_testing",  
                content="If your community center design aims to promote intergenerational interaction, name one specific design feature and how you'd measure its success.",  
                options=\[\],  
                correct\_answer="Observable design features (sightlines, shared spaces, varied seating) with measurable outcomes",  
                skill\_area="hypothesis\_formulation",  
                difficulty="advanced"  
            ),  
              
            Question(  
                id="ct\_005",  
                type="likelihood\_assessment",  
                content="Rank these approaches for making design decisions about program allocation: (A) Expert judgment, (B) Demographic analysis, (C) Focus groups, (D) Usage pattern analysis",  
                options=\[  
                    "A) Expert judgment based on similar projects",  
                    "B) Demographic analysis of the surrounding neighborhood",   
                    "C) Focus group sessions with community members",  
                    "D) Analysis of existing community facility usage patterns"  
                \],  
                correct\_answer="D, C, B, A",  
                skill\_area="evidence\_evaluation",  
                difficulty="advanced"  
            ),  
              
            Question(  
                id="ct\_006",  
                type="causal\_reasoning",  
                content="A warehouse conversion project shows high energy costs after renovation. What's the most likely primary cause?",  
                options=\[  
                    "A) Large volume → difficult to heat → high energy costs",  
                    "B) Industrial windows → poor insulation → heat loss → high costs",  
                    "C) Open plan → air circulation → temperature variation → system overwork → high costs",  
                    "D) All factors contribute through interconnected relationships"  
                \],  
                correct\_answer="D",  
                skill\_area="systems\_thinking",  
                difficulty="intermediate"  
            ),  
              
            Question(  
                id="ct\_007",  
                type="alternative\_solutions",  
                content="For wheelchair access to a 4-foot elevated site, list two different solution approaches.",  
                options=\[\],  
                correct\_answer="Multiple valid approaches: ramps, platform lifts, split-level design",  
                skill\_area="creative\_problem\_solving",  
                difficulty="advanced"  
            ),  
              
            Question(  
                id="ct\_008",  
                type="logical\_fallacies",  
                content="Identify the logical fallacy: 'Famous architect X uses concrete extensively, so our community center should be primarily concrete to be architecturally significant.'",  
                options=\[  
                    "A) Appeal to authority",  
                    "B) False cause",  
                    "C) Hasty generalization",   
                    "D) False analogy"  
                \],  
                correct\_answer="A",  
                skill\_area="logical\_reasoning",  
                difficulty="intermediate"  
            )  
        \]  
                    "B) Monumental entrances effectively communicate civic values",  
                    "C) The community desires a prominent architectural statement",  
                    "D) All of the above assumptions are present"  
                \],  
                correct\_answer="D",  
                skill\_area="assumption\_analysis",  
                difficulty="intermediate"  
            ),  
              
            Question(  
                id="ct\_004",  
                type="hypothesis\_testing",  
                content="If your community center design aims to promote intergenerational interaction, what specific design features would test this hypothesis and how would you measure success?",  
                options=\[\],  
                correct\_answer="Observable design features (sightlines, shared spaces, varied seating) with measurable outcomes (usage patterns, survey data, behavioral observation)",  
                skill\_area="hypothesis\_formulation",  
                difficulty="advanced"  
            ),  
              
            Question(  
                id="ct\_005",  
                type="likelihood\_assessment",  
                content="Given limited community input data, rank these approaches to making design decisions about program allocation from most to least reliable:",  
                options=\[  
                    "A) Expert architectural judgment based on similar projects",  
                    "B) Demographic analysis of the surrounding neighborhood",   
                    "C) Small focus group sessions with community members",  
                    "D) Analysis of existing community facility usage patterns"  
                \],  
                correct\_answer="D, C, B, A",  
                skill\_area="evidence\_evaluation",  
                difficulty="advanced"  
            ),  
              
            Question(  
                id="ct\_006",  
                type="causal\_reasoning",  
                content="A warehouse conversion project shows high energy costs after renovation. Identify the most likely causal chain:",  
                options=\[  
                    "A) Large volume → difficult to heat → high energy costs",  
                    "B) Industrial windows → poor insulation → heat loss → high costs",  
                    "C) Open plan → air circulation → temperature variation → system overwork → high costs",  
                    "D) All factors contribute through interconnected relationships"  
                \],  
                correct\_answer="D",  
                skill\_area="systems\_thinking",  
                difficulty="intermediate"  
            ),  
              
            Question(  
                id="ct\_007",  
                type="alternative\_solutions",  
                content="A community center design must accommodate wheelchair users, but the existing warehouse has a 4-foot elevation change across the site. Generate and evaluate three different solution approaches.",  
                options=\[\],  
                correct\_answer="Multiple valid approaches: ramps, platform lifts, split-level design \- evaluated on cost, aesthetics, functionality, and user experience",  
                skill\_area="creative\_problem\_solving",  
                difficulty="advanced"  
            ),  
              
            Question(  
                id="ct\_008",  
                type="perspective\_taking",  
                content="How might the following groups view a proposed glass-walled community meeting room differently: (1) teenagers, (2) elderly residents, (3) community organizers, (4) security personnel?",  
                options=\[\],  
                correct\_answer="Diverse perspectives: teenagers (visibility/surveillance concerns), elderly (acoustic/privacy needs), organizers (transparency/accessibility), security (monitoring/safety)",  
                skill\_area="stakeholder\_analysis",  
                difficulty="intermediate"  
            ),  
              
            Question(  
                id="ct\_009",  
                type="logical\_fallacies",  
                content="Identify the logical fallacy in this statement: 'The famous architect Tadao Ando uses concrete extensively, so our community center should be primarily concrete to be architecturally significant.'",  
                options=\[  
                    "A) Appeal to authority",  
                    "B) False cause",  
                    "C) Hasty generalization",   
                    "D) False analogy"  
                \],  
                correct\_answer="A",  
                skill\_area="logical\_reasoning",  
                difficulty="intermediate"  
            ),  
              
            Question(  
                id="ct\_010",  
                type="inference\_quality",  
                content="From the observation that 'current community meetings are poorly attended,' what can we most reliably infer about design requirements?",  
                options=\[  
                    "A) The meeting spaces should be smaller",  
                    "B) The design should make meetings more appealing",  
                    "C) We need to investigate why attendance is low before designing",  
                    "D) Meeting spaces are not a priority for this community"  
                \],  
                correct\_answer="C",  
                skill\_area="evidence\_interpretation",  
                difficulty="intermediate"  
            )  
        \]  
          
        return questions  
      
    def generate\_test(self, difficulty\_level: str \= "mixed", num\_questions: int \= 8\) \-\> Dict\[str, Any\]:  
        """Generate a customized critical thinking test"""  
          
        if difficulty\_level \== "mixed":  
            \# Select questions across difficulty levels for shorter test  
            beginner\_q \= \[q for q in self.questions if q.difficulty \== "beginner"\]  
            intermediate\_q \= \[q for q in self.questions if q.difficulty \== "intermediate"\]   
            advanced\_q \= \[q for q in self.questions if q.difficulty \== "advanced"\]  
              
            selected \= (  
                random.sample(beginner\_q, min(1, len(beginner\_q))) \+  
                random.sample(intermediate\_q, min(4, len(intermediate\_q))) \+  
                random.sample(advanced\_q, min(3, len(advanced\_q)))  
            )  
        else:  
            filtered\_questions \= \[q for q in self.questions if q.difficulty \== difficulty\_level\]  
            selected \= random.sample(filtered\_questions, min(num\_questions, len(filtered\_questions)))  
          
        \# Randomize order  
        random.shuffle(selected)  
          
        return {  
            "test\_id": str(uuid.uuid4()),  
            "questions": \[  
                {  
                    "id": q.id,  
                    "type": q.type,  
                    "content": q.content,  
                    "options": q.options,  
                    "skill\_area": q.skill\_area,  
                    "difficulty": q.difficulty  
                }  
                for q in selected  
            \],  
            "instructions": self.\_get\_test\_instructions(),  
            "time\_limit": 480,  \# 8 minutes  
            "scoring\_rubric": self.\_get\_scoring\_rubric()  
        }  
      
    def score\_test(self, test\_responses: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Score critical thinking test responses"""  
          
        total\_score \= 0  
        skill\_scores \= {}  
        detailed\_feedback \= {}  
          
        for response in test\_responses.get("responses", \[\]):  
            question\_id \= response.get("question\_id")  
            user\_answer \= response.get("answer", "")  
              
            \# Find the question  
            question \= next((q for q in self.questions if q.id \== question\_id), None)  
            if not question:  
                continue  
              
            \# Score the response  
            score \= self.\_score\_individual\_response(question, user\_answer)  
            total\_score \+= score  
              
            \# Track skill area scores  
            if question.skill\_area not in skill\_scores:  
                skill\_scores\[question.skill\_area\] \= \[\]  
            skill\_scores\[question.skill\_area\].append(score)  
              
            \# Generate feedback  
            detailed\_feedback\[question\_id\] \= self.\_generate\_question\_feedback(question, user\_answer, score)  
          
        \# Calculate skill area averages  
        skill\_averages \= {  
            skill: sum(scores) / len(scores)   
            for skill, scores in skill\_scores.items()  
        }  
          
        return {  
            "total\_score": total\_score,  
            "percentage": (total\_score / len(test\_responses.get("responses", \[\]))) \* 100,  
            "skill\_scores": skill\_averages,  
            "detailed\_feedback": detailed\_feedback,  
            "recommendations": self.\_generate\_recommendations(skill\_averages),  
            "completion\_time": test\_responses.get("completion\_time", 0\)  
        }  
      
    def \_score\_individual\_response(self, question: Question, user\_answer: str) \-\> float:  
        """Score an individual question response"""  
          
        if question.type in \["multiple\_choice"\]:  
            return 1.0 if user\_answer.strip().upper() \== question.correct\_answer else 0.0  
          
        elif question.type in \["argument\_analysis", "problem\_solving", "hypothesis\_testing"\]:  
            \# Use keyword-based scoring for open-ended questions  
            return self.\_score\_open\_ended\_response(question, user\_answer)  
          
        else:  
            \# Default partial credit scoring  
            return self.\_calculate\_partial\_credit(question, user\_answer)  
      
    def \_score\_open\_ended\_response(self, question: Question, user\_answer: str) \-\> float:  
        """Score open-ended responses using keyword analysis"""  
          
        answer\_lower \= user\_answer.lower()  
          
        \# Define key concepts for each question type  
        scoring\_criteria \= {  
            "ct\_002": {  \# Problem solving question  
                "keywords": \["systematic", "approach", "stakeholder", "flexible", "priority", "consultation"\],  
                "required\_elements": \["identification of constraints", "solution generation", "evaluation criteria"\],  
                "max\_score": 1.0  
            },  
            "ct\_004": {  \# Hypothesis testing  
                "keywords": \["measurable", "observable", "data", "survey", "behavioral", "sightlines", "interaction"\],  
                "required\_elements": \["specific design features", "measurement method", "success criteria"\],  
                "max\_score": 1.0  
            },  
            "ct\_007": {  \# Alternative solutions  
                "keywords": \["ramp", "lift", "platform", "split-level", "cost", "aesthetic", "functional", "accessibility"\],  
                "required\_elements": \["multiple solutions", "evaluation criteria", "trade-offs"\],  
                "max\_score": 1.0  
            }  
        }  
          
        criteria \= scoring\_criteria.get(question.id, {})  
        if not criteria:  
            return 0.5  \# Default score for unscored questions  
          
        score \= 0.0  
        keyword\_score \= sum(1 for keyword in criteria.get("keywords", \[\]) if keyword in answer\_lower)  
        score \+= min(0.5, keyword\_score / len(criteria.get("keywords", \[\])))  
          
        \# Check for required elements (simplified heuristic)  
        element\_score \= 0  
        for element in criteria.get("required\_elements", \[\]):  
            if any(word in answer\_lower for word in element.split()):  
                element\_score \+= 1  
          
        score \+= min(0.5, element\_score / len(criteria.get("required\_elements", \[\])))  
          
        return min(1.0, score)  
      
    def \_calculate\_partial\_credit(self, question: Question, user\_answer: str) \-\> float:  
        """Calculate partial credit for complex responses"""  
          
        \# Length-based heuristic (longer responses often show more thinking)  
        length\_score \= min(0.3, len(user\_answer.split()) / 50\)  
          
        \# Complexity heuristic (presence of reasoning words)  
        reasoning\_words \= \["because", "therefore", "however", "although", "considering", "given", "since"\]  
        reasoning\_score \= min(0.4, sum(1 for word in reasoning\_words if word in user\_answer.lower()) / 5\)  
          
        \# Architectural vocabulary usage  
        arch\_terms \= \["space", "scale", "proportion", "circulation", "program", "context", "material", "structure"\]  
        vocab\_score \= min(0.3, sum(1 for term in arch\_terms if term in user\_answer.lower()) / 5\)  
          
        return length\_score \+ reasoning\_score \+ vocab\_score  
      
    def \_generate\_question\_feedback(self, question: Question, user\_answer: str, score: float) \-\> str:  
        """Generate personalized feedback for each question"""  
          
        if score \>= 0.8:  
            return f"Excellent response\! You demonstrated strong {question.skill\_area} skills."  
        elif score \>= 0.6:  
            return f"Good thinking on this {question.skill\_area} question. Consider expanding your analysis further."  
        elif score \>= 0.4:  
            return f"You're on the right track with {question.skill\_area}. Try to be more systematic in your approach."  
        else:  
            return f"This {question.skill\_area} question needs more development. Focus on breaking down the problem systematically."  
      
    def \_generate\_recommendations(self, skill\_scores: Dict\[str, float\]) \-\> List\[str\]:  
        """Generate learning recommendations based on skill performance"""  
          
        recommendations \= \[\]  
          
        for skill, score in skill\_scores.items():  
            if score \< 0.6:  
                recommendations.append(f"Focus on developing {skill.replace('\_', ' ')} skills through targeted practice")  
          
        if skill\_scores.get("systems\_thinking", 0\) \< 0.6:  
            recommendations.append("Practice analyzing complex problems with multiple interconnected factors")  
          
        if skill\_scores.get("evidence\_evaluation", 0\) \< 0.6:  
            recommendations.append("Work on distinguishing between strong and weak evidence in design decisions")  
          
        return recommendations  
      
    def \_get\_test\_instructions(self) \-\> str:  
        """Get test instructions"""  
        return """  
        CRITICAL THINKING ASSESSMENT FOR ARCHITECTURAL DESIGN  
          
        This assessment evaluates your critical thinking skills in architectural design contexts.  
          
        Instructions:  
        \- Read each question carefully  
        \- For multiple choice questions, select the best answer  
        \- For open-ended questions, provide concise explanations showing your reasoning  
        \- Consider multiple perspectives when relevant  
        \- You have 8 minutes to complete 8 questions  
          
        Focus on demonstrating your ability to:  
        \- Analyze arguments and identify assumptions  
        \- Solve design problems systematically  
        \- Evaluate evidence and make informed decisions  
        \- Generate alternative solutions  
        """  
      
    def \_get\_scoring\_rubric(self) \-\> Dict\[str, Any\]:  
        """Get detailed scoring rubric"""  
        return {  
            "excellent": {  
                "range": "80-100%",  
                "description": "Demonstrates sophisticated critical thinking with clear reasoning, consideration of multiple perspectives, and well-supported conclusions"  
            },  
            "proficient": {  
                "range": "60-79%",   
                "description": "Shows solid critical thinking skills with adequate reasoning and some consideration of alternatives"  
            },  
            "developing": {  
                "range": "40-59%",  
                "description": "Demonstrates basic critical thinking with simple reasoning and limited perspective-taking"  
            },  
            "beginning": {  
                "range": "0-39%",  
                "description": "Shows minimal critical thinking with unclear reasoning and little evidence of systematic analysis"  
            }  
        }

### **5\. API Routes**

**File: `src/api/routes.py`**

"""  
API Routes for MENTOR B-Tests Platform  
"""

from fastapi import APIRouter, HTTPException, Depends, UploadFile, File  
from typing import List, Dict, Any, Optional  
import json  
import uuid  
from datetime import datetime

from ..benchmarking.cognitive\_metrics import CognitiveMetricsEngine  
from ..data\_processing.session\_manager import SessionManager, SessionConfig  
from ..assessment.critical\_thinking\_test import CriticalThinkingAssessment  
from ..assessment.spatial\_reasoning\_test import SpatialReasoningAssessment  
from ..assessment.architecture\_knowledge\_test import ArchitectureKnowledgeAssessment  
from ..utils.ai\_integrations import MentorAI, GenericAI  
from ..utils.file\_processing import ImageProcessor, ModelProcessor

router \= APIRouter()

\# Initialize components  
cognitive\_engine \= CognitiveMetricsEngine()  
session\_manager \= SessionManager()  
ct\_assessment \= CriticalThinkingAssessment()  
sr\_assessment \= SpatialReasoningAssessment()  
ak\_assessment \= ArchitectureKnowledgeAssessment()  
mentor\_ai \= MentorAI()  
generic\_ai \= GenericAI()  
image\_processor \= ImageProcessor()  
model\_processor \= ModelProcessor()

@router.post("/sessions/create")  
async def create\_session(session\_data: Dict\[str, Any\]):  
    """Create a new test session"""  
      
    try:  
        config \= SessionConfig(  
            participant\_id=session\_data\["participant\_id"\],  
            group\_assignment=session\_data\["group\_assignment"\],  
            phase=session\_data.get("phase", "ideation"),  
            test\_configuration=session\_data.get("configuration", {})  
        )  
          
        session\_id \= await session\_manager.create\_session(config)  
          
        return {  
            "session\_id": session\_id,  
            "status": "created",  
            "next\_steps": await \_get\_next\_steps(config.group\_assignment, config.phase)  
        }  
          
    except Exception as e:  
        raise HTTPException(status\_code=400, detail=str(e))

@router.post("/sessions/{session\_id}/interactions")  
async def process\_interaction(session\_id: str, interaction\_data: Dict\[str, Any\]):  
    """Process user interaction and return AI response with metrics"""  
      
    try:  
        \# Get session configuration  
        session\_data \= await session\_manager.get\_session\_data(session\_id)  
        group\_assignment \= session\_data\["session"\]\["group\_assignment"\]  
        phase \= session\_data\["session"\]\["phase"\]  
          
        \# Generate AI response based on group assignment  
        if group\_assignment \== "mentor":  
            ai\_response \= await mentor\_ai.generate\_response(  
                user\_input=interaction\_data\["user\_input"\],  
                session\_context=session\_data,  
                phase=phase  
            )  
        elif group\_assignment \== "generic\_ai":  
            ai\_response \= await generic\_ai.generate\_response(  
                user\_input=interaction\_data\["user\_input"\],  
                session\_context=session\_data  
            )  
        else:  \# control group  
            ai\_response \= ""  
          
        \# Add AI response to interaction data  
        interaction\_data\["ai\_response"\] \= ai\_response  
          
        \# Process through cognitive metrics engine  
        metrics\_result \= await cognitive\_engine.process\_interaction(  
            session\_id=session\_id,  
            interaction\_data=interaction\_data  
        )  
          
        \# Save interaction  
        interaction\_id \= await session\_manager.save\_interaction(session\_id, {  
            \*\*interaction\_data,  
            "cognitive\_metrics": metrics\_result\["metrics"\].\_\_dict\_\_  
        })  
          
        return {  
            "interaction\_id": interaction\_id,  
            "ai\_response": ai\_response,  
            "cognitive\_metrics": metrics\_result\["metrics"\].\_\_dict\_\_,  
            "recommendations": metrics\_result\["recommendations"\],  
            "alerts": metrics\_result\["alerts"\]  
        }  
          
    except Exception as e:  
        raise HTTPException(status\_code=500, detail=str(e))

@router.post("/sessions/{session\_id}/upload")  
async def upload\_design\_file(session\_id: str, file: UploadFile \= File(...), file\_type: str \= "image"):  
    """Upload and process design files (images, 3D models)"""  
      
    try:  
        \# Save uploaded file  
        file\_path \= f"uploads/{session\_id}/{uuid.uuid4()}\_{file.filename}"  
          
        \# Process based on file type  
        if file\_type \== "image":  
            analysis\_result \= await image\_processor.analyze\_image(file, file\_path)  
        elif file\_type \== "3d\_model":  
            analysis\_result \= await model\_processor.analyze\_model(file, file\_path)  
        else:  
            raise ValueError(f"Unsupported file type: {file\_type}")  
          
        \# Save design output  
        output\_data \= {  
            "type": file\_type,  
            "file\_path": file\_path,  
            "analysis\_result": analysis\_result,  
            "timestamp": datetime.now()  
        }  
          
        \# Save to database  
        await session\_manager.save\_design\_output(session\_id, output\_data)  
          
        return {  
            "upload\_id": str(uuid.uuid4()),  
            "analysis": analysis\_result,  
            "status": "processed"  
        }  
          
    except Exception as e:  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/assessments/critical-thinking")  
async def get\_critical\_thinking\_test(difficulty: str \= "mixed", num\_questions: int \= 15):  
    """Generate critical thinking assessment"""  
      
    try:  
        test \= ct\_assessment.generate\_test(difficulty, num\_questions)  
        return test  
    except Exception as e:  
        raise HTTPException(status\_code=500, detail=str(e))

@router.post("/assessments/critical-thinking/score")  
async def score\_critical\_thinking\_test(test\_responses: Dict\[str, Any\]):  
    """Score critical thinking test responses"""  
      
    try:  
        scores \= ct\_assessment.score\_test(test\_responses)  
        return scores  
    except Exception as e:  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/assessments/spatial-reasoning")  
async def get\_spatial\_reasoning\_test():  
    """Generate spatial reasoning assessment"""  
      
    try:  
        test \= sr\_assessment.generate\_test()  
        return test  
    except Exception as e:  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/assessments/architecture-knowledge")  
async def get\_architecture\_knowledge\_test():  
    """Generate architecture knowledge assessment"""  
      
    try:  
        test \= ak\_assessment.generate\_test()  
        return test  
    except Exception as e:  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/sessions/{session\_id}/metrics")  
async def get\_session\_metrics(session\_id: str):  
    """Get comprehensive metrics for a session"""  
      
    try:  
        session\_data \= await session\_manager.get\_session\_data(session\_id)  
          
        \# Calculate session-level metrics  
        interactions \= session\_data\["interactions"\]  
          
        if not interactions:  
            return {"error": "No interactions found for session"}  
          
        \# Extract cognitive metrics progression  
        metrics\_progression \= \[\]  
        for interaction in interactions:  
            if "cognitive\_metrics" in interaction:  
                metrics\_progression.append({  
                    "timestamp": interaction\["timestamp"\],  
                    "metrics": interaction\["cognitive\_metrics"\]  
                })  
          
        \# Calculate summary statistics  
        if metrics\_progression:  
            latest\_metrics \= metrics\_progression\[-1\]\["metrics"\]  
            avg\_metrics \= \_calculate\_average\_metrics(metrics\_progression)  
            trend\_analysis \= \_analyze\_trends(metrics\_progression)  
        else:  
            latest\_metrics \= {}  
            avg\_metrics \= {}  
            trend\_analysis \= {}  
          
        return {  
            "session\_id": session\_id,  
            "latest\_metrics": latest\_metrics,  
            "average\_metrics": avg\_metrics,  
            "trend\_analysis": trend\_analysis,  
            "metrics\_progression": metrics\_progression,  
            "session\_summary": {  
                "total\_interactions": len(interactions),  
                "session\_duration": \_calculate\_session\_duration(session\_data),  
                "phase": session\_data\["session"\]\["phase"\],  
                "group\_assignment": session\_data\["session"\]\["group\_assignment"\]  
            }  
        }  
          
    except Exception as e:  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/sessions/{session\_id}/export")  
async def export\_session\_data(session\_id: str, format: str \= "json"):  
    """Export complete session data"""  
      
    try:  
        session\_data \= await session\_manager.get\_session\_data(session\_id)  
          
        if format \== "json":  
            return session\_data  
        elif format \== "csv":  
            \# Convert to CSV format  
            csv\_data \= \_convert\_to\_csv(session\_data)  
            return {"csv\_data": csv\_data}  
        else:  
            raise ValueError(f"Unsupported export format: {format}")  
              
    except Exception as e:  
        raise HTTPException(status\_code=500, detail=str(e))

@router.post("/sessions/{session\_id}/close")  
async def close\_session(session\_id: str):  
    """Close and finalize session"""  
      
    try:  
        await session\_manager.close\_session(session\_id)  
          
        \# Generate final report  
        final\_report \= await \_generate\_final\_report(session\_id)  
          
        return {  
            "status": "closed",  
            "final\_report": final\_report  
        }  
          
    except Exception as e:  
        raise HTTPException(status\_code=500, detail=str(e))

\# Helper functions  
async def \_get\_next\_steps(group\_assignment: str, phase: str) \-\> List\[str\]:  
    """Get next steps based on group assignment and phase"""  
      
    next\_steps \= \[\]  
      
    if phase \== "ideation":  
        next\_steps.append("Complete pre-test assessments")  
        next\_steps.append("Begin architectural concept development task")  
          
    if group\_assignment \== "mentor":  
        next\_steps.append("Interact with MENTOR AI system")  
    elif group\_assignment \== "generic\_ai":  
        next\_steps.append("Use generic AI assistant")  
    else:  
        next\_steps.append("Complete tasks without AI assistance")  
      
    return next\_steps

def \_calculate\_average\_metrics(metrics\_progression: List\[Dict\]) \-\> Dict\[str, float\]:  
    """Calculate average metrics across session"""  
      
    if not metrics\_progression:  
        return {}  
      
    metric\_sums \= {}  
    count \= len(metrics\_progression)  
      
    for entry in metrics\_progression:  
        metrics \= entry\["metrics"\]  
        for key, value in metrics.items():  
            if isinstance(value, (int, float)):  
                metric\_sums\[key\] \= metric\_sums.get(key, 0\) \+ value  
      
    return {key: total / count for key, total in metric\_sums.items()}

def \_analyze\_trends(metrics\_progression: List\[Dict\]) \-\> Dict\[str, str\]:  
    """Analyze trends in metrics over time"""  
      
    if len(metrics\_progression) \< 3:  
        return {"trend": "insufficient\_data"}  
      
    \# Simple trend analysis  
    first\_half \= metrics\_progression\[:len(metrics\_progression)//2\]  
    second\_half \= metrics\_progression\[len(metrics\_progression)//2:\]  
      
    first\_avg \= \_calculate\_average\_metrics(first\_half)  
    second\_avg \= \_calculate\_average\_metrics(second\_half)  
      
    trends \= {}  
    for key in first\_avg:  
        if key in second\_avg:  
            if second\_avg\[key\] \> first\_avg\[key\] \* 1.1:  
                trends\[key\] \= "improving"  
            elif second\_avg\[key\] \< first\_avg\[key\] \* 0.9:  
                trends\[key\] \= "declining"  
            else:  
                trends\[key\] \= "stable"  
      
    return trends

def \_calculate\_session\_duration(session\_data: Dict) \-\> int:  
    """Calculate session duration in minutes"""  
      
    interactions \= session\_data\["interactions"\]  
    if not interactions:  
        return 0  
      
    start\_time \= datetime.fromisoformat(interactions\[0\]\["timestamp"\])  
    end\_time \= datetime.fromisoformat(interactions\[-1\]\["timestamp"\])  
      
    duration \= (end\_time \- start\_time).total\_seconds() / 60  
    return int(duration)

def \_convert\_to\_csv(session\_data: Dict) \-\> str:  
    """Convert session data to CSV format"""  
      
    import csv  
    import io  
      
    output \= io.StringIO()  
    writer \= csv.writer(output)  
      
    \# Write headers  
    writer.writerow(\[  
        "interaction\_id", "timestamp", "user\_input", "ai\_response",   
        "cop\_score", "dte\_score", "se\_score", "ki\_score", "lp\_score", "ma\_score", "composite\_score"  
    \])  
      
    \# Write interaction data  
    for interaction in session\_data\["interactions"\]:  
        metrics \= interaction.get("cognitive\_metrics", {})  
        writer.writerow(\[  
            interaction\["id"\],  
            interaction\["timestamp"\],  
            interaction\["user\_input"\],  
            interaction\["ai\_response"\],  
            metrics.get("cop\_score", ""),  
            metrics.get("dte\_score", ""),  
            metrics.get("se\_score", ""),  
            metrics.get("ki\_score", ""),  
            metrics.get("lp\_score", ""),  
            metrics.get("ma\_score", ""),  
            metrics.get("composite\_score", "")  
        \])  
      
    return output.getvalue()

async def \_generate\_final\_report(session\_id: str) \-\> Dict\[str, Any\]:  
    """Generate comprehensive final report for session"""  
      
    session\_data \= await session\_manager.get\_session\_data(session\_id)  
      
    \# Calculate final metrics  
    interactions \= session\_data\["interactions"\]  
    final\_metrics \= {}  
      
    if interactions and "cognitive\_metrics" in interactions\[-1\]:  
        final\_metrics \= interactions\[-1\]\["cognitive\_metrics"\]  
      
    \# Assessment scores  
    assessment\_scores \= {}  
    for assessment in session\_data\["assessments"\]:  
        assessment\_scores\[assessment\["type"\]\] \= assessment\["scores"\]  
      
    \# Design quality evaluation (would be filled by expert review)  
    design\_quality \= {  
        "creativity": None,  
        "technical\_competence": None,  
        "community\_responsiveness": None,  
        "process\_quality": None  
    }  
      
    return {  
        "session\_id": session\_id,  
        "participant\_id": session\_data\["session"\]\["participant\_id"\],  
        "group\_assignment": session\_data\["session"\]\["group\_assignment"\],  
        "final\_cognitive\_metrics": final\_metrics,  
        "assessment\_scores": assessment\_scores,  
        "design\_quality\_scores": design\_quality,  
        "session\_statistics": {  
            "total\_interactions": len(interactions),  
            "duration\_minutes": \_calculate\_session\_duration(session\_data),  
            "uploads\_count": len(session\_data\["design\_outputs"\])  
        },  
        "recommendations": await \_generate\_participant\_recommendations(session\_data)  
    }

async def \_generate\_participant\_recommendations(session\_data: Dict) \-\> List\[str\]:  
    """Generate personalized recommendations for participant"""  
      
    recommendations \= \[\]  
      
    \# Analyze cognitive metrics trends  
    interactions \= session\_data\["interactions"\]  
    if interactions and "cognitive\_metrics" in interactions\[-1\]:  
        final\_metrics \= interactions\[-1\]\["cognitive\_metrics"\]  
          
        if final\_metrics.get("cop\_score", 0\) \< 0.6:  
            recommendations.append("Focus on asking more exploratory questions rather than seeking direct answers")  
          
        if final\_metrics.get("dte\_score", 0\) \< 0.6:  
            recommendations.append("Practice deeper reflection and analysis of design decisions")  
          
        if final\_metrics.get("ki\_score", 0\) \< 0.6:  
            recommendations.append("Work on connecting concepts across different architectural domains")  
      
    \# Analyze assessment performance  
    for assessment in session\_data\["assessments"\]:  
        if assessment\["type"\] \== "critical\_thinking":  
            if assessment\["scores"\].get("percentage", 0\) \< 70:  
                recommendations.append("Strengthen critical thinking skills through structured problem-solving exercises")  
      
    return recommendations

### **6\. Enhanced Database Models for Linkography**

**File: `src/utils/models.py`**

"""  
Enhanced Database Models with Linkography Support  
"""

from sqlalchemy import Column, String, DateTime, Boolean, Integer, Float, Text, JSON, ForeignKey  
from sqlalchemy.ext.declarative import declarative\_base  
from sqlalchemy.orm import relationship  
from sqlalchemy.dialects.postgresql import UUID  
import uuid  
from datetime import datetime

Base \= declarative\_base()

\# ... existing models ...

class DesignMove(Base):  
    \_\_tablename\_\_ \= "design\_moves"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
    timestamp \= Column(DateTime, default=datetime.utcnow)  
    sequence\_number \= Column(Integer, nullable=False)  
    content \= Column(Text, nullable=False)  
      
    \# Linkography classification fields  
    move\_type \= Column(String(20), nullable=False)  \# analysis|synthesis|evaluation|transformation|reflection  
    phase \= Column(String(20), nullable=False)  \# ideation|visualization|materialization  
    modality \= Column(String(20), nullable=False)  \# text|sketch|image|voice|upload  
    cognitive\_operation \= Column(String(20), nullable=False)  \# proposal|clarification|assessment|support|reference  
    design\_focus \= Column(String(20), nullable=False)  \# function|form|structure|material|environment|culture  
      
    \# Context and metadata  
    context \= Column(JSON, nullable=True)  
    linkography\_metadata \= Column(JSON, nullable=True)  
      
    \# Relationships  
    session \= relationship("Session", back\_populates="design\_moves")  
    outgoing\_links \= relationship("LinkographyLink", foreign\_keys="LinkographyLink.source\_move\_id", back\_populates="source\_move")  
    incoming\_links \= relationship("LinkographyLink", foreign\_keys="LinkographyLink.target\_move\_id", back\_populates="target\_move")

class LinkographyLink(Base):  
    \_\_tablename\_\_ \= "linkography\_links"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    source\_move\_id \= Column(UUID(as\_uuid=True), ForeignKey("design\_moves.id"), nullable=False)  
    target\_move\_id \= Column(UUID(as\_uuid=True), ForeignKey("design\_moves.id"), nullable=False)  
      
    \# Link properties  
    link\_strength \= Column(Float, nullable=False)  
    link\_type \= Column(String(30), nullable=False)  \# phase\_transition|focus\_development|analysis\_synthesis|etc  
    semantic\_similarity \= Column(Float, nullable=False)  
    temporal\_distance \= Column(Float, nullable=False)  \# seconds between moves  
      
    \# Transition indicators  
    phase\_transition \= Column(Boolean, default=False)  
    move\_type\_transition \= Column(Boolean, default=False)  
      
    created\_at \= Column(DateTime, default=datetime.utcnow)  
      
    \# Relationships  
    source\_move \= relationship("DesignMove", foreign\_keys=\[source\_move\_id\], back\_populates="outgoing\_links")  
    target\_move \= relationship("DesignMove", foreign\_keys=\[target\_move\_id\], back\_populates="incoming\_links")

class LinkographySession(Base):  
    \_\_tablename\_\_ \= "linkography\_sessions"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
      
    \# Linkography metrics  
    total\_moves \= Column(Integer, default=0)  
    total\_links \= Column(Integer, default=0)  
    link\_index \= Column(Float, default=0.0)  \# L.I. \= links/moves  
      
    \# Phase-specific metrics  
    ideation\_metrics \= Column(JSON, nullable=True)  
    visualization\_metrics \= Column(JSON, nullable=True)  
    materialization\_metrics \= Column(JSON, nullable=True)  
      
    \# Critical moves identification  
    critical\_moves \= Column(JSON, nullable=True)  \# List of move IDs  
      
    \# Distribution analyses  
    move\_type\_distribution \= Column(JSON, nullable=True)  
    design\_focus\_distribution \= Column(JSON, nullable=True)  
      
    \# Pattern analysis results  
    temporal\_patterns \= Column(JSON, nullable=True)  
    conceptual\_patterns \= Column(JSON, nullable=True)  
    complexity\_patterns \= Column(JSON, nullable=True)  
      
    last\_updated \= Column(DateTime, default=datetime.utcnow)  
      
    \# Relationships  
    session \= relationship("Session")

\# Update Session model to include design\_moves relationship  
Session.design\_moves \= relationship("DesignMove", back\_populates="session")

### **7\. Database Migration for Linkography**

**File: `scripts/migrate_linkography.py`**

"""  
Database migration script for Linkography functionality  
"""

import asyncio  
import logging  
from sqlalchemy.ext.asyncio import create\_async\_engine  
from sqlalchemy import text  
import os  
from dotenv import load\_dotenv

load\_dotenv()

logging.basicConfig(level=logging.INFO)  
logger \= logging.getLogger(\_\_name\_\_)

async def run\_linkography\_migration():  
    """Run database migration for Linkography tables"""  
      
    database\_url \= os.getenv("DATABASE\_URL")  
    engine \= create\_async\_engine(database\_url, echo=True)  
      
    migration\_sql \= """  
    \-- Design Moves table  
    CREATE TABLE IF NOT EXISTS design\_moves (  
        id UUID PRIMARY KEY DEFAULT gen\_random\_uuid(),  
        session\_id UUID REFERENCES sessions(id) NOT NULL,  
        timestamp TIMESTAMP DEFAULT CURRENT\_TIMESTAMP,  
        sequence\_number INTEGER NOT NULL,  
        content TEXT NOT NULL,  
        move\_type VARCHAR(20) NOT NULL CHECK (move\_type IN ('analysis', 'synthesis', 'evaluation', 'transformation', 'reflection')),  
        phase VARCHAR(20) NOT NULL CHECK (phase IN ('ideation', 'visualization', 'materialization')),  
        modality VARCHAR(20) NOT NULL CHECK (modality IN ('text', 'sketch', 'image', 'voice', 'upload')),  
        cognitive\_operation VARCHAR(20) NOT NULL CHECK (cognitive\_operation IN ('proposal', 'clarification', 'assessment', 'support', 'reference')),  
        design\_focus VARCHAR(20) NOT NULL CHECK (design\_focus IN ('function', 'form', 'structure', 'material', 'environment', 'culture')),  
        context JSONB,  
        linkography\_metadata JSONB  
    );  
      
    \-- Linkography Links table  
    CREATE TABLE IF NOT EXISTS linkography\_links (  
        id UUID PRIMARY KEY DEFAULT gen\_random\_uuid(),  
        source\_move\_id UUID REFERENCES design\_moves(id) NOT NULL,  
        target\_move\_id UUID REFERENCES design\_moves(id) NOT NULL,  
        link\_strength FLOAT NOT NULL CHECK (link\_strength \>= 0 AND link\_strength \<= 1),  
        link\_type VARCHAR(30) NOT NULL,  
        semantic\_similarity FLOAT NOT NULL CHECK (semantic\_similarity \>= 0 AND semantic\_similarity \<= 1),  
        temporal\_distance FLOAT NOT NULL CHECK (temporal\_distance \>= 0),  
        phase\_transition BOOLEAN DEFAULT FALSE,  
        move\_type\_transition BOOLEAN DEFAULT FALSE,  
        created\_at TIMESTAMP DEFAULT CURRENT\_TIMESTAMP,  
        CONSTRAINT no\_self\_links CHECK (source\_move\_id \!= target\_move\_id)  
    );  
      
    \-- Linkography Sessions table  
    CREATE TABLE IF NOT EXISTS linkography\_sessions (  
        id UUID PRIMARY KEY DEFAULT gen\_random\_uuid(),  
        session\_id UUID REFERENCES sessions(id) NOT NULL UNIQUE,  
        total\_moves INTEGER DEFAULT 0,  
        total\_links INTEGER DEFAULT 0,  
        link\_index FLOAT DEFAULT 0.0,  
        ideation\_metrics JSONB,  
        visualization\_metrics JSONB,  
        materialization\_metrics JSONB,  
        critical\_moves JSONB,  
        move\_type\_distribution JSONB,  
        design\_focus\_distribution JSONB,  
        temporal\_patterns JSONB,  
        conceptual\_patterns JSONB,  
        complexity\_patterns JSONB,  
        last\_updated TIMESTAMP DEFAULT CURRENT\_TIMESTAMP  
    );  
      
    \-- Create indexes for performance  
    CREATE INDEX IF NOT EXISTS idx\_design\_moves\_session ON design\_moves(session\_id);  
    CREATE INDEX IF NOT EXISTS idx\_design\_moves\_timestamp ON design\_moves(timestamp);  
    CREATE INDEX IF NOT EXISTS idx\_design\_moves\_phase ON design\_moves(phase);  
    CREATE INDEX IF NOT EXISTS idx\_design\_moves\_sequence ON design\_moves(session\_id, sequence\_number);  
      
    CREATE INDEX IF NOT EXISTS idx\_linkography\_links\_source ON linkography\_links(source\_move\_id);  
    CREATE INDEX IF NOT EXISTS idx\_linkography\_links\_target ON linkography\_links(target\_move\_id);  
    CREATE INDEX IF NOT EXISTS idx\_linkography\_links\_strength ON linkography\_links(link\_strength);  
      
    CREATE INDEX IF NOT EXISTS idx\_linkography\_sessions\_session ON linkography\_sessions(session\_id);  
      
    \-- Create function for automatic linkography session updates  
    CREATE OR REPLACE FUNCTION update\_linkography\_session\_stats()  
    RETURNS TRIGGER AS $  
    BEGIN  
        INSERT INTO linkography\_sessions (session\_id, total\_moves, total\_links, link\_index, last\_updated)  
        SELECT   
            dm.session\_id,  
            COUNT(DISTINCT dm.id) as total\_moves,  
            COUNT(ll.id) as total\_links,  
            CASE   
                WHEN COUNT(DISTINCT dm.id) \> 0 THEN COUNT(ll.id)::FLOAT / COUNT(DISTINCT dm.id)  
                ELSE 0   
            END as link\_index,  
            NOW()  
        FROM design\_moves dm  
        LEFT JOIN linkography\_links ll ON dm.id \= ll.source\_move\_id  
        WHERE dm.session\_id \= COALESCE(NEW.session\_id, OLD.session\_id)  
        GROUP BY dm.session\_id  
        ON CONFLICT (session\_id) DO UPDATE SET  
            total\_moves \= EXCLUDED.total\_moves,  
            total\_links \= EXCLUDED.total\_links,  
            link\_index \= EXCLUDED.link\_index,  
            last\_updated \= EXCLUDED.last\_updated;  
          
        RETURN COALESCE(NEW, OLD);  
    END;  
    $ LANGUAGE plpgsql;  
      
    \-- Create triggers for automatic updates  
    CREATE TRIGGER trigger\_update\_linkography\_on\_move\_change  
        AFTER INSERT OR UPDATE OR DELETE ON design\_moves  
        FOR EACH ROW EXECUTE FUNCTION update\_linkography\_session\_stats();  
      
    CREATE TRIGGER trigger\_update\_linkography\_on\_link\_change  
        AFTER INSERT OR UPDATE OR DELETE ON linkography\_links  
        FOR EACH ROW EXECUTE FUNCTION update\_linkography\_session\_stats();  
    """  
      
    try:  
        async with engine.begin() as conn:  
            await conn.execute(text(migration\_sql))  
          
        logger.info("Linkography database migration completed successfully")  
          
    except Exception as e:  
        logger.error(f"Migration failed: {e}")  
        raise  
    finally:  
        await engine.dispose()

if \_\_name\_\_ \== "\_\_main\_\_":  
    asyncio.run(run\_linkography\_migration())

"""  
Database Models for MENTOR B-Tests Platform  
"""

from sqlalchemy import Column, String, DateTime, Boolean, Integer, Float, Text, JSON, ForeignKey  
from sqlalchemy.ext.declarative import declarative\_base  
from sqlalchemy.orm import relationship  
from sqlalchemy.dialects.postgresql import UUID  
import uuid  
from datetime import datetime

Base \= declarative\_base()

class Session(Base):  
    \_\_tablename\_\_ \= "sessions"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    participant\_id \= Column(String(50), nullable=False)  
    group\_assignment \= Column(String(20), nullable=False)  \# 'mentor', 'generic\_ai', 'control'  
    phase \= Column(String(20), nullable=False)  \# 'ideation', 'visualization', 'materialization'  
    start\_time \= Column(DateTime, default=datetime.utcnow)  
    end\_time \= Column(DateTime, nullable=True)  
    completed \= Column(Boolean, default=False)  
    configuration \= Column(JSON, nullable=True)  
      
    \# Relationships  
    interactions \= relationship("Interaction", back\_populates="session")  
    assessments \= relationship("Assessment", back\_populates="session")  
    design\_outputs \= relationship("DesignOutput", back\_populates="session")

class Interaction(Base):  
    \_\_tablename\_\_ \= "interactions"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
    timestamp \= Column(DateTime, default=datetime.utcnow)  
    interaction\_type \= Column(String(50), nullable=False)  
    user\_input \= Column(Text, nullable=True)  
    ai\_response \= Column(Text, nullable=True)  
    cognitive\_metrics \= Column(JSON, nullable=True)  
    metadata \= Column(JSON, nullable=True)  
    processing\_time \= Column(Integer, nullable=True)  \# milliseconds  
      
    \# Relationships  
    session \= relationship("Session", back\_populates="interactions")

class Assessment(Base):  
    \_\_tablename\_\_ \= "assessments"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
    assessment\_type \= Column(String(50), nullable=False)  \# 'critical\_thinking', 'spatial\_reasoning', etc.  
    questions \= Column(JSON, nullable=False)  
    responses \= Column(JSON, nullable=False)  
    scores \= Column(JSON, nullable=False)  
    completion\_time \= Column(Integer, nullable=False)  \# seconds  
    timestamp \= Column(DateTime, default=datetime.utcnow)  
      
    \# Relationships  
    session \= relationship("Session", back\_populates="assessments")

class DesignOutput(Base):  
    \_\_tablename\_\_ \= "design\_outputs"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
    phase \= Column(String(20), nullable=False)  
    output\_type \= Column(String(50), nullable=False)  \# 'text', 'image', '3d\_model'  
    content \= Column(Text, nullable=True)  
    file\_path \= Column(String(255), nullable=True)  
    expert\_scores \= Column(JSON, nullable=True)  
    created\_at \= Column(DateTime, default=datetime.utcnow)  
      
    \# Relationships  
    session \= relationship("Session", back\_populates="design\_outputs")

class ExpertEvaluation(Base):  
    \_\_tablename\_\_ \= "expert\_evaluations"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    design\_output\_id \= Column(UUID(as\_uuid=True), ForeignKey("design\_outputs.id"), nullable=False)  
    evaluator\_id \= Column(String(50), nullable=False)  
    creativity\_score \= Column(Float, nullable=True)  
    technical\_score \= Column(Float, nullable=True)  
    community\_score \= Column(Float, nullable=True)  
    process\_score \= Column(Float, nullable=True)  
    overall\_score \= Column(Float, nullable=True)  
    comments \= Column(Text, nullable=True)  
    evaluation\_date \= Column(DateTime, default=datetime.utcnow)

class ParticipantProfile(Base):  
    \_\_tablename\_\_ \= "participant\_profiles"  
      
    id \= Column(String(50), primary\_key=True)  
    age \= Column(Integer, nullable=True)  
    education\_level \= Column(String(50), nullable=True)  
    architecture\_experience \= Column(String(50), nullable=True)  \# 'beginner', 'intermediate', 'advanced'  
    ai\_experience \= Column(String(50), nullable=True)  
    consent\_given \= Column(Boolean, default=False)  
    created\_at \= Column(DateTime, default=datetime.utcnow)

### **7\. Configuration and Environment Setup**

**File: `config/config.yaml`**

\# MENTOR B-Tests Platform Configuration

app:  
  name: "MENTOR B-Tests Platform"  
  version: "1.0.0"  
  debug: false  
  host: "0.0.0.0"  
  port: 8000

database:  
  url: "${DATABASE\_URL}"  
  echo: false  
  pool\_size: 10  
  max\_overflow: 20

redis:  
  url: "${REDIS\_URL}"  
  db: 0

ai\_models:  
  anthropic:  
    api\_key: "${ANTHROPIC\_API\_KEY}"  
    model: "claude-3-sonnet-20240229"  
    max\_tokens: 4000  
      
  openai:  
    api\_key: "${OPENAI\_API\_KEY}"  
    model: "gpt-4-1106-preview"  
    max\_tokens: 4000

cognitive\_benchmarking:  
  weights:  
    cop: 0.20  
    dte: 0.20  
    se: 0.15  
    ki: 0.15  
    lp: 0.15  
    ma: 0.15  
    
  thresholds:  
    excellent: 0.85  
    good: 0.70  
    adequate: 0.55  
    needs\_improvement: 0.40  
    
  normalization:  
    method: "z\_score"  
    outlier\_threshold: 3.0

file\_processing:  
  upload\_dir: "uploads"  
  max\_file\_size: 10485760  \# 10MB  
  allowed\_extensions: \[".jpg", ".jpeg", ".png", ".pdf", ".dwg", ".3dm", ".step", ".iges"\]

testing:  
  session\_timeout: 7200  \# 2 hours  
  auto\_save\_interval: 30  \# seconds  
  assessment\_time\_limits:  
    critical\_thinking: 1800  \# 30 minutes  
    spatial\_reasoning: 1200  \# 20 minutes  
    architecture\_knowledge: 1500  \# 25 minutes

logging:  
  level: "INFO"  
  file: "logs/mentor\_btests.log"  
  format: "%(asctime)s \- %(name)s \- %(levelname)s \- %(message)s"

**File: `.env.example`**

\# Database Configuration  
DATABASE\_URL=postgresql://username:password@localhost:5432/mentor\_btests  
REDIS\_URL=redis://localhost:6379

\# AI API Keys  
ANTHROPIC\_API\_KEY=your\_anthropic\_api\_key\_here  
OPENAI\_API\_KEY=your\_openai\_api\_key\_here

\# Security  
SECRET\_KEY=your\_secret\_key\_here  
ALGORITHM=HS256  
ACCESS\_TOKEN\_EXPIRE\_MINUTES=30

\# Application Settings  
ENVIRONMENT=development  
DEBUG=true

\# File Storage  
UPLOAD\_PATH=./uploads  
STATIC\_PATH=./static

\# Logging  
LOG\_LEVEL=INFO  
LOG\_FILE=./logs/application.log

### **8\. Deployment Scripts**

**File: `scripts/setup_database.py`**

"""  
Database setup and initialization script  
"""

import asyncio  
import asyncpg  
import logging  
from sqlalchemy.ext.asyncio import create\_async\_engine  
from sqlalchemy import text  
import os  
from dotenv import load\_dotenv

load\_dotenv()

logging.basicConfig(level=logging.INFO)  
logger \= logging.getLogger(\_\_name\_\_)

async def create\_database():  
    """Create database if it doesn't exist"""  
      
    database\_url \= os.getenv("DATABASE\_URL")  
    if not database\_url:  
        raise ValueError("DATABASE\_URL environment variable not set")  
      
    \# Extract database name  
    db\_name \= database\_url.split("/")\[-1\]  
    base\_url \= database\_url.rsplit("/", 1)\[0\]  
      
    try:  
        \# Connect to postgres database to create target database  
        conn \= await asyncpg.connect(f"{base\_url}/postgres")  
          
        \# Check if database exists  
        exists \= await conn.fetchval(  
            "SELECT 1 FROM pg\_database WHERE datname \= $1", db\_name  
        )  
          
        if not exists:  
            await conn.execute(f'CREATE DATABASE "{db\_name}"')  
            logger.info(f"Created database: {db\_name}")  
        else:  
            logger.info(f"Database {db\_name} already exists")  
          
        await conn.close()  
          
    except Exception as e:  
        logger.error(f"Error creating database: {e}")  
        raise

async def initialize\_tables():  
    """Create all tables"""  
      
    from src.utils.models import Base  
      
    database\_url \= os.getenv("DATABASE\_URL")  
    engine \= create\_async\_engine(database\_url, echo=True)  
      
    async with engine.begin() as conn:  
        await conn.run\_sync(Base.metadata.create\_all)  
      
    logger.info("Database tables created successfully")  
    await engine.dispose()

async def main():  
    """Main setup function"""  
    logger.info("Starting database setup...")  
      
    await create\_database()  
    await initialize\_tables()  
      
    logger.info("Database setup completed successfully\!")

if \_\_name\_\_ \== "\_\_main\_\_":  
    asyncio.run(main())

**File: `scripts/run_tests.py`**

"""  
Test runner script for the MENTOR B-Tests platform  
"""

import asyncio  
import pytest  
import logging  
import sys  
import os

\# Add src to path  
sys.path.insert(0, os.path.join(os.path.dirname(\_\_file\_\_), '..', 'src'))

logging.basicConfig(level=logging.INFO)  
logger \= logging.getLogger(\_\_name\_\_)

def run\_unit\_tests():  
    """Run unit tests"""  
    logger.info("Running unit tests...")  
      
    result \= pytest.main(\[  
        "tests/unit/",  
        "-v",  
        "--tb=short",  
        "--cov=src",  
        "--cov-report=html",  
        "--cov-report=term-missing"  
    \])  
      
    return result \== 0

def run\_integration\_tests():  
    """Run integration tests"""  
    logger.info("Running integration tests...")  
      
    result \= pytest.main(\[  
        "tests/integration/",  
        "-v",   
        "--tb=short"  
    \])  
      
    return result \== 0

def run\_api\_tests():  
    """Run API tests"""  
    logger.info("Running API tests...")  
      
    result \= pytest.main(\[  
        "tests/api/",  
        "-v",  
        "--tb=short"  
    \])  
      
    return result \== 0

def main():  
    """Run all test suites"""  
    logger.info("Starting test execution...")  
      
    success \= True  
      
    \# Run unit tests  
    if not run\_unit\_tests():  
        logger.error("Unit tests failed")  
        success \= False  
      
    \# Run integration tests  
    if not run\_integration\_tests():  
        logger.error("Integration tests failed")  
        success \= False  
      
    \# Run API tests  
    if not run\_api\_tests():  
        logger.error("API tests failed")  
        success \= False  
      
    if success:  
        logger.info("All tests passed successfully\!")  
        return 0  
    else:  
        logger.error("Some tests failed")  
        return 1

if \_\_name\_\_ \== "\_\_main\_\_":  
    exit\_code \= main()  
    sys.exit(exit\_code)

### **9\. Docker Configuration**

**File: `docker-compose.yml`**

version: '3.8'

services:  
  app:  
    build: .  
    ports:  
      \- "8000:8000"  
    environment:  
      \- DATABASE\_URL=postgresql://mentor\_user:mentor\_pass@db:5432/mentor\_btests  
      \- REDIS\_URL=redis://redis:6379  
    depends\_on:  
      \- db  
      \- redis  
    volumes:  
      \- ./uploads:/app/uploads  
      \- ./logs:/app/logs  
    restart: unless-stopped

  db:  
    image: postgres:14  
    environment:  
      \- POSTGRES\_DB=mentor\_btests  
      \- POSTGRES\_USER=mentor\_user  
      \- POSTGRES\_PASSWORD=mentor\_pass  
    volumes:  
      \- postgres\_data:/var/lib/postgresql/data  
    ports:  
      \- "5432:5432"

  redis:  
    image: redis:7-alpine  
    ports:  
      \- "6379:6379"  
    volumes:  
      \- redis\_data:/data

  streamlit:  
    build:  
      context: .  
      dockerfile: Dockerfile.streamlit  
    ports:  
      \- "8501:8501"  
    environment:  
      \- API\_BASE\_URL=http://app:8000  
    depends\_on:  
      \- app

volumes:  
  postgres\_data:  
  redis\_data:

**File: `Dockerfile`**

FROM python:3.11-slim

WORKDIR /app

\# Install system dependencies  
RUN apt-get update && apt-get install \-y \\  
    gcc \\  
    g++ \\  
    libpq-dev \\  
    && rm \-rf /var/lib/apt/lists/\*

\# Copy requirements and install Python dependencies  
COPY requirements.txt .  
RUN pip install \--no-cache-dir \-r requirements.txt

\# Copy application code  
COPY src/ ./src/  
COPY config/ ./config/  
COPY scripts/ ./scripts/

\# Create necessary directories  
RUN mkdir \-p uploads logs static

\# Set environment variables  
ENV PYTHONPATH=/app  
ENV PYTHONUNBUFFERED=1

\# Expose port  
EXPOSE 8000

\# Health check  
HEALTHCHECK \--interval=30s \--timeout=10s \--start-period=60s \--retries=3 \\  
    CMD curl \-f http://localhost:8000/health || exit 1

\# Run application  
CMD \["python", "-m", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"\]

---

## **Deployment Instructions for Claude Code**

### **Step 1: Environment Setup**

\# Clone or create project directory  
mkdir mentor-btests-platform  
cd mentor-btests-platform

\# Initialize virtual environment  
python \-m venv venv  
source venv/bin/activate  \# On Windows: venv\\Scripts\\activate

\# Create basic structure  
mkdir \-p {src/{api,ui,benchmarking,assessment,data\_processing,utils},tests/{unit,integration,api},data,docs,config,scripts,uploads,logs,static}

### **Step 2: Install Dependencies**

\# Install all requirements  
pip install \-r requirements.txt

\# Set up pre-commit hooks (optional but recommended)  
pip install pre-commit  
pre-commit install

### **Step 3: Configuration**

\# Copy environment template  
cp .env.example .env

\# Edit .env with your actual values  
\# Add your API keys and database credentials

### **Step 4: Database Setup**

\# Run database setup script  
python scripts/setup\_database.py

\# Verify database connection  
python \-c "  
import asyncio  
from src.utils.database import get\_db\_session  
asyncio.run(get\_db\_session().\_\_anext\_\_())  
print('Database connection successful\!')  
"

### **Step 5: Run Tests**

\# Run test suite  
python scripts/run\_tests.py

\# Or run specific test categories  
pytest tests/unit/ \-v  
pytest tests/integration/ \-v  
pytest tests/api/ \-v

### **Step 6: Start Application**

\# Start the main API server  
uvicorn src.main:app \--reload \--host 0.0.0.0 \--port 8000

\# In another terminal, start Streamlit UI  
streamlit run src/ui/streamlit\_app.py \--server.port 8501

\# Or use Docker Compose  
docker-compose up \--build

### **Step 7: Verification**

\# Test API endpoints  
curl http://localhost:8000/health  
curl http://localhost:8000/api/v1/assessments/critical-thinking

\# Access Streamlit interface  
\# Navigate to http://localhost:8501 in browser

### **Step 8: Production Deployment**

\# Build production Docker image  
docker build \-t mentor-btests:latest .

\# Deploy with environment-specific configuration  
docker-compose \-f docker-compose.prod.yml up \-d

\# Set up reverse proxy (nginx/traefik)  
\# Configure SSL certificates  
\# Set up monitoring and logging

---

## **Usage Instructions for Testing**

### **Creating a Test Session**

import requests

\# Create new session  
session\_data \= {  
    "participant\_id": "PART\_001",  
    "group\_assignment": "mentor",  \# or "generic\_ai", "control"  
    "phase": "ideation",  
    "configuration": {  
        "test\_type": "comprehensive",  
        "difficulty": "intermediate"  
    }  
}

response \= requests.post("http://localhost:8000/api/v1/sessions/create", json=session\_data)  
session\_id \= response.json()\["session\_id"\]

### **Running an Assessment**

\# Get critical thinking test  
ct\_test \= requests.get("http://localhost:8000/api/v1/assessments/critical-thinking?difficulty=mixed\&num\_questions=15").json()

\# Submit responses  
test\_responses \= {  
    "test\_id": ct\_test\["test\_id"\],  
    "responses": \[  
        {"question\_id": "ct\_001", "answer": "Strong argument with assumption about community needs"},  
        \# ... more responses  
    \],  
    "completion\_time": 1425  \# seconds  
}

scores \= requests.post("http://localhost:8000/api/v1/assessments/critical-thinking/score", json=test\_responses).json()

### **Processing Interactions**

\# Send user interaction  
interaction\_data \= {  
    "user\_input": "How should I approach the spatial organization of this community center?",  
    "phase": "ideation",  
    "type": "question",  
    "metadata": {"complexity\_level": "high"}  
}

response \= requests.post(f"http://localhost:8000/api/v1/sessions/{session\_id}/interactions", json=interaction\_data).json()

print(f"AI Response: {response\['ai\_response'\]}")  
print(f"Cognitive Metrics: {response\['cognitive\_metrics'\]}")

This comprehensive implementation provides a robust foundation for your MENTOR B-Tests platform, integrating all the cognitive benchmarking capabilities with a scalable, maintainable architecture optimized for Claude Code development.