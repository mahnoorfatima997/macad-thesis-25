# **No AI Control Group Implementation Guide \- Group 3**

## **Overview for Claude Code Integration**

This implementation guide provides comprehensive instructions for creating the baseline control testing platform using Claude Code in Visual Studio Code. The implementation focuses on capturing pure, unassisted design thinking processes through linkographic analysis while maintaining complete independence from AI assistance. This platform serves as the essential baseline for comparing cognitive development across all three experimental groups.

---

## **Project Initialization Commands**

### **1\. Baseline Control Setup**

\# Initialize baseline control project structure  
mkdir no-ai-control-baseline-platform  
cd no-ai-control-baseline-platform

\# Create Python virtual environment  
python \-m venv venv  
source venv/bin/activate  \# On Windows: venv\\Scripts\\activate

\# Initialize git repository  
git init  
echo "\# No AI Control Baseline Platform" \> README.md  
git add README.md  
git commit \-m "Initial commit \- baseline control platform"

\# Create comprehensive directory structure for baseline testing  
mkdir \-p {src/{api,ui,baseline\_interface,linkography,benchmarking,assessment,data\_processing,utils},tests,data/{raw,processed,baseline\_linkography,exports},docs,config,scripts,models/{baseline\_linkography,baseline\_cognitive}}

### **2\. Dependencies Installation for Baseline Platform**

\# Core dependencies (minimal AI-related packages)  
pip install fastapi uvicorn sqlalchemy psycopg2-binary redis pandas numpy scipy scikit-learn  
pip install streamlit plotly seaborn matplotlib opencv-python Pillow  
pip install pytest pytest-asyncio black flake8 mypy  
pip install python-dotenv pydantic alembic asyncpg

\# Linkography and analysis dependencies (no AI models)  
pip install networkx spacy nltk  
pip install python-igraph  \# For network analysis  
pip install textstat nltk  \# Text complexity analysis  
pip install opencv-python pytesseract  \# For sketch analysis

\# Minimal NLP for move parsing (no AI inference)  
python \-m spacy download en\_core\_web\_sm  
python \-m nltk.downloader punkt stopwords

\# Create requirements.txt  
pip freeze \> requirements.txt

---

## **Core Implementation Files for Baseline Control**

### **1\. Baseline Database Schema**

**File: `src/data_processing/baseline_database_models.py`**

"""  
Baseline Database Models for No AI Control Group  
Captures natural design thinking without AI assistance  
"""

from sqlalchemy import Column, String, Integer, Float, Boolean, DateTime, Text, JSON, ForeignKey  
from sqlalchemy.dialects.postgresql import UUID  
from sqlalchemy.ext.declarative import declarative\_base  
from sqlalchemy.orm import relationship  
import uuid  
from datetime import datetime

Base \= declarative\_base()

class BaselineDesignMove(Base):  
    """Pure self-generated design moves without AI influence"""  
    \_\_tablename\_\_ \= "baseline\_design\_moves"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
    interaction\_id \= Column(UUID(as\_uuid=True), ForeignKey("interactions.id"), nullable=True)  
      
    \# Basic move information  
    move\_id \= Column(String(50), nullable=False, unique=True)  
    sequence\_number \= Column(Integer, nullable=False)  
    timestamp \= Column(DateTime, default=datetime.utcnow)  
      
    \# Move content and classification  
    content \= Column(Text, nullable=False)  
    move\_type \= Column(String(20), nullable=False)  \# analysis|synthesis|evaluation|transformation|reflection  
    phase \= Column(String(20), nullable=False)  \# ideation|visualization|materialization  
    modality \= Column(String(20), nullable=False)  \# text|sketch|image|voice|upload  
    cognitive\_operation \= Column(String(20), nullable=False)  \# proposal|clarification|assessment|support|reference  
    design\_focus \= Column(String(20), nullable=False)  \# function|form|structure|material|environment|culture  
      
    \# Baseline-specific classification  
    move\_source \= Column(String(20), nullable=False, default='self\_generated')  \# self\_generated|resource\_referenced|platform\_prompted  
    cognitive\_load \= Column(String(10), nullable=False)  \# high|medium|low  
    independence\_level \= Column(String(20), nullable=False, default='fully\_autonomous')  \# fully\_autonomous|resource\_assisted|platform\_guided  
      
    \# Natural thinking metrics  
    self\_generation\_strength \= Column(Float, default=1.0)  \# Always 1.0 for baseline  
    reasoning\_depth \= Column(Float, default=0.0)  \# Self-assessed reasoning depth  
    self\_evaluation\_frequency \= Column(Integer, default=0)  \# Count of self-assessments  
    resource\_dependency \= Column(Float, default=0.0)  \# Dependency on reference materials  
    independent\_synthesis \= Column(Boolean, default=True)  \# Independent connection making  
    natural\_progression\_indicator \= Column(Float, default=1.0)  \# Natural flow score  
      
    \# Complexity and quality metrics (baseline)  
    linguistic\_complexity \= Column(Float, default=0.0)  
    technical\_depth \= Column(Float, default=0.0)  
    innovation\_score \= Column(Float, default=0.0)  
    self\_confidence\_level \= Column(Float, default=0.0)  \# Self-reported confidence  
      
    \# Multimodal context (no AI annotations)  
    concurrent\_sketch\_id \= Column(String(100), nullable=True)  
    manual\_annotations \= Column(JSON, nullable=True)  \# User-provided annotations only  
    spatial\_coordinates \= Column(JSON, nullable=True)  
    interaction\_context \= Column(JSON, nullable=True)  
      
    created\_at \= Column(DateTime, default=datetime.utcnow)

class BaselineLinkographicLink(Base):  
    """Natural links between design moves without AI mediation"""  
    \_\_tablename\_\_ \= "baseline\_linkographic\_links"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
      
    \# Link endpoints  
    source\_move\_id \= Column(String(50), ForeignKey("baseline\_design\_moves.move\_id"), nullable=False)  
    target\_move\_id \= Column(String(50), ForeignKey("baseline\_design\_moves.move\_id"), nullable=False)  
      
    \# Natural link classification  
    link\_type \= Column(String(20), nullable=False)  \# semantic|temporal|causal|refinement|contradiction  
    link\_strength \= Column(Float, nullable=False)  \# 0.0 to 1.0  
    semantic\_similarity \= Column(Float, default=0.0)  \# Natural semantic connection  
    temporal\_distance \= Column(Integer, default=0)  \# Moves apart  
    conceptual\_distance \= Column(Float, default=0.0)  \# Natural conceptual distance  
      
    \# Baseline link characteristics  
    ai\_mediated \= Column(Boolean, default=False)  \# Always False for baseline  
    self\_identified \= Column(Boolean, default=True)  \# Participant identified connection  
    cross\_modal \= Column(Boolean, default=False)  \# Links across modalities  
    phase\_bridging \= Column(Boolean, default=False)  \# Links across phases  
      
    \# Natural link quality  
    critical\_link \= Column(Boolean, default=False)  \# High-impact natural link  
    innovation\_link \= Column(Boolean, default=False)  \# Novel natural connection  
    integration\_link \= Column(Boolean, default=False)  \# Natural knowledge integration  
    self\_reflection\_link \= Column(Boolean, default=False)  \# Self-reflective connection  
      
    \# Context and metadata  
    detection\_method \= Column(String(30), default='natural')  \# natural|participant\_identified  
    confidence\_score \= Column(Float, default=1.0)  \# Natural connection confidence  
    link\_metadata \= Column(JSON, nullable=True)  
      
    created\_at \= Column(DateTime, default=datetime.utcnow)

class BaselineSession(Base):  
    """Session-level baseline analysis without AI influence"""  
    \_\_tablename\_\_ \= "baseline\_sessions"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
      
    \# Basic baseline metrics  
    total\_moves \= Column(Integer, default=0)  
    total\_links \= Column(Integer, default=0)  
    natural\_link\_index \= Column(Float, default=0.0)  \# Natural links/moves ratio  
      
    \# Pure self-generation metrics  
    self\_generated\_moves \= Column(Integer, default=0)  \# Should equal total\_moves  
    resource\_referenced\_moves \= Column(Integer, default=0)  
    platform\_prompted\_moves \= Column(Integer, default=0)  
      
    \# Independence indicators  
    full\_autonomy\_ratio \= Column(Float, default=1.0)  \# Proportion of fully autonomous moves  
    resource\_dependency\_level \= Column(Float, default=0.0)  
    self\_scaffolding\_effectiveness \= Column(Float, default=0.0)  
      
    \# Natural network metrics  
    natural\_critical\_move\_count \= Column(Integer, default=0)  
    independent\_critical\_density \= Column(Float, default=0.0)  
    natural\_cross\_phase\_links \= Column(Integer, default=0)  
    self\_directed\_semantic\_density \= Column(Float, default=0.0)  
    autonomous\_temporal\_flow \= Column(Float, default=0.0)  
      
    \# Baseline cognitive development indicators  
    natural\_concept\_synthesis \= Column(Float, default=0.0)  
    independent\_evaluation\_sophistication \= Column(Float, default=0.0)  
    spontaneous\_metacognitive\_awareness \= Column(Float, default=0.0)  
    autonomous\_knowledge\_integration \= Column(Float, default=0.0)  
      
    \# Self-reported metrics  
    perceived\_difficulty \= Column(Float, default=0.0)  \# User-reported difficulty  
    self\_assessed\_confidence \= Column(Float, default=0.0)  \# User confidence rating  
    resource\_utilization\_rating \= Column(Float, default=0.0)  \# How much they used references  
      
    \# Analysis metadata  
    analysis\_timestamp \= Column(DateTime, default=datetime.utcnow)  
    analysis\_version \= Column(String(20), default='baseline\_1.0')  
    quality\_flags \= Column(JSON, nullable=True)  
      
    created\_at \= Column(DateTime, default=datetime.utcnow)

class BaselineInteraction(Base):  
    """Pure user interactions without AI assistance"""  
    \_\_tablename\_\_ \= "baseline\_interactions"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
      
    \# Interaction basics  
    interaction\_type \= Column(String(20), nullable=False)  \# text\_input|sketch|upload|navigation  
    user\_input \= Column(Text, nullable=False)  
    system\_response \= Column(Text, nullable=True)  \# Minimal acknowledgment only  
    timestamp \= Column(DateTime, default=datetime.utcnow)  
      
    \# Baseline interaction metrics  
    input\_length \= Column(Integer, default=0)  
    thinking\_pause\_duration \= Column(Float, default=0.0)  \# Time between interactions  
    self\_correction\_count \= Column(Integer, default=0)  
    hesitation\_indicators \= Column(JSON, nullable=True)  \# Detected uncertainty  
      
    \# Independence verification  
    external\_assistance\_requested \= Column(Boolean, default=False)  \# Should always be False  
    resource\_consultation \= Column(Boolean, default=False)  \# Used reference materials  
    platform\_guidance\_received \= Column(Boolean, default=False)  \# Should always be False  
      
    \# Generated move counts  
    moves\_generated \= Column(Integer, default=0)  
    self\_evaluations\_included \= Column(Integer, default=0)  
    cross\_references\_made \= Column(Integer, default=0)  
      
    \# Context  
    phase \= Column(String(20), nullable=False)  
    interaction\_metadata \= Column(JSON, nullable=True)  
      
    created\_at \= Column(DateTime, default=datetime.utcnow)

### **2\. Natural Move Parser for Baseline Control**

**File: `src/linkography/natural_move_parser.py`**

"""  
Natural Move Parser for Baseline Control Group  
Extracts and classifies pure self-generated design moves  
"""

import re  
import spacy  
import numpy as np  
from typing import List, Dict, Any, Tuple  
from datetime import datetime  
import json  
import logging

logger \= logging.getLogger(\_\_name\_\_)

class NaturalMoveParser:  
    """Parse natural design moves without AI influence"""  
      
    def \_\_init\_\_(self):  
        self.nlp \= spacy.load("en\_core\_web\_sm")  
          
        \# Move classification patterns (same as other groups for consistency)  
        self.move\_patterns \= {  
            'analysis': \[  
                r'analyze|examine|evaluate|assess|consider|investigate',  
                r'what.\*(?:problems?|issues?|challenges?)',  
                r'how.\*(?:works?|functions?|performs?)'  
            \],  
            'synthesis': \[  
                r'combine|integrate|merge|unify|synthesize',  
                r'bring together|put together|connect',  
                r'relationship between|connection.\*between'  
            \],  
            'evaluation': \[  
                r'judge|rate|score|rank|compare|contrast',  
                r'better|worse|superior|inferior|optimal',  
                r'pros? and cons?|advantages? and disadvantages?'  
            \],  
            'transformation': \[  
                r'modify|change|alter|adapt|transform|convert',  
                r'instead of|rather than|alternative',  
                r'redesign|rework|revise'  
            \],  
            'reflection': \[  
                r'think about|reflect on|consider',  
                r'what if|suppose|imagine',  
                r'learn.\*from|takeaway|insight'  
            \]  
        }  
          
        \# Self-generated move indicators  
        self.self\_generation\_patterns \= {  
            'autonomous\_reasoning': \[  
                r'I think|I believe|I feel|my approach',  
                r'in my opinion|from my perspective|I would',  
                r'let me think|I need to consider'  
            \],  
            'self\_evaluation': \[  
                r'this might work|this could be|I\\'m not sure',  
                r'maybe|perhaps|possibly|potentially',  
                r'I should reconsider|let me rethink'  
            \],  
            'independent\_discovery': \[  
                r'I realize|I notice|I see that',  
                r'this makes me think|this suggests',  
                r'I understand|I recognize'  
            \]  
        }  
          
        \# Design focus categories (same as other groups)  
        self.design\_focus\_patterns \= {  
            'function': \[r'function|purpose|use|activity|program|space'\],  
            'form': \[r'shape|form|geometry|proportion|scale|massing'\],  
            'structure': \[r'structure|structural|support|load|beam|column'\],  
            'material': \[r'material|concrete|steel|wood|glass|facade'\],  
            'environment': \[r'environment|climate|energy|sustainable|green'\],  
            'culture': \[r'culture|community|social|cultural|neighborhood'\]  
        }  
          
        \# Resource reference indicators  
        self.resource\_reference\_patterns \= \[  
            r'according to|based on|from the reference',  
            r'the example shows|as seen in|similar to',  
            r'research indicates|precedent suggests'  
        \]  
      
    async def extract\_natural\_moves\_from\_input(self,   
                                             user\_input: str,  
                                             interaction\_context: Dict\[str, Any\]) \-\> List\[Dict\[str, Any\]\]:  
        """Extract pure self-generated design moves from user input"""  
          
        moves \= \[\]  
          
        \# Parse user input for natural moves  
        natural\_moves \= await self.\_parse\_text\_for\_natural\_moves(  
            user\_input,   
            context=interaction\_context  
        )  
        moves.extend(natural\_moves)  
          
        \# Analyze self-generation patterns  
        moves \= await self.\_analyze\_self\_generation\_patterns(moves, user\_input)  
          
        return moves  
      
    async def \_parse\_text\_for\_natural\_moves(self,   
                                          text: str,   
                                          context: Dict\[str, Any\]) \-\> List\[Dict\[str, Any\]\]:  
        """Parse text into discrete natural design moves"""  
          
        doc \= self.nlp(text)  
        sentences \= \[sent.text.strip() for sent in doc.sents if sent.text.strip()\]  
          
        moves \= \[\]  
        for i, sentence in enumerate(sentences):  
            move \= await self.\_classify\_natural\_move(sentence, context)  
            move\['sequence\_number'\] \= context.get('base\_sequence', 0\) \+ i  
            move\['timestamp'\] \= datetime.now().isoformat()  
            moves.append(move)  
          
        return moves  
      
    async def \_classify\_natural\_move(self,   
                                   sentence: str,   
                                   context: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Classify individual natural move"""  
          
        sentence\_lower \= sentence.lower()  
          
        \# Determine move type (same logic as other groups)  
        move\_type \= 'analysis'  \# default  
        for type\_name, patterns in self.move\_patterns.items():  
            if any(re.search(pattern, sentence\_lower) for pattern in patterns):  
                move\_type \= type\_name  
                break  
          
        \# Determine design focus  
        design\_focus \= 'function'  \# default  
        for focus\_name, patterns in self.design\_focus\_patterns.items():  
            if any(re.search(pattern, sentence\_lower) for pattern in patterns):  
                design\_focus \= focus\_name  
                break  
          
        \# Determine move source (baseline specific)  
        move\_source \= await self.\_determine\_baseline\_move\_source(sentence)  
          
        \# Calculate baseline-specific metrics  
        baseline\_metrics \= await self.\_calculate\_baseline\_move\_metrics(sentence)  
          
        \# Determine cognitive operation  
        cognitive\_operation \= await self.\_determine\_cognitive\_operation(sentence)  
          
        move \= {  
            'move\_id': f"baseline\_move\_{datetime.now().timestamp()}\_{hash(sentence) % 10000}",  
            'content': sentence,  
            'move\_type': move\_type,  
            'phase': context.get('phase', 'ideation'),  
            'modality': context.get('modality', 'text'),  
            'cognitive\_operation': cognitive\_operation,  
            'design\_focus': design\_focus,  
            'move\_source': move\_source,  
            'cognitive\_load': baseline\_metrics\['cognitive\_load'\],  
            'independence\_level': baseline\_metrics\['independence\_level'\],  
            'self\_generation\_strength': 1.0,  \# Always 1.0 for baseline  
            'reasoning\_depth': baseline\_metrics\['reasoning\_depth'\],  
            'self\_evaluation\_frequency': baseline\_metrics\['self\_evaluation\_count'\],  
            'resource\_dependency': baseline\_metrics\['resource\_dependency'\],  
            'independent\_synthesis': baseline\_metrics\['independent\_synthesis'\],  
            'natural\_progression\_indicator': baseline\_metrics\['natural\_progression'\],  
            'linguistic\_complexity': baseline\_metrics\['linguistic\_complexity'\],  
            'technical\_depth': baseline\_metrics\['technical\_depth'\],  
            'self\_confidence\_level': baseline\_metrics\['confidence\_level'\],  
            'context': context  
        }  
          
        return move  
      
    async def \_determine\_baseline\_move\_source(self, sentence: str) \-\> str:  
        """Determine source of baseline move"""  
          
        sentence\_lower \= sentence.lower()  
          
        \# Check for resource references  
        for pattern in self.resource\_reference\_patterns:  
            if re.search(pattern, sentence\_lower):  
                return 'resource\_referenced'  
          
        \# Check for platform prompting (minimal in baseline)  
        if any(phrase in sentence\_lower for phrase in \['as requested', 'to answer the question'\]):  
            return 'platform\_prompted'  
          
        \# Default to self-generated for baseline  
        return 'self\_generated'  
      
    async def \_calculate\_baseline\_move\_metrics(self, sentence: str) \-\> Dict\[str, Any\]:  
        """Calculate baseline-specific metrics for move"""  
          
        import textstat  
          
        sentence\_lower \= sentence.lower()  
          
        \# Linguistic complexity  
        flesch\_score \= textstat.flesch\_reading\_ease(sentence)  
        linguistic\_complexity \= max(0, (100 \- flesch\_score) / 100\)  
          
        \# Cognitive load assessment  
        cognitive\_load \= 'low'  
        if any(word in sentence\_lower for word in \['analyze', 'synthesize', 'evaluate', 'integrate', 'optimize'\]):  
            cognitive\_load \= 'high'  
        elif any(word in sentence\_lower for word in \['consider', 'compare', 'assess', 'relate'\]):  
            cognitive\_load \= 'medium'  
          
        \# Independence level (always high for baseline)  
        independence\_level \= 'fully\_autonomous'  
        if any(pattern in sentence\_lower for pattern in self.resource\_reference\_patterns):  
            independence\_level \= 'resource\_assisted'  
          
        \# Reasoning depth indicators  
        reasoning\_indicators \= \['because', 'therefore', 'since', 'due to', 'as a result', 'consequently'\]  
        reasoning\_depth \= sum(1 for indicator in reasoning\_indicators if indicator in sentence\_lower) / len(sentence.split())  
          
        \# Self-evaluation frequency  
        self\_eval\_indicators \= \['I think', 'maybe', 'perhaps', 'might', 'could be', 'not sure'\]  
        self\_evaluation\_count \= sum(1 for indicator in self\_eval\_indicators if indicator in sentence\_lower)  
          
        \# Resource dependency  
        resource\_dependency \= 0.1 if any(pattern in sentence\_lower for pattern in self.resource\_reference\_patterns) else 0.0  
          
        \# Independent synthesis indicators  
        synthesis\_indicators \= \['connect', 'combine', 'integrate', 'relationship', 'together'\]  
        independent\_synthesis \= any(indicator in sentence\_lower for indicator in synthesis\_indicators)  
          
        \# Natural progression (flow indicators)  
        flow\_indicators \= \['then', 'next', 'also', 'additionally', 'furthermore', 'building on'\]  
        natural\_progression \= min(1.0, sum(1 for indicator in flow\_indicators if indicator in sentence\_lower) / 2\)  
          
        \# Technical depth  
        technical\_terms \= \['structural', 'mechanical', 'environmental', 'sustainable', 'energy', 'load', 'thermal'\]  
        technical\_depth \= sum(1 for term in technical\_terms if term in sentence\_lower) / len(technical\_terms)  
          
        \# Self-confidence indicators  
        confidence\_indicators \= \['confident', 'certain', 'sure', 'definitely', 'clearly'\]  
        uncertainty\_indicators \= \['unsure', 'uncertain', 'maybe', 'perhaps', 'might'\]  
        confidence\_score \= len(\[i for i in confidence\_indicators if i in sentence\_lower\])  
        uncertainty\_score \= len(\[i for i in uncertainty\_indicators if i in sentence\_lower\])  
        confidence\_level \= max(0, min(1, (confidence\_score \- uncertainty\_score) / 2 \+ 0.5))  
          
        return {  
            'cognitive\_load': cognitive\_load,  
            'independence\_level': independence\_level,  
            'reasoning\_depth': reasoning\_depth,  
            'self\_evaluation\_count': self\_evaluation\_count,  
            'resource\_dependency': resource\_dependency,  
            'independent\_synthesis': independent\_synthesis,  
            'natural\_progression': natural\_progression,  
            'linguistic\_complexity': linguistic\_complexity,  
            'technical\_depth': technical\_depth,  
            'confidence\_level': confidence\_level  
        }  
      
    async def \_determine\_cognitive\_operation(self, sentence: str) \-\> str:  
        """Determine the cognitive operation (same as other groups)"""  
          
        sentence\_lower \= sentence.lower()  
          
        if any(word in sentence\_lower for word in \['propose', 'suggest', 'design', 'create'\]):  
            return 'proposal'  
        elif any(word in sentence\_lower for word in \['clarify', 'explain', 'define', 'describe'\]):  
            return 'clarification'  
        elif any(word in sentence\_lower for word in \['assess', 'evaluate', 'judge', 'critique'\]):  
            return 'assessment'  
        elif any(word in sentence\_lower for word in \['support', 'evidence', 'because', 'therefore'\]):  
            return 'support'  
        elif any(word in sentence\_lower for word in \['reference', 'example', 'precedent', 'case'\]):  
            return 'reference'  
        else:  
            return 'proposal'  \# default  
      
    async def \_analyze\_self\_generation\_patterns(self,   
                                              moves: List\[Dict\[str, Any\]\],   
                                              user\_input: str) \-\> List\[Dict\[str, Any\]\]:  
        """Analyze patterns of self-generation in moves"""  
          
        \# Count self-generation indicators  
        autonomous\_count \= 0  
        self\_eval\_count \= 0  
        discovery\_count \= 0  
          
        user\_input\_lower \= user\_input.lower()  
          
        for pattern\_type, patterns in self.self\_generation\_patterns.items():  
            pattern\_count \= sum(1 for pattern in patterns if re.search(pattern, user\_input\_lower))  
            if pattern\_type \== 'autonomous\_reasoning':  
                autonomous\_count \= pattern\_count  
            elif pattern\_type \== 'self\_evaluation':  
                self\_eval\_count \= pattern\_count  
            elif pattern\_type \== 'independent\_discovery':  
                discovery\_count \= pattern\_count  
          
        \# Update moves with self-generation analysis  
        for move in moves:  
            move\['autonomous\_reasoning\_indicators'\] \= autonomous\_count  
            move\['self\_evaluation\_indicators'\] \= self\_eval\_count  
            move\['independent\_discovery\_indicators'\] \= discovery\_count  
            move\['overall\_self\_generation\_score'\] \= min(1.0, (autonomous\_count \+ self\_eval\_count \+ discovery\_count) / 3\)  
          
        return moves

### **3\. Baseline Linkographic Analyzer**

**File: `src/linkography/baseline_linkographic_analyzer.py`**

"""  
Baseline Linkographic Analyzer for No AI Control Group  
Analyzes natural design thinking patterns without AI influence  
"""

import numpy as np  
import networkx as nx  
from typing import List, Dict, Any, Tuple  
from sklearn.metrics.pairwise import cosine\_similarity  
from datetime import datetime  
import json  
import logging

logger \= logging.getLogger(\_\_name\_\_)

class BaselineLinkographicAnalyzer:  
    """Analyze natural linkographic patterns without AI influence"""  
      
    def \_\_init\_\_(self):  
        \# Use same sentence model as other groups for consistency  
        try:  
            from sentence\_transformers import SentenceTransformer  
            self.sentence\_model \= SentenceTransformer('all-MiniLM-L6-v2')  
        except ImportError:  
            logger.warning("SentenceTransformer not available, using basic similarity")  
            self.sentence\_model \= None  
          
        self.semantic\_threshold \= 0.3  \# Same as other groups  
        self.temporal\_window \= 10  \# Same as other groups  
          
    async def analyze\_baseline\_session\_linkography(self,   
                                                 moves: List\[Dict\[str, Any\]\],   
                                                 session\_context: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Complete baseline linkographic analysis for a session"""  
          
        \# Calculate natural links  
        links \= await self.\_calculate\_natural\_links(moves)  
          
        \# Build baseline network  
        network \= await self.\_build\_baseline\_network(moves, links)  
          
        \# Calculate baseline network metrics  
        network\_metrics \= await self.\_calculate\_baseline\_metrics(network, moves, links)  
          
        \# Analyze natural patterns (no AI influence)  
        natural\_patterns \= await self.\_analyze\_natural\_patterns(moves, links, network)  
          
        \# Identify naturally critical moves  
        critical\_moves \= await self.\_identify\_natural\_critical\_moves(moves, links, network)  
          
        \# Calculate baseline cognitive development  
        cognitive\_indicators \= await self.\_calculate\_baseline\_cognitive\_indicators(  
            moves, links, network, session\_context  
        )  
          
        return {  
            'moves': moves,  
            'links': links,  
            'network': network,  
            'baseline\_network\_metrics': network\_metrics,  
            'natural\_patterns': natural\_patterns,  
            'natural\_critical\_moves': critical\_moves,  
            'baseline\_cognitive\_indicators': cognitive\_indicators,  
            'baseline\_session\_summary': await self.\_generate\_baseline\_summary(  
                moves, links, network\_metrics, natural\_patterns  
            )  
        }  
      
    async def \_calculate\_natural\_links(self, moves: List\[Dict\[str, Any\]\]) \-\> List\[Dict\[str, Any\]\]:  
        """Calculate natural semantic and temporal links"""  
          
        links \= \[\]  
          
        if not moves or len(moves) \< 2:  
            return links  
          
        \# Extract move contents for similarity calculation  
        move\_contents \= \[move\['content'\] for move in moves\]  
          
        \# Calculate embeddings if available  
        if self.sentence\_model:  
            embeddings \= self.sentence\_model.encode(move\_contents)  
            similarity\_matrix \= cosine\_similarity(embeddings)  
        else:  
            \# Fallback to simple word overlap  
            similarity\_matrix \= await self.\_calculate\_word\_overlap\_similarity(move\_contents)  
          
        for i, move\_i in enumerate(moves):  
            for j, move\_j in enumerate(moves):  
                if i \>= j:  \# Only forward links  
                    continue  
                  
                \# Calculate semantic similarity  
                semantic\_sim \= similarity\_matrix\[i\]\[j\] if hasattr(similarity\_matrix, 'shape') else similarity\_matrix\[i\]\[j\]  
                  
                \# Calculate temporal distance  
                temporal\_distance\_moves \= j \- i  
                  
                \# Create link if meets threshold  
                if semantic\_sim \>= self.semantic\_threshold or temporal\_distance\_moves \<= 3:  
                    link \= await self.\_create\_natural\_link(  
                        move\_i, move\_j, semantic\_sim, temporal\_distance\_moves  
                    )  
                    links.append(link)  
          
        return links  
      
    async def \_calculate\_word\_overlap\_similarity(self, texts: List\[str\]) \-\> List\[List\[float\]\]:  
        """Fallback similarity calculation using word overlap"""  
          
        n \= len(texts)  
        similarity\_matrix \= \[\[0.0 for \_ in range(n)\] for \_ in range(n)\]  
          
        for i in range(n):  
            for j in range(n):  
                if i \== j:  
                    similarity\_matrix\[i\]\[j\] \= 1.0  
                else:  
                    words\_i \= set(texts\[i\].lower().split())  
                    words\_j \= set(texts\[j\].lower().split())  
                    intersection \= len(words\_i & words\_j)  
                    union \= len(words\_i | words\_j)  
                    similarity\_matrix\[i\]\[j\] \= intersection / union if union \> 0 else 0.0  
          
        return similarity\_matrix  
      
    async def \_create\_natural\_link(self,   
                                 source\_move: Dict\[str, Any\],   
                                 target\_move: Dict\[str, Any\],  
                                 semantic\_similarity: float,  
                                 temporal\_distance: int) \-\> Dict\[str, Any\]:  
        """Create a natural link between moves"""  
          
        \# Classify link type  
        link\_type \= await self.\_classify\_natural\_link\_type(source\_move, target\_move, semantic\_similarity)  
          
        \# Calculate natural link strength  
        link\_strength \= await self.\_calculate\_natural\_link\_strength(  
            source\_move, target\_move, semantic\_similarity, temporal\_distance  
        )  
          
        \# Check for natural patterns  
        cross\_modal \= source\_move\['modality'\] \!= target\_move\['modality'\]  
        phase\_bridging \= source\_move\['phase'\] \!= target\_move\['phase'\]  
        self\_identified \= await self.\_is\_self\_identified\_link(source\_move, target\_move)  
          
        link \= {  
            'link\_id': f"baseline\_link\_{source\_move\['move\_id'\]}\_{target\_move\['move\_id'\]}",  
            'source\_move\_id': source\_move\['move\_id'\],  
            'target\_move\_id': target\_move\['move\_id'\],  
            'link\_type': link\_type,  
            'link\_strength': link\_strength,  
            'semantic\_similarity': semantic\_similarity,  
            'temporal\_distance': temporal\_distance,  
            'conceptual\_distance': 1.0 \- semantic\_similarity,  
            'ai\_mediated': False,  \# Always False for baseline  
            'self\_identified': self\_identified,  
            'cross\_modal': cross\_modal,  
            'phase\_bridging': phase\_bridging,  
            'detection\_method': 'natural',  
            'confidence\_score': semantic\_similarity,  
            'created\_at': datetime.now().isoformat()  
        }  
          
        return link  
      
    async def \_classify\_natural\_link\_type(self,   
                                        source\_move: Dict\[str, Any\],   
                                        target\_move: Dict\[str, Any\],  
                                        semantic\_similarity: float) \-\> str:  
        """Classify natural link type"""  
          
        \# Same logic as other groups but with natural emphasis  
        temporal\_distance \= abs(int(target\_move\['sequence\_number'\]) \- int(source\_move\['sequence\_number'\]))  
          
        if temporal\_distance \<= 1:  
            return 'temporal'  
        elif semantic\_similarity \> 0.7:  
            if (source\_move\['move\_type'\] \== target\_move\['move\_type'\] and  
                source\_move\['design\_focus'\] \== target\_move\['design\_focus'\]):  
                return 'refinement'  
            else:  
                return 'semantic'  
        elif semantic\_similarity \> 0.4:  
            if (source\_move\['cognitive\_operation'\] \== 'assessment' and  
                target\_move\['cognitive\_operation'\] \== 'proposal'):  
                return 'causal'  
            else:  
                return 'semantic'  
        else:  
            return 'semantic'  
      
    async def \_calculate\_natural\_link\_strength(self,   
                                             source\_move: Dict\[str, Any\],   
                                             target\_move: Dict\[str, Any\],  
                                             semantic\_similarity: float,  
                                             temporal\_distance: int) \-\> float:  
        """Calculate strength of natural link"""  
          
        \# Base strength from semantic similarity  
        base\_strength \= semantic\_similarity  
          
        \# Temporal proximity bonus  
        temporal\_bonus \= max(0, (10 \- temporal\_distance) / 10\) \* 0.2  
          
        \# Same design focus bonus  
        focus\_bonus \= 0.1 if source\_move\['design\_focus'\] \== target\_move\['design\_focus'\] else 0  
          
        \# Self-generation bonus (always applies for baseline)  
        self\_generation\_bonus \= 0.1  
          
        \# Natural progression bonus  
        natural\_flow\_bonus \= min(0.1,   
            source\_move.get('natural\_progression\_indicator', 0\) \*   
            target\_move.get('natural\_progression\_indicator', 0\)  
        )  
          
        total\_strength \= min(1.0, base\_strength \+ temporal\_bonus \+ focus\_bonus \+   
                           self\_generation\_bonus \+ natural\_flow\_bonus)  
          
        return total\_strength  
      
    async def \_is\_self\_identified\_link(self, source\_move: Dict\[str, Any\], target\_move: Dict\[str, Any\]) \-\> bool:  
        """Determine if link represents self-identified connection"""  
          
        \# Look for explicit connection language in target move  
        connection\_indicators \= \['this relates to', 'building on', 'similar to', 'connects with'\]  
        target\_content\_lower \= target\_move\['content'\].lower()  
          
        return any(indicator in target\_content\_lower for indicator in connection\_indicators)  
      
    async def \_build\_baseline\_network(self,   
                                    moves: List\[Dict\[str, Any\]\],   
                                    links: List\[Dict\[str, Any\]\]) \-\> nx.DiGraph:  
        """Build baseline NetworkX graph"""  
          
        G \= nx.DiGraph()  
          
        \# Add nodes (moves) with baseline attributes  
        for move in moves:  
            G.add\_node(move\['move\_id'\], \*\*move)  
          
        \# Add edges (links) with natural attributes  
        for link in links:  
            G.add\_edge(  
                link\['source\_move\_id'\],   
                link\['target\_move\_id'\],  
                \*\*link  
            )  
          
        return G  
      
    async def \_calculate\_baseline\_metrics(self,   
                                        network: nx.DiGraph,   
                                        moves: List\[Dict\[str, Any\]\],   
                                        links: List\[Dict\[str, Any\]\]) \-\> Dict\[str, Any\]:  
        """Calculate comprehensive baseline network metrics"""  
          
        total\_moves \= len(moves)  
        total\_links \= len(links)  
          
        \# Basic baseline metrics  
        natural\_link\_index \= total\_links / total\_moves if total\_moves \> 0 else 0  
          
        \# Move source distribution (should be mostly self-generated)  
        move\_sources \= \[move\['move\_source'\] for move in moves\]  
        self\_generated \= sum(1 for source in move\_sources if source \== 'self\_generated')  
        resource\_referenced \= sum(1 for source in move\_sources if source \== 'resource\_referenced')  
        platform\_prompted \= sum(1 for source in move\_sources if source \== 'platform\_prompted')  
          
        \# Independence metrics  
        full\_autonomy\_ratio \= self\_generated / total\_moves if total\_moves \> 0 else 0  
        resource\_dependency\_level \= resource\_referenced / total\_moves if total\_moves \> 0 else 0  
          
        \# Network connectivity (if sufficient nodes)  
        if total\_moves \> 1:  
            try:  
                avg\_clustering \= nx.average\_clustering(network.to\_undirected())  
                density \= nx.density(network)  
                  
                \# Centrality measures  
                betweenness\_centrality \= nx.betweenness\_centrality(network)  
                degree\_centrality \= nx.degree\_centrality(network.to\_undirected())  
                  
                \# Identify naturally central nodes  
                natural\_central\_nodes \= \[  
                    node for node, centrality in betweenness\_centrality.items()   
                    if centrality \> 0.1  
                \]  
                  
            except:  
                avg\_clustering \= 0  
                density \= 0  
                natural\_central\_nodes \= \[\]  
        else:  
            avg\_clustering \= 0  
            density \= 0  
            natural\_central\_nodes \= \[\]  
          
        \# Link type distribution  
        link\_types \= \[link\['link\_type'\] for link in links\]  
        semantic\_links \= sum(1 for lt in link\_types if lt \== 'semantic')  
        temporal\_links \= sum(1 for lt in link\_types if lt \== 'temporal')  
        causal\_links \= sum(1 for lt in link\_types if lt \== 'causal')  
        refinement\_links \= sum(1 for lt in link\_types if lt \== 'refinement')  
          
        \# Natural link characteristics  
        self\_identified\_links \= sum(1 for link in links if link.get('self\_identified', False))  
        cross\_modal\_links \= sum(1 for link in links if link\['cross\_modal'\])  
        phase\_bridging\_links \= sum(1 for link in links if link\['phase\_bridging'\])  
          
        \# Cognitive load distribution  
        cognitive\_loads \= \[move.get('cognitive\_load', 'low') for move in moves\]  
        high\_load\_moves \= sum(1 for load in cognitive\_loads if load \== 'high')  
        medium\_load\_moves \= sum(1 for load in cognitive\_loads if load \== 'medium')  
        low\_load\_moves \= sum(1 for load in cognitive\_loads if load \== 'low')  
          
        return {  
            'total\_moves': total\_moves,  
            'total\_links': total\_links,  
            'natural\_link\_index': natural\_link\_index,  
            'self\_generated\_moves': self\_generated,  
            'resource\_referenced\_moves': resource\_referenced,  
            'platform\_prompted\_moves': platform\_prompted,  
            'full\_autonomy\_ratio': full\_autonomy\_ratio,  
            'resource\_dependency\_level': resource\_dependency\_level,  
            'average\_clustering': avg\_clustering,  
            'network\_density': density,  
            'natural\_central\_nodes': natural\_central\_nodes,  
            'semantic\_links': semantic\_links,  
            'temporal\_links': temporal\_links,  
            'causal\_links': causal\_links,  
            'refinement\_links': refinement\_links,  
            'self\_identified\_links': self\_identified\_links,  
            'cross\_modal\_links': cross\_modal\_links,  
            'phase\_bridging\_links': phase\_bridging\_links,  
            'high\_cognitive\_load\_moves': high\_load\_moves,  
            'medium\_cognitive\_load\_moves': medium\_load\_moves,  
            'low\_cognitive\_load\_moves': low\_load\_moves,  
            'natural\_semantic\_density': semantic\_links / total\_moves if total\_moves \> 0 else 0,  
            'autonomous\_temporal\_flow': temporal\_links / total\_moves if total\_moves \> 0 else 0  
        }  
      
    async def \_analyze\_natural\_patterns(self,   
                                      moves: List\[Dict\[str, Any\]\],   
                                      links: List\[Dict\[str, Any\]\],   
                                      network: nx.DiGraph) \-\> Dict\[str, Any\]:  
        """Analyze natural thinking patterns without AI influence"""  
          
        natural\_patterns \= {  
            'autonomous\_reasoning\_frequency': 0,  
            'self\_evaluation\_instances': 0,  
            'independent\_synthesis\_events': 0,  
            'natural\_discovery\_moments': \[\],  
            'self\_directed\_refinements': \[\],  
            'spontaneous\_reflections': \[\]  
        }  
          
        for move in moves:  
            \# Count autonomous reasoning  
            if move.get('autonomous\_reasoning\_indicators', 0\) \> 0:  
                natural\_patterns\['autonomous\_reasoning\_frequency'\] \+= 1  
              
            \# Count self-evaluations  
            if move.get('self\_evaluation\_indicators', 0\) \> 0:  
                natural\_patterns\['self\_evaluation\_instances'\] \+= 1  
              
            \# Count independent synthesis  
            if move.get('independent\_synthesis', False):  
                natural\_patterns\['independent\_synthesis\_events'\] \+= 1  
              
            \# Identify discovery moments  
            if move.get('independent\_discovery\_indicators', 0\) \> 0:  
                natural\_patterns\['natural\_discovery\_moments'\].append(move\['move\_id'\])  
              
            \# Identify self-directed refinements  
            if move\['move\_type'\] \== 'transformation' and move\['move\_source'\] \== 'self\_generated':  
                natural\_patterns\['self\_directed\_refinements'\].append(move\['move\_id'\])  
              
            \# Identify spontaneous reflections  
            if move\['move\_type'\] \== 'reflection' and move\['move\_source'\] \== 'self\_generated':  
                natural\_patterns\['spontaneous\_reflections'\].append(move\['move\_id'\])  
          
        return natural\_patterns  
      
    async def \_identify\_natural\_critical\_moves(self,   
                                             moves: List\[Dict\[str, Any\]\],   
                                             links: List\[Dict\[str, Any\]\],   
                                             network: nx.DiGraph) \-\> List\[Dict\[str, Any\]\]:  
        """Identify naturally critical moves"""  
          
        critical\_moves \= \[\]  
          
        try:  
            betweenness\_centrality \= nx.betweenness\_centrality(network)  
            degree\_centrality \= nx.degree\_centrality(network.to\_undirected())  
              
            for move in moves:  
                move\_id \= move\['move\_id'\]  
                  
                betweenness \= betweenness\_centrality.get(move\_id, 0\)  
                degree \= degree\_centrality.get(move\_id, 0\)  
                  
                \# Count natural forward and backward links  
                forward\_links \= len(\[l for l in links if l\['source\_move\_id'\] \== move\_id\])  
                backward\_links \= len(\[l for l in links if l\['target\_move\_id'\] \== move\_id\])  
                  
                \# Natural criticality indicators  
                natural\_criticality \= (  
                    betweenness \> 0.1 or   
                    degree \> 0.3 or   
                    forward\_links \> 3 or  
                    (move\['move\_type'\] \== 'synthesis' and move\['move\_source'\] \== 'self\_generated')  
                )  
                  
                if natural\_criticality:  
                    critical\_move \= {  
                        'move\_id': move\_id,  
                        'move': move,  
                        'betweenness\_centrality': betweenness,  
                        'degree\_centrality': degree,  
                        'forward\_links': forward\_links,  
                        'backward\_links': backward\_links,  
                        'natural\_criticality\_score': betweenness \+ degree \* 0.5 \+ forward\_links \* 0.1,  
                        'self\_generated': move\['move\_source'\] \== 'self\_generated'  
                    }  
                    critical\_moves.append(critical\_move)  
          
        except Exception as e:  
            logger.warning(f"Error calculating natural critical moves: {e}")  
          
        \# Sort by natural criticality score  
        critical\_moves.sort(key=lambda x: x\['natural\_criticality\_score'\], reverse=True)  
          
        return critical\_moves  
      
    async def \_calculate\_baseline\_cognitive\_indicators(self,   
                                                     moves: List\[Dict\[str, Any\]\],   
                                                     links: List\[Dict\[str, Any\]\],   
                                                     network: nx.DiGraph,  
                                                     session\_context: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Calculate baseline cognitive development indicators"""  
          
        \# Group moves by phase for analysis  
        phases \= \['ideation', 'visualization', 'materialization'\]  
        phase\_moves \= {phase: \[m for m in moves if m\['phase'\] \== phase\] for phase in phases}  
          
        indicators \= {}  
          
        \# Natural concept synthesis evolution  
        synthesis\_moves\_by\_phase \= {  
            phase: \[m for m in phase\_moves\[phase\] if m\['move\_type'\] \== 'synthesis' and m\['move\_source'\] \== 'self\_generated'\]  
            for phase in phases  
        }  
          
        natural\_concept\_synthesis \= \[\]  
        for phase in phases:  
            synthesis\_count \= len(synthesis\_moves\_by\_phase\[phase\])  
            avg\_complexity \= np.mean(\[  
                m.get('linguistic\_complexity', 0\) for m in synthesis\_moves\_by\_phase\[phase\]  
            \]) if synthesis\_moves\_by\_phase\[phase\] else 0  
              
            natural\_concept\_synthesis.append({  
                'phase': phase,  
                'natural\_synthesis\_count': synthesis\_count,  
                'average\_complexity': avg\_complexity  
            })  
          
        indicators\['natural\_concept\_synthesis'\] \= natural\_concept\_synthesis  
          
        \# Independent evaluation sophistication  
        evaluation\_moves \= \[m for m in moves   
                          if m\['move\_type'\] \== 'evaluation' and m\['move\_source'\] \== 'self\_generated'\]  
        independent\_evaluation\_sophistication \= np.mean(\[  
            m.get('technical\_depth', 0\) \+ m.get('linguistic\_complexity', 0\)  
            for m in evaluation\_moves  
        \]) if evaluation\_moves else 0  
          
        indicators\['independent\_evaluation\_sophistication'\] \= independent\_evaluation\_sophistication  
          
        \# Spontaneous metacognitive awareness  
        reflection\_moves \= \[m for m in moves   
                          if m\['move\_type'\] \== 'reflection' and m\['move\_source'\] \== 'self\_generated'\]  
        spontaneous\_metacognitive\_awareness \= len(reflection\_moves) / len(moves) if moves else 0  
          
        indicators\['spontaneous\_metacognitive\_awareness'\] \= spontaneous\_metacognitive\_awareness  
          
        \# Autonomous knowledge integration depth  
        cross\_focus\_links \= \[  
            l for l in links   
            if any(m\['move\_id'\] \== l\['source\_move\_id'\] and   
                   any(m2\['move\_id'\] \== l\['target\_move\_id'\] and   
                       m\['design\_focus'\] \!= m2\['design\_focus'\] and  
                       m\['move\_source'\] \== 'self\_generated' and   
                       m2\['move\_source'\] \== 'self\_generated'  
                       for m2 in moves)  
                   for m in moves)  
        \]  
          
        autonomous\_knowledge\_integration \= len(cross\_focus\_links) / len(links) if links else 0  
          
        indicators\['autonomous\_knowledge\_integration'\] \= autonomous\_knowledge\_integration  
          
        return indicators  
      
    async def \_generate\_baseline\_summary(self,   
                                       moves: List\[Dict\[str, Any\]\],   
                                       links: List\[Dict\[str, Any\]\],   
                                       network\_metrics: Dict\[str, Any\],  
                                       natural\_patterns: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Generate comprehensive baseline session summary"""  
          
        return {  
            'baseline\_session\_overview': {  
                'total\_moves': len(moves),  
                'total\_links': len(links),  
                'natural\_link\_index': network\_metrics\['natural\_link\_index'\],  
                'full\_autonomy\_ratio': network\_metrics\['full\_autonomy\_ratio'\]  
            },  
            'natural\_process\_characteristics': {  
                'dominant\_move\_types': self.\_get\_dominant\_move\_types(moves),  
                'primary\_design\_focuses': self.\_get\_primary\_design\_focuses(moves),  
                'cognitive\_load\_distribution': self.\_get\_cognitive\_load\_distribution(moves),  
                'independence\_level\_distribution': self.\_get\_independence\_distribution(moves)  
            },  
            'autonomous\_thinking\_summary': {  
                'autonomous\_reasoning\_events': natural\_patterns\['autonomous\_reasoning\_frequency'\],  
                'self\_evaluation\_instances': natural\_patterns\['self\_evaluation\_instances'\],  
                'independent\_synthesis\_events': natural\_patterns\['independent\_synthesis\_events'\],  
                'natural\_discovery\_count': len(natural\_patterns\['natural\_discovery\_moments'\]),  
                'spontaneous\_reflection\_count': len(natural\_patterns\['spontaneous\_reflections'\])  
            },  
            'baseline\_network\_characteristics': {  
                'connectivity\_level': 'high' if network\_metrics\['network\_density'\] \> 0.3 else 'medium' if network\_metrics\['network\_density'\] \> 0.1 else 'low',  
                'natural\_critical\_move\_count': len(network\_metrics.get('natural\_central\_nodes', \[\])),  
                'cross\_phase\_integration': network\_metrics\['phase\_bridging\_links'\],  
                'self\_directed\_coherence': network\_metrics.get('average\_clustering', 0\)  
            }  
        }  
      
    def \_get\_dominant\_move\_types(self, moves: List\[Dict\[str, Any\]\]) \-\> List\[Tuple\[str, int\]\]:  
        """Get dominant move types for baseline"""  
        move\_type\_counts \= {}  
        for move in moves:  
            if move\['move\_source'\] \== 'self\_generated':  \# Focus on self-generated for baseline  
                move\_type \= move\['move\_type'\]  
                move\_type\_counts\[move\_type\] \= move\_type\_counts.get(move\_type, 0\) \+ 1  
          
        return sorted(move\_type\_counts.items(), key=lambda x: x\[1\], reverse=True)  
      
    def \_get\_primary\_design\_focuses(self, moves: List\[Dict\[str, Any\]\]) \-\> List\[Tuple\[str, int\]\]:  
        """Get primary design focuses for baseline"""  
        focus\_counts \= {}  
        for move in moves:  
            if move\['move\_source'\] \== 'self\_generated':  
                focus \= move\['design\_focus'\]  
                focus\_counts\[focus\] \= focus\_counts.get(focus, 0\) \+ 1  
          
        return sorted(focus\_counts.items(), key=lambda x: x\[1\], reverse=True)  
      
    def \_get\_cognitive\_load\_distribution(self, moves: List\[Dict\[str, Any\]\]) \-\> Dict\[str, int\]:  
        """Get distribution of cognitive load levels for baseline"""  
        load\_counts \= {'high': 0, 'medium': 0, 'low': 0}  
        for move in moves:  
            if move\['move\_source'\] \== 'self\_generated':  
                load \= move.get('cognitive\_load', 'low')  
                load\_counts\[load\] \+= 1  
          
        return load\_counts  
      
    def \_get\_independence\_distribution(self, moves: List\[Dict\[str, Any\]\]) \-\> Dict\[str, int\]:  
        """Get distribution of independence levels"""  
        independence\_counts \= {'fully\_autonomous': 0, 'resource\_assisted': 0, 'platform\_guided': 0}  
        for move in moves:  
            independence \= move.get('independence\_level', 'fully\_autonomous')  
            independence\_counts\[independence\] \+= 1  
          
        return independence\_counts

### **4\. Baseline Interface System**

**File: `src/baseline_interface/minimal_interface.py`**

"""  
Minimal Interface System for Baseline Control Group  
Provides basic interaction without AI assistance or guidance  
"""

import streamlit as st  
import asyncio  
import logging  
from typing import Dict, Any, List  
from datetime import datetime  
import json

from ..linkography.natural\_move\_parser import NaturalMoveParser  
from ..linkography.baseline\_linkographic\_analyzer import BaselineLinkographicAnalyzer  
from ..data\_processing.baseline\_session\_manager import BaselineSessionManager

logger \= logging.getLogger(\_\_name\_\_)

class MinimalBaselineInterface:  
    """Minimal interface for baseline control group"""  
      
    def \_\_init\_\_(self):  
        self.move\_parser \= NaturalMoveParser()  
        self.linkographic\_analyzer \= BaselineLinkographicAnalyzer()  
        self.session\_manager \= BaselineSessionManager()  
          
        \# Minimal response templates  
        self.acknowledgments \= \[  
            "Input recorded.",  
            "Thank you for your input.",  
            "Recorded.",  
            "Input captured."  
        \]  
          
        self.neutral\_prompts \= \[  
            "Please continue with your design process.",  
            "You may proceed to the next phase when ready.",  
            "Please document your thinking as you work.",  
            "Continue developing your concept."  
        \]  
          
        \# Prohibited responses (to ensure no AI assistance)  
        self.prohibited\_patterns \= \[  
            'suggest', 'recommend', 'consider', 'you should', 'try',  
            'good idea', 'excellent', 'perhaps', 'maybe you could',  
            'have you thought about', 'what if you', 'you might'  
        \]  
      
    def render\_baseline\_interface(self):  
        """Render minimal interface for baseline testing"""  
          
        st.title("Architectural Design Challenge")  
        st.caption("Independent Design Process \- No AI Assistance")  
          
        \# Session initialization  
        if 'baseline\_session\_id' not in st.session\_state:  
            participant\_id \= st.text\_input("Enter Participant ID:")  
            if participant\_id:  
                session\_data \= {  
                    "participant\_id": participant\_id,  
                    "group\_assignment": "baseline\_control",  
                    "phase": "ideation",  
                    "interface\_type": "minimal\_baseline"  
                }  
                  
                \# Initialize session  
                session\_id \= asyncio.run(self.session\_manager.create\_baseline\_session(session\_data))  
                st.session\_state.baseline\_session\_id \= session\_id  
                st.session\_state.participant\_id \= participant\_id  
                st.session\_state.current\_phase \= "ideation"  
                st.session\_state.interaction\_history \= \[\]  
                st.session\_state.total\_moves \= 0  
                st.rerun()  
          
        \# Main interface (only if session is initialized)  
        if 'baseline\_session\_id' in st.session\_state:  
            self.\_render\_main\_baseline\_interface()  
      
    def \_render\_main\_baseline\_interface(self):  
        """Render main baseline interface"""  
          
        session\_id \= st.session\_state.baseline\_session\_id  
        current\_phase \= st.session\_state.current\_phase  
          
        \# Phase indicator  
        st.subheader(f"Phase: {current\_phase.title()}")  
          
        \# Phase-specific prompts  
        phase\_prompts \= {  
            'ideation': "Design a community center for a diverse urban neighborhood of 15,000 residents. The site is a former industrial warehouse (150m x 80m x 12m height). Consider: community needs, cultural sensitivity, sustainability, and adaptive reuse principles.",  
            'visualization': "Transform your community center concept into visual representations. Create spatial diagrams, section studies, or perspective sketches. Consider: circulation patterns, spatial relationships, and environmental systems.",  
            'materialization': "Develop your community center design into implementable solutions. Address: structural systems, environmental controls, material specifications, and construction sequencing."  
        }  
          
        st.info(phase\_prompts\[current\_phase\])  
          
        \# Input methods  
        col1, col2 \= st.columns(\[2, 1\])  
          
        with col1:  
            \# Text input  
            user\_input \= st.text\_area(  
                "Document your design thinking and decisions:",  
                height=150,  
                key=f"text\_input\_{current\_phase}",  
                placeholder="Describe your design process, reasoning, and decisions..."  
            )  
              
            if st.button("Submit Input", key=f"submit\_{current\_phase}"):  
                if user\_input.strip():  
                    asyncio.run(self.\_process\_baseline\_input(  
                        session\_id, user\_input, current\_phase, 'text'  
                    ))  
                    st.success("Input recorded.")  
                    st.rerun()  
          
        with col2:  
            \# File upload for sketches  
            uploaded\_file \= st.file\_uploader(  
                "Upload sketch or diagram:",  
                type=\['png', 'jpg', 'jpeg', 'pdf'\],  
                key=f"upload\_{current\_phase}"  
            )  
              
            if uploaded\_file:  
                \# Save uploaded file  
                file\_path \= f"data/uploads/{session\_id}\_{uploaded\_file.name}"  
                with open(file\_path, "wb") as f:  
                    f.write(uploaded\_file.getbuffer())  
                  
                \# Process upload  
                description \= st.text\_input(  
                    "Describe your sketch:",  
                    key=f"sketch\_desc\_{uploaded\_file.name}"  
                )  
                  
                if st.button("Submit Sketch", key=f"submit\_sketch\_{uploaded\_file.name}"):  
                    if description.strip():  
                        asyncio.run(self.\_process\_baseline\_input(  
                            session\_id, description, current\_phase, 'sketch', file\_path  
                        ))  
                        st.success("Sketch recorded.")  
                        st.rerun()  
          
        \# Phase progression  
        st.markdown("---")  
          
        phase\_order \= \['ideation', 'visualization', 'materialization'\]  
        current\_index \= phase\_order.index(current\_phase)  
          
        if current\_index \< len(phase\_order) \- 1:  
            next\_phase \= phase\_order\[current\_index \+ 1\]  
            if st.button(f"Proceed to {next\_phase.title()} Phase"):  
                st.session\_state.current\_phase \= next\_phase  
                st.rerun()  
        else:  
            if st.button("Complete Design Process"):  
                asyncio.run(self.\_complete\_baseline\_session(session\_id))  
                st.success("Design process completed\!")  
                st.balloons()  
          
        \# Progress indicator  
        self.\_show\_progress\_indicator()  
      
    async def \_process\_baseline\_input(self,   
                                    session\_id: str,   
                                    user\_input: str,   
                                    phase: str,   
                                    modality: str,  
                                    file\_path: str \= None):  
        """Process user input without AI assistance"""  
          
        try:  
            \# Create interaction context  
            interaction\_context \= {  
                'session\_id': session\_id,  
                'phase': phase,  
                'modality': modality,  
                'timestamp': datetime.now().isoformat(),  
                'file\_path': file\_path,  
                'base\_sequence': st.session\_state.total\_moves  
            }  
              
            \# Extract natural moves  
            moves \= await self.move\_parser.extract\_natural\_moves\_from\_input(  
                user\_input, interaction\_context  
            )  
              
            \# Update move count  
            st.session\_state.total\_moves \+= len(moves)  
              
            \# Store interaction  
            interaction\_record \= await self.session\_manager.store\_baseline\_interaction(  
                session\_id=session\_id,  
                user\_input=user\_input,  
                system\_response=self.\_generate\_minimal\_response(),  
                moves=moves,  
                interaction\_context=interaction\_context  
            )  
              
            \# Update session state  
            st.session\_state.interaction\_history.append({  
                'user\_input': user\_input,  
                'system\_response': interaction\_record\['system\_response'\],  
                'timestamp': datetime.now().isoformat(),  
                'moves\_count': len(moves),  
                'phase': phase  
            })  
              
        except Exception as e:  
            logger.error(f"Error processing baseline input: {e}")  
            st.error("Error processing input. Please try again.")  
      
    def \_generate\_minimal\_response(self) \-\> str:  
        """Generate minimal, non-assistive response"""  
          
        import random  
        return random.choice(self.acknowledgments)  
      
    async def \_complete\_baseline\_session(self, session\_id: str):  
        """Complete baseline session and generate analysis"""  
          
        try:  
            \# Generate complete linkographic analysis  
            complete\_analysis \= await self.session\_manager.generate\_complete\_baseline\_analysis(  
                session\_id  
            )  
              
            \# Store completion  
            await self.session\_manager.mark\_session\_complete(session\_id)  
              
            \# Show summary (minimal)  
            st.session\_state.session\_complete \= True  
            st.session\_state.final\_analysis \= complete\_analysis  
              
        except Exception as e:  
            logger.error(f"Error completing baseline session: {e}")  
            st.error("Error completing session.")  
      
    def \_show\_progress\_indicator(self):  
        """Show minimal progress information"""  
          
        st.sidebar.markdown("\#\#\# Progress")  
        st.sidebar.text(f"Current Phase: {st.session\_state.current\_phase.title()}")  
        st.sidebar.text(f"Total Inputs: {len(st.session\_state.interaction\_history)}")  
        st.sidebar.text(f"Design Moves: {st.session\_state.total\_moves}")  
          
        \# Phase completion indicator  
        phases \= \['ideation', 'visualization', 'materialization'\]  
        current\_index \= phases.index(st.session\_state.current\_phase)  
          
        for i, phase in enumerate(phases):  
            if i \< current\_index:  
                st.sidebar.text(f" {phase.title()}")  
            elif i \== current\_index:  
                st.sidebar.text(f" {phase.title()}")  
            else:  
                st.sidebar.text(f"  {phase.title()}")

### **5\. Baseline Session Manager**

**File: `src/data_processing/baseline_session_manager.py`**

"""  
Baseline Session Manager for No AI Control Group  
Manages pure baseline data without AI assistance  
"""

import asyncio  
import uuid  
from datetime import datetime  
from typing import Dict, List, Any, Optional  
import logging  
import json

from .enhanced\_session\_manager import SessionManager  
from ..benchmarking.baseline\_cognitive\_metrics import BaselineCognitiveMetrics  
from ..linkography.baseline\_linkographic\_analyzer import BaselineLinkographicAnalyzer

logger \= logging.getLogger(\_\_name\_\_)

class BaselineSessionManager(SessionManager):  
    """Session manager for baseline control group"""  
      
    def \_\_init\_\_(self):  
        super().\_\_init\_\_()  
        self.cognitive\_metrics \= BaselineCognitiveMetrics()  
        self.linkographic\_analyzer \= BaselineLinkographicAnalyzer()  
          
        \# Track baseline sessions  
        self.baseline\_sessions \= {}  \# session\_id \-\> session\_data  
        self.baseline\_interactions \= {}  \# session\_id \-\> List\[interactions\]  
        self.baseline\_moves \= {}  \# session\_id \-\> List\[moves\]  
      
    async def create\_baseline\_session(self, session\_data: Dict\[str, Any\]) \-\> str:  
        """Create new baseline session"""  
          
        session\_id \= str(uuid.uuid4())  
          
        baseline\_session \= {  
            'session\_id': session\_id,  
            'participant\_id': session\_data\['participant\_id'\],  
            'group\_assignment': 'baseline\_control',  
            'start\_time': datetime.now().isoformat(),  
            'current\_phase': session\_data.get('phase', 'ideation'),  
            'interface\_type': 'minimal\_baseline',  
            'total\_interactions': 0,  
            'total\_moves': 0,  
            'independence\_verified': True,  
            'ai\_assistance\_provided': False,  \# Always False for baseline  
            'status': 'active'  
        }  
          
        self.baseline\_sessions\[session\_id\] \= baseline\_session  
        self.baseline\_interactions\[session\_id\] \= \[\]  
        self.baseline\_moves\[session\_id\] \= \[\]  
          
        logger.info(f"Created baseline session {session\_id} for participant {session\_data\['participant\_id'\]}")  
          
        return session\_id  
      
    async def store\_baseline\_interaction(self,   
                                       session\_id: str,  
                                       user\_input: str,  
                                       system\_response: str,  
                                       moves: List\[Dict\[str, Any\]\],  
                                       interaction\_context: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Store baseline interaction without AI assistance"""  
          
        interaction\_id \= str(uuid.uuid4())  
          
        \# Create interaction record  
        interaction \= {  
            'interaction\_id': interaction\_id,  
            'session\_id': session\_id,  
            'timestamp': datetime.now().isoformat(),  
            'user\_input': user\_input,  
            'system\_response': system\_response,  
            'interaction\_type': interaction\_context.get('modality', 'text'),  
            'phase': interaction\_context.get('phase', 'ideation'),  
            'moves\_generated': len(moves),  
            'ai\_assistance\_provided': False,  \# Always False  
            'independence\_maintained': True,  \# Always True  
            'thinking\_pause\_duration': interaction\_context.get('thinking\_pause', 0),  
            'context': interaction\_context  
        }  
          
        \# Store interaction  
        if session\_id not in self.baseline\_interactions:  
            self.baseline\_interactions\[session\_id\] \= \[\]  
        self.baseline\_interactions\[session\_id\].append(interaction)  
          
        \# Store moves  
        if session\_id not in self.baseline\_moves:  
            self.baseline\_moves\[session\_id\] \= \[\]  
        self.baseline\_moves\[session\_id\].extend(moves)  
          
        \# Update session statistics  
        if session\_id in self.baseline\_sessions:  
            self.baseline\_sessions\[session\_id\]\['total\_interactions'\] \+= 1  
            self.baseline\_sessions\[session\_id\]\['total\_moves'\] \+= len(moves)  
            self.baseline\_sessions\[session\_id\]\['last\_interaction'\] \= datetime.now().isoformat()  
          
        \# Calculate real-time baseline metrics  
        baseline\_metrics \= await self.cognitive\_metrics.calculate\_baseline\_interaction\_metrics(  
            interaction, moves, self.baseline\_moves\[session\_id\]  
        )  
          
        return {  
            'interaction\_id': interaction\_id,  
            'system\_response': system\_response,  
            'moves\_generated': len(moves),  
            'baseline\_metrics': baseline\_metrics,  
            'independence\_verified': True,  
            'total\_session\_moves': len(self.baseline\_moves\[session\_id\])  
        }  
      
    async def generate\_complete\_baseline\_analysis(self, session\_id: str) \-\> Dict\[str, Any\]:  
        """Generate complete baseline linkographic analysis"""  
          
        if session\_id not in self.baseline\_moves:  
            raise ValueError(f"No baseline data found for session {session\_id}")  
          
        all\_moves \= self.baseline\_moves\[session\_id\]  
        session\_context \= {'session\_id': session\_id, 'type': 'baseline\_control'}  
          
        \# Perform complete linkographic analysis  
        linkographic\_analysis \= await self.linkographic\_analyzer.analyze\_baseline\_session\_linkography(  
            moves=all\_moves,  
            session\_context=session\_context  
        )  
          
        \# Calculate comprehensive baseline cognitive metrics  
        cognitive\_metrics \= await self.cognitive\_metrics.calculate\_complete\_baseline\_metrics(  
            session\_id, linkographic\_analysis  
        )  
          
        \# Generate comparative baseline data  
        comparative\_data \= await self.cognitive\_metrics.generate\_baseline\_comparative\_data(  
            linkographic\_analysis, cognitive\_metrics  
        )  
          
        return {  
            'session\_id': session\_id,  
            'analysis\_type': 'baseline\_control',  
            'linkographic\_analysis': linkographic\_analysis,  
            'cognitive\_metrics': cognitive\_metrics,  
            'comparative\_baseline\_data': comparative\_data,  
            'session\_summary': linkographic\_analysis.get('baseline\_session\_summary', {}),  
            'independence\_verification': await self.\_verify\_independence(session\_id),  
            'analysis\_timestamp': datetime.now().isoformat()  
        }  
      
    async def \_verify\_independence(self, session\_id: str) \-\> Dict\[str, Any\]:  
        """Verify that no AI assistance was provided"""  
          
        interactions \= self.baseline\_interactions.get(session\_id, \[\])  
        moves \= self.baseline\_moves.get(session\_id, \[\])  
          
        \# Verify all moves are self-generated  
        self\_generated\_moves \= sum(1 for move in moves if move.get('move\_source') \== 'self\_generated')  
        total\_moves \= len(moves)  
          
        \# Verify no AI assistance in interactions  
        ai\_assistance\_count \= sum(1 for interaction in interactions   
                                if interaction.get('ai\_assistance\_provided', False))  
          
        \# Verify system responses are minimal  
        prohibited\_responses \= 0  
        for interaction in interactions:  
            response \= interaction.get('system\_response', '').lower()  
            if any(word in response for word in \['suggest', 'recommend', 'consider', 'try'\]):  
                prohibited\_responses \+= 1  
          
        independence\_score \= (  
            (self\_generated\_moves / max(total\_moves, 1)) \* 0.5 \+  
            (1.0 if ai\_assistance\_count \== 0 else 0.0) \* 0.3 \+  
            (1.0 if prohibited\_responses \== 0 else 0.0) \* 0.2  
        )  
          
        return {  
            'independence\_verified': independence\_score \>= 0.95,  
            'independence\_score': independence\_score,  
            'self\_generated\_move\_ratio': self\_generated\_moves / max(total\_moves, 1),  
            'ai\_assistance\_instances': ai\_assistance\_count,  
            'prohibited\_response\_count': prohibited\_responses,  
            'total\_interactions': len(interactions),  
            'total\_moves': total\_moves,  
            'verification\_timestamp': datetime.now().isoformat()  
        }  
      
    async def get\_baseline\_session\_data(self, session\_id: str) \-\> Dict\[str, Any\]:  
        """Get baseline session data"""  
          
        if session\_id not in self.baseline\_sessions:  
            raise ValueError(f"Baseline session {session\_id} not found")  
          
        session\_data \= self.baseline\_sessions\[session\_id\].copy()  
        session\_data\['interactions'\] \= self.baseline\_interactions.get(session\_id, \[\])  
        session\_data\['moves'\] \= self.baseline\_moves.get(session\_id, \[\])  
          
        return session\_data  
      
    async def mark\_session\_complete(self, session\_id: str):  
        """Mark baseline session as complete"""  
          
        if session\_id in self.baseline\_sessions:  
            self.baseline\_sessions\[session\_id\]\['status'\] \= 'completed'  
            self.baseline\_sessions\[session\_id\]\['end\_time'\] \= datetime.now().isoformat()  
              
            \# Calculate session duration  
            start\_time \= datetime.fromisoformat(self.baseline\_sessions\[session\_id\]\['start\_time'\])  
            end\_time \= datetime.now()  
            duration \= (end\_time \- start\_time).total\_seconds()  
            self.baseline\_sessions\[session\_id\]\['duration\_seconds'\] \= duration  
              
            logger.info(f"Marked baseline session {session\_id} as complete")  
      
    async def export\_baseline\_data(self, session\_id: str, format: str \= 'json') \-\> Dict\[str, Any\]:  
        """Export baseline data for analysis"""  
          
        complete\_analysis \= await self.generate\_complete\_baseline\_analysis(session\_id)  
        session\_data \= await self.get\_baseline\_session\_data(session\_id)  
          
        export\_data \= {  
            'session\_metadata': session\_data,  
            'complete\_analysis': complete\_analysis,  
            'export\_timestamp': datetime.now().isoformat(),  
            'export\_format': format,  
            'data\_type': 'baseline\_control'  
        }  
          
        if format.lower() \== 'csv':  
            return await self.\_convert\_baseline\_to\_csv(export\_data)  
        else:  
            return export\_data  
      
    async def \_convert\_baseline\_to\_csv(self, export\_data: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Convert baseline data to CSV format"""  
          
        \# Implementation would convert baseline data to CSV format  
        \# This is a placeholder for actual CSV conversion  
          
        return {  
            'format': 'csv',  
            'files': {  
                'baseline\_moves.csv': 'baseline\_moves\_csv\_content',  
                'baseline\_links.csv': 'baseline\_links\_csv\_content',  
                'baseline\_metrics.csv': 'baseline\_metrics\_csv\_content',  
                'baseline\_sessions.csv': 'baseline\_sessions\_csv\_content'  
            }  
        }

### **6\. Baseline API Routes**

**File: `src/api/baseline_control_routes.py`**

"""  
API Routes for Baseline Control Group  
Provides endpoints for no-AI testing with linkographic analysis  
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks  
from typing import Dict, Any, List  
import logging  
from datetime import datetime

from ..baseline\_interface.minimal\_interface import MinimalBaselineInterface  
from ..data\_processing.baseline\_session\_manager import BaselineSessionManager  
from ..benchmarking.baseline\_cognitive\_metrics import BaselineCognitiveMetrics

logger \= logging.getLogger(\_\_name\_\_)  
router \= APIRouter(prefix="/baseline-control", tags=\["baseline-control"\])

\# Initialize baseline components  
baseline\_interface \= MinimalBaselineInterface()  
baseline\_session\_manager \= BaselineSessionManager()  
baseline\_cognitive\_metrics \= BaselineCognitiveMetrics()

@router.post("/sessions/create")  
async def create\_baseline\_session(session\_data: Dict\[str, Any\]):  
    """Create new baseline control session"""  
      
    try:  
        \# Validate required fields  
        required\_fields \= \['participant\_id'\]  
        for field in required\_fields:  
            if field not in session\_data:  
                raise ValueError(f"Missing required field: {field}")  
          
        \# Add baseline-specific data  
        session\_data\['group\_assignment'\] \= 'baseline\_control'  
        session\_data\['interface\_type'\] \= 'minimal\_baseline'  
        session\_data\['ai\_assistance\_enabled'\] \= False  
          
        \# Create session  
        session\_id \= await baseline\_session\_manager.create\_baseline\_session(session\_data)  
          
        return {  
            'session\_id': session\_id,  
            'group\_assignment': 'baseline\_control',  
            'ai\_assistance\_enabled': False,  
            'interface\_type': 'minimal\_baseline',  
            'status': 'created',  
            'timestamp': datetime.now().isoformat()  
        }  
          
    except Exception as e:  
        logger.error(f"Error creating baseline session: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.post("/sessions/{session\_id}/interact")  
async def process\_baseline\_interaction(  
    session\_id: str,  
    interaction\_data: Dict\[str, Any\],  
    background\_tasks: BackgroundTasks  
):  
    """Process baseline interaction without AI assistance"""  
      
    try:  
        \# Validate input  
        required\_fields \= \['user\_input', 'phase'\]  
        for field in required\_fields:  
            if field not in interaction\_data:  
                raise ValueError(f"Missing required field: {field}")  
          
        \# Ensure no AI assistance flags  
        if interaction\_data.get('request\_ai\_assistance', False):  
            return {  
                'error': 'AI assistance not available in baseline control group',  
                'system\_response': 'Input recorded. Please continue independently.',  
                'ai\_assistance\_provided': False  
            }  
          
        \# Extract natural moves  
        moves \= await baseline\_interface.move\_parser.extract\_natural\_moves\_from\_input(  
            interaction\_data\['user\_input'\],  
            {  
                'session\_id': session\_id,  
                'phase': interaction\_data\['phase'\],  
                'modality': interaction\_data.get('modality', 'text'),  
                'timestamp': datetime.now().isoformat()  
            }  
        )  
          
        \# Store interaction  
        interaction\_result \= await baseline\_session\_manager.store\_baseline\_interaction(  
            session\_id=session\_id,  
            user\_input=interaction\_data\['user\_input'\],  
            system\_response=baseline\_interface.\_generate\_minimal\_response(),  
            moves=moves,  
            interaction\_context=interaction\_data  
        )  
          
        \# Schedule background analysis  
        background\_tasks.add\_task(  
            update\_baseline\_analysis,  
            session\_id,  
            moves  
        )  
          
        return {  
            'interaction\_id': interaction\_result\['interaction\_id'\],  
            'system\_response': interaction\_result\['system\_response'\],  
            'moves\_generated': len(moves),  
            'ai\_assistance\_provided': False,  
            'independence\_maintained': True,  
            'baseline\_metrics': interaction\_result\['baseline\_metrics'\],  
            'total\_session\_moves': interaction\_result\['total\_session\_moves'\],  
            'recommendations': \[\],  \# Always empty for baseline  
            'alerts': \[\]  \# Always empty for baseline  
        }  
          
    except Exception as e:  
        logger.error(f"Error processing baseline interaction: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/sessions/{session\_id}/analysis")  
async def get\_baseline\_analysis(session\_id: str):  
    """Get complete baseline linkographic analysis"""  
      
    try:  
        complete\_analysis \= await baseline\_session\_manager.generate\_complete\_baseline\_analysis(  
            session\_id  
        )  
          
        return {  
            'session\_id': session\_id,  
            'analysis\_type': 'baseline\_control',  
            'linkographic\_analysis': complete\_analysis\['linkographic\_analysis'\],  
            'cognitive\_metrics': complete\_analysis\['cognitive\_metrics'\],  
            'comparative\_baseline\_data': complete\_analysis\['comparative\_baseline\_data'\],  
            'independence\_verification': complete\_analysis\['independence\_verification'\],  
            'analysis\_timestamp': datetime.now().isoformat()  
        }  
          
    except Exception as e:  
        logger.error(f"Error retrieving baseline analysis: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/sessions/{session\_id}/independence-verification")  
async def verify\_session\_independence(session\_id: str):  
    """Verify that session maintained independence from AI assistance"""  
      
    try:  
        verification\_result \= await baseline\_session\_manager.\_verify\_independence(session\_id)  
          
        return {  
            'session\_id': session\_id,  
            'verification\_result': verification\_result,  
            'independence\_status': 'verified' if verification\_result\['independence\_verified'\] else 'compromised',  
            'verification\_timestamp': datetime.now().isoformat()  
        }  
          
    except Exception as e:  
        logger.error(f"Error verifying independence: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/sessions/{session\_id}/export")  
async def export\_baseline\_data(  
    session\_id: str,  
    format: str \= "json",  
    include\_raw\_data: bool \= True  
):  
    """Export baseline data for external analysis"""  
      
    try:  
        export\_data \= await baseline\_session\_manager.export\_baseline\_data(  
            session\_id, format  
        )  
          
        if not include\_raw\_data:  
            \# Remove detailed raw data, keep only summary metrics  
            export\_data \= {  
                'session\_metadata': export\_data\['session\_metadata'\],  
                'summary\_metrics': export\_data\['complete\_analysis'\]\['cognitive\_metrics'\],  
                'linkographic\_summary': export\_data\['complete\_analysis'\]\['linkographic\_analysis'\]\['baseline\_session\_summary'\],  
                'export\_timestamp': export\_data\['export\_timestamp'\]  
            }  
          
        return export\_data  
          
    except Exception as e:  
        logger.error(f"Error exporting baseline data: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.post("/sessions/{session\_id}/complete")  
async def complete\_baseline\_session(session\_id: str):  
    """Mark baseline session as complete"""  
      
    try:  
        await baseline\_session\_manager.mark\_session\_complete(session\_id)  
          
        \# Generate final analysis  
        final\_analysis \= await baseline\_session\_manager.generate\_complete\_baseline\_analysis(  
            session\_id  
        )  
          
        return {  
            'session\_id': session\_id,  
            'status': 'completed',  
            'completion\_timestamp': datetime.now().isoformat(),  
            'final\_analysis': final\_analysis,  
            'independence\_verified': final\_analysis\['independence\_verification'\]\['independence\_verified'\]  
        }  
          
    except Exception as e:  
        logger.error(f"Error completing baseline session: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/baseline-statistics")  
async def get\_baseline\_statistics():  
    """Get aggregate statistics for baseline control group"""  
      
    try:  
        \# This would calculate aggregate statistics across all baseline sessions  
        \# Implementation would depend on your specific requirements  
          
        return {  
            'total\_baseline\_sessions': len(baseline\_session\_manager.baseline\_sessions),  
            'completed\_sessions': len(\[s for s in baseline\_session\_manager.baseline\_sessions.values()   
                                     if s.get('status') \== 'completed'\]),  
            'average\_session\_duration': 0,  \# Calculate from completed sessions  
            'average\_moves\_per\_session': 0,  \# Calculate from completed sessions  
            'independence\_verification\_rate': 1.0,  \# Should always be 1.0 for baseline  
            'generated\_timestamp': datetime.now().isoformat()  
        }  
          
    except Exception as e:  
        logger.error(f"Error generating baseline statistics: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

\# Background task functions  
async def update\_baseline\_analysis(session\_id: str, new\_moves: List\[Dict\[str, Any\]\]):  
    """Background task to update baseline analysis"""  
      
    try:  
        \# Update session move data  
        if session\_id in baseline\_session\_manager.baseline\_moves:  
            baseline\_session\_manager.baseline\_moves\[session\_id\].extend(new\_moves)  
          
        \# Update metrics (if needed for real-time tracking)  
        logger.info(f"Updated baseline analysis for session {session\_id}")  
          
    except Exception as e:  
        logger.error(f"Error updating baseline analysis: {e}")

### **7\. Main Application Integration**

**File: `src/main.py` (Updated)**

"""  
Main FastAPI Application with Baseline Control Integration  
"""

from fastapi import FastAPI  
from fastapi.middleware.cors import CORSMiddleware  
import logging

\# Import all route modules  
from .api.enhanced\_mentor\_routes import router as mentor\_router  
from .api.enhanced\_generic\_ai\_routes import router as generic\_ai\_router  
from .api.baseline\_control\_routes import router as baseline\_router

\# Configure logging  
logging.basicConfig(level=logging.INFO)  
logger \= logging.getLogger(\_\_name\_\_)

\# Create FastAPI app  
app \= FastAPI(  
    title="Cognitive Benchmarking Platform with Linkography",  
    description="Complete platform for testing MENTOR, Generic AI, and Baseline control groups",  
    version="3.0.0"  
)

\# Add CORS middleware  
app.add\_middleware(  
    CORSMiddleware,  
    allow\_origins=\["\*"\],  
    allow\_credentials=True,  
    allow\_methods=\["\*"\],  
    allow\_headers=\["\*"\],  
)

\# Include all routers  
app.include\_router(mentor\_router, prefix="/api/v1")  
app.include\_router(generic\_ai\_router, prefix="/api/v1")  
app.include\_router(baseline\_router, prefix="/api/v1")

@app.get("/")  
async def root():  
    return {  
        "message": "Cognitive Benchmarking Platform with Linkography",  
        "version": "3.0.0",  
        "available\_groups": \[  
            "MENTOR (Group A) \- AI-assisted with cognitive scaffolding",  
            "Generic AI (Group B) \- Standard AI assistance",   
            "Baseline Control (Group C) \- No AI assistance"  
        \],  
        "endpoints": {  
            "mentor": "/api/v1/enhanced-mentor/\*",  
            "generic\_ai": "/api/v1/enhanced-generic-ai/\*",  
            "baseline": "/api/v1/baseline-control/\*"  
        }  
    }

@app.get("/health")  
async def health\_check():  
    return {"status": "healthy", "timestamp": "2024-01-01T00:00:00Z"}

if \_\_name\_\_ \== "\_\_main\_\_":  
    import uvicorn  
    uvicorn.run(app, host="0.0.0.0", port=8000)

## **Usage Instructions**

### **1\. Setup and Deployment**

\# Clone and setup  
git clone \<repository\>  
cd no-ai-control-baseline-platform

\# Install dependencies  
pip install \-r requirements.txt

\# Run database migrations  
python scripts/setup\_baseline\_database.py

\# Start the application  
uvicorn src.main:app \--reload \--port 8000

\# Start baseline interface (separate terminal)  
streamlit run src/baseline\_interface/minimal\_interface.py \--server.port 8503

### **2\. Testing Baseline Control Group**

\# Access baseline interface  
http://localhost:8503

\# API endpoints for baseline testing  
POST /api/v1/baseline-control/sessions/create  
POST /api/v1/baseline-control/sessions/{session\_id}/interact  
GET /api/v1/baseline-control/sessions/{session\_id}/analysis  
GET /api/v1/baseline-control/sessions/{session\_id}/independence-verification

### **3\. Data Export and Analysis**

\# Export baseline data  
GET /api/v1/baseline-control/sessions/{session\_id}/export?format=json\&include\_raw\_data=true

\# Get comparative statistics  
GET /api/v1/baseline-control/baseline-statistics

## **Key Features for Baseline Control**

1. **Zero AI Assistance** \- Complete independence verification  
2. **Identical Measurement Framework** \- Same linkographic analysis as other groups  
3. **Natural Thinking Capture** \- Pure self-generated design move analysis  
4. **Minimal Interface** \- Basic tools without guidance or suggestions  
5. **Independence Verification** \- Automated checks for AI assistance contamination  
6. **Comparative Baseline Data** \- Essential metrics for comparing with AI-assisted groups

This implementation provides the essential baseline control for your three-group comparison study, ensuring that the cognitive development impacts of both MENTOR and Generic AI tools can be accurately measured against natural, unassisted design thinking performance.

