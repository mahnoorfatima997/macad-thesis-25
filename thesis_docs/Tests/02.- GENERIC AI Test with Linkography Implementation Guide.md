# **Generic AI Linkography Implementation Guide \- Enhanced**

## **Overview for Claude Code Integration**

This enhanced guide provides comprehensive instructions for implementing the Generic AI B-Tests with full Linkography integration using Claude Code in Visual Studio Code. The implementation focuses on creating a research-grade testing platform that captures design thinking processes through linkographic analysis while maintaining compatibility with your cognitive benchmarking system.

---

## **Project Initialization Commands**

### **1\. Enhanced Setup with Linkography Support**

\# Initialize enhanced project structure  
mkdir generic-ai-linkography-platform  
cd generic-ai-linkography-platform

\# Create Python virtual environment  
python \-m venv venv  
source venv/bin/activate  \# On Windows: venv\\Scripts\\activate

\# Initialize git repository  
git init  
echo "\# Generic AI Linkography Platform" \> README.md  
git add README.md  
git commit \-m "Initial commit with linkography support"

\# Create comprehensive directory structure  
mkdir \-p {src/{api,ui,ai\_integrations,linkography,benchmarking,assessment,data\_processing,utils},tests,data/{raw,processed,linkography,exports},docs,config,scripts,models/{linkography,cognitive}}

### **2\. Enhanced Dependencies Installation**

\# Core dependencies  
pip install fastapi uvicorn sqlalchemy psycopg2-binary redis pandas numpy scipy scikit-learn  
pip install anthropic openai torch transformers sentence-transformers  
pip install streamlit plotly seaborn matplotlib opencv-python Pillow  
pip install pytest pytest-asyncio black flake8 mypy  
pip install python-dotenv pydantic alembic asyncpg

\# Linkography-specific dependencies  
pip install networkx spacy nltk gensim  
pip install python-igraph graph-tool  \# Advanced graph analysis  
pip install umap-learn hdbscan  \# For semantic clustering  
pip install textstat nltk  \# Text complexity analysis  
pip install opencv-python pytesseract  \# For sketch/image analysis

\# NLP models for semantic analysis  
python \-m spacy download en\_core\_web\_sm  
python \-m nltk.downloader punkt stopwords wordnet

\# Create requirements.txt  
pip freeze \> requirements.txt

---

## **Core Implementation Files with Linkography Integration**

### **1\. Enhanced Database Schema with Linkography Support**

**File: `src/data_processing/enhanced_database_models.py`**

"""  
Enhanced Database Models with Linkography Support  
Extends existing models to capture design moves and linkographic relationships  
"""

from sqlalchemy import Column, String, Integer, Float, Boolean, DateTime, Text, JSON, ForeignKey  
from sqlalchemy.dialects.postgresql import UUID  
from sqlalchemy.ext.declarative import declarative\_base  
from sqlalchemy.orm import relationship  
import uuid  
from datetime import datetime

Base \= declarative\_base()

class DesignMove(Base):  
    """Individual design moves with enhanced classification"""  
    \_\_tablename\_\_ \= "design\_moves"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
    interaction\_id \= Column(UUID(as\_uuid=True), ForeignKey("interactions.id"), nullable=True)  
      
    \# Basic move information  
    move\_id \= Column(String(50), nullable=False, unique=True)  
    sequence\_number \= Column(Integer, nullable=False)  
    timestamp \= Column(DateTime, default=datetime.utcnow)  
      
    \# Move content and classification  
    content \= Column(Text, nullable=False)  
    move\_type \= Column(String(20), nullable=False)  \# analysis|synthesis|evaluation|transformation|reflection  
    phase \= Column(String(20), nullable=False)  \# ideation|visualization|materialization  
    modality \= Column(String(20), nullable=False)  \# text|sketch|image|voice|upload  
    cognitive\_operation \= Column(String(20), nullable=False)  \# proposal|clarification|assessment|support|reference  
    design\_focus \= Column(String(20), nullable=False)  \# function|form|structure|material|environment|culture  
      
    \# Enhanced classification for Generic AI  
    move\_source \= Column(String(20), nullable=False)  \# user\_generated|ai\_provided|ai\_prompted|hybrid  
    cognitive\_load \= Column(String(10), nullable=False)  \# high|medium|low  
    directness\_level \= Column(String(20), nullable=False)  \# direct\_answer|guided\_discovery|exploratory  
      
    \# AI-specific metrics  
    ai\_influence\_strength \= Column(Float, default=0.0)  \# 0.0 to 1.0  
    response\_directness \= Column(Float, default=0.0)  \# 0.0 to 1.0  
    information\_density \= Column(Float, default=0.0)  
    scaffolding\_presence \= Column(Boolean, default=False)  
    solution\_completeness \= Column(Float, default=0.0)  \# percentage  
    cognitive\_offloading\_risk \= Column(Float, default=0.0)  \# 0.0 to 1.0  
      
    \# Complexity and quality metrics  
    linguistic\_complexity \= Column(Float, default=0.0)  
    technical\_depth \= Column(Float, default=0.0)  
    innovation\_score \= Column(Float, default=0.0)  
      
    \# Multimodal context  
    concurrent\_sketch\_id \= Column(String(100), nullable=True)  
    image\_annotations \= Column(JSON, nullable=True)  
    spatial\_coordinates \= Column(JSON, nullable=True)  
    interaction\_context \= Column(JSON, nullable=True)  
      
    created\_at \= Column(DateTime, default=datetime.utcnow)

class LinkographicLink(Base):  
    """Links between design moves"""  
    \_\_tablename\_\_ \= "linkographic\_links"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
      
    \# Link endpoints  
    source\_move\_id \= Column(String(50), ForeignKey("design\_moves.move\_id"), nullable=False)  
    target\_move\_id \= Column(String(50), ForeignKey("design\_moves.move\_id"), nullable=False)  
      
    \# Link classification  
    link\_type \= Column(String(20), nullable=False)  \# semantic|temporal|causal|refinement|contradiction  
    link\_strength \= Column(Float, nullable=False)  \# 0.0 to 1.0  
    semantic\_similarity \= Column(Float, default=0.0)  \# cosine similarity  
    temporal\_distance \= Column(Integer, default=0)  \# moves apart  
    conceptual\_distance \= Column(Float, default=0.0)  \# semantic space distance  
      
    \# AI-influenced link analysis  
    ai\_mediated \= Column(Boolean, default=False)  \# Link involves AI interaction  
    cross\_modal \= Column(Boolean, default=False)  \# Links across modalities  
    phase\_bridging \= Column(Boolean, default=False)  \# Links across phases  
      
    \# Link quality and significance  
    critical\_link \= Column(Boolean, default=False)  \# High-impact link  
    innovation\_link \= Column(Boolean, default=False)  \# Novel connection  
    integration\_link \= Column(Boolean, default=False)  \# Knowledge integration  
      
    \# Context and metadata  
    detection\_method \= Column(String(30), default='automated')  \# automated|expert\_verified|user\_identified  
    confidence\_score \= Column(Float, default=0.0)  \# Detection confidence  
    link\_metadata \= Column(JSON, nullable=True)  
      
    created\_at \= Column(DateTime, default=datetime.utcnow)

class LinkographicSession(Base):  
    """Session-level linkographic analysis"""  
    \_\_tablename\_\_ \= "linkographic\_sessions"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    session\_id \= Column(UUID(as\_uuid=True), ForeignKey("sessions.id"), nullable=False)  
      
    \# Basic metrics  
    total\_moves \= Column(Integer, default=0)  
    total\_links \= Column(Integer, default=0)  
    link\_index \= Column(Float, default=0.0)  \# links/moves ratio  
      
    \# Move distribution  
    user\_generated\_moves \= Column(Integer, default=0)  
    ai\_provided\_moves \= Column(Integer, default=0)  
    ai\_prompted\_moves \= Column(Integer, default=0)  
    hybrid\_moves \= Column(Integer, default=0)  
      
    \# AI influence metrics  
    ai\_dependency\_ratio \= Column(Float, default=0.0)  
    cognitive\_offloading\_factor \= Column(Float, default=0.0)  
    scaffolding\_effectiveness \= Column(Float, default=0.0)  
      
    \# Network metrics  
    critical\_move\_count \= Column(Integer, default=0)  
    critical\_move\_density \= Column(Float, default=0.0)  
    cross\_phase\_links \= Column(Integer, default=0)  
    semantic\_link\_density \= Column(Float, default=0.0)  
    temporal\_link\_density \= Column(Float, default=0.0)  
      
    \# Cognitive development indicators  
    concept\_synthesis\_evolution \= Column(Float, default=0.0)  
    evaluation\_sophistication \= Column(Float, default=0.0)  
    metacognitive\_awareness \= Column(Float, default=0.0)  
    knowledge\_integration\_depth \= Column(Float, default=0.0)  
      
    \# Comparative metrics (vs baseline/MENTOR)  
    relative\_link\_richness \= Column(Float, default=0.0)  
    relative\_critical\_density \= Column(Float, default=0.0)  
    relative\_ai\_dependency \= Column(Float, default=0.0)  
      
    \# Analysis metadata  
    analysis\_timestamp \= Column(DateTime, default=datetime.utcnow)  
    analysis\_version \= Column(String(20), default='1.0')  
    quality\_flags \= Column(JSON, nullable=True)  
      
    created\_at \= Column(DateTime, default=datetime.utcnow)

class GenericAIInteractionMetrics(Base):  
    """Enhanced Generic AI interaction metrics with linkography"""  
    \_\_tablename\_\_ \= "generic\_ai\_interaction\_metrics"  
      
    id \= Column(UUID(as\_uuid=True), primary\_key=True, default=uuid.uuid4)  
    interaction\_id \= Column(UUID(as\_uuid=True), ForeignKey("interactions.id"), nullable=False)  
      
    \# Basic AI metrics  
    response\_directness \= Column(Float, nullable=False)  
    information\_density \= Column(Float, nullable=False)  
    suggestion\_count \= Column(Integer, nullable=False)  
    example\_count \= Column(Integer, nullable=False)  
    question\_count \= Column(Integer, nullable=False)  
    technical\_depth \= Column(Float, nullable=False)  
    scaffolding\_present \= Column(Boolean, nullable=False)  
    token\_efficiency \= Column(Float, nullable=False)  
      
    \# Model and performance data  
    model\_used \= Column(String(20), nullable=False)  
    tokens\_input \= Column(Integer, nullable=True)  
    tokens\_output \= Column(Integer, nullable=True)  
    api\_response\_time \= Column(Float, nullable=True)  
      
    \# Enhanced linkographic metrics  
    moves\_generated \= Column(Integer, default=0)  
    user\_moves\_triggered \= Column(Integer, default=0)  
    links\_created \= Column(Integer, default=0)  
    critical\_moves\_influenced \= Column(Integer, default=0)  
      
    \# Cognitive impact assessment  
    cognitive\_load\_reduction \= Column(Float, default=0.0)  
    knowledge\_transfer\_efficiency \= Column(Float, default=0.0)  
    creativity\_support\_score \= Column(Float, default=0.0)  
    reflection\_stimulation \= Column(Float, default=0.0)  
      
    \# Linkographic influence patterns  
    semantic\_link\_influence \= Column(Float, default=0.0)  
    temporal\_disruption\_score \= Column(Float, default=0.0)  
    cross\_phase\_bridge\_count \= Column(Integer, default=0)  
    innovation\_catalyst\_score \= Column(Float, default=0.0)  
      
    \# Comparative flags  
    comparative\_flags \= Column(JSON, nullable=True)  
    linkographic\_metadata \= Column(JSON, nullable=True)  
      
    created\_at \= Column(DateTime, default=datetime.utcnow)

### **2\. Advanced Move Parser for Generic AI**

**File: `src/linkography/enhanced_move_parser.py`**

"""  
Enhanced Move Parser for Generic AI Interactions  
Extracts and classifies design moves with AI influence analysis  
"""

import re  
import spacy  
import numpy as np  
from typing import List, Dict, Any, Tuple  
from sentence\_transformers import SentenceTransformer  
from datetime import datetime  
import json  
import logging

logger \= logging.getLogger(\_\_name\_\_)

class EnhancedMoveParser:  
    """Advanced move parsing with AI influence detection"""  
      
    def \_\_init\_\_(self):  
        self.nlp \= spacy.load("en\_core\_web\_sm")  
        self.sentence\_model \= SentenceTransformer('all-MiniLM-L6-v2')  
          
        \# Move classification patterns  
        self.move\_patterns \= {  
            'analysis': \[  
                r'analyze|examine|evaluate|assess|consider|investigate',  
                r'what.\*(?:problems?|issues?|challenges?)',  
                r'how.\*(?:works?|functions?|performs?)'  
            \],  
            'synthesis': \[  
                r'combine|integrate|merge|unify|synthesize',  
                r'bring together|put together|connect',  
                r'relationship between|connection.\*between'  
            \],  
            'evaluation': \[  
                r'judge|rate|score|rank|compare|contrast',  
                r'better|worse|superior|inferior|optimal',  
                r'pros? and cons?|advantages? and disadvantages?'  
            \],  
            'transformation': \[  
                r'modify|change|alter|adapt|transform|convert',  
                r'instead of|rather than|alternative',  
                r'redesign|rework|revise'  
            \],  
            'reflection': \[  
                r'think about|reflect on|consider',  
                r'what if|suppose|imagine',  
                r'learn.\*from|takeaway|insight'  
            \]  
        }  
          
        \# AI influence indicators  
        self.ai\_influence\_patterns \= {  
            'direct\_provision': \[  
                r'here.\*solution|solution.\*is|answer.\*is',  
                r'you should|recommend.\*that|suggest.\*that',  
                r'best approach.\*is|optimal.\*is'  
            \],  
            'guided\_discovery': \[  
                r'what.\*think about|consider.\*whether',  
                r'have you.\*considered|might.\*explore',  
                r'questions?.\*to ask'  
            \],  
            'information\_delivery': \[  
                r'research shows|studies indicate|according to',  
                r'typically|generally|commonly|usually',  
                r'standards|guidelines|requirements'  
            \]  
        }  
          
        \# Design focus categories  
        self.design\_focus\_patterns \= {  
            'function': \[r'function|purpose|use|activity|program|space'\],  
            'form': \[r'shape|form|geometry|proportion|scale|massing'\],  
            'structure': \[r'structure|structural|support|load|beam|column'\],  
            'material': \[r'material|concrete|steel|wood|glass|facade'\],  
            'environment': \[r'environment|climate|energy|sustainable|green'\],  
            'culture': \[r'culture|community|social|cultural|neighborhood'\]  
        }  
      
    async def extract\_moves\_from\_interaction(self,   
                                           user\_input: str,  
                                           ai\_response: str,  
                                           interaction\_context: Dict\[str, Any\]) \-\> List\[Dict\[str, Any\]\]:  
        """Extract design moves from user-AI interaction"""  
          
        moves \= \[\]  
          
        \# Parse user input for moves  
        user\_moves \= await self.\_parse\_text\_for\_moves(  
            user\_input,   
            source='user\_generated',  
            context=interaction\_context  
        )  
        moves.extend(user\_moves)  
          
        \# Parse AI response for moves  
        ai\_moves \= await self.\_parse\_ai\_response\_for\_moves(  
            ai\_response,  
            user\_input,  
            context=interaction\_context  
        )  
        moves.extend(ai\_moves)  
          
        \# Analyze AI influence on user moves  
        moves \= await self.\_analyze\_ai\_influence(moves, user\_input, ai\_response)  
          
        return moves  
      
    async def \_parse\_text\_for\_moves(self,   
                                  text: str,   
                                  source: str,  
                                  context: Dict\[str, Any\]) \-\> List\[Dict\[str, Any\]\]:  
        """Parse text into discrete design moves"""  
          
        doc \= self.nlp(text)  
        sentences \= \[sent.text.strip() for sent in doc.sents if sent.text.strip()\]  
          
        moves \= \[\]  
        for i, sentence in enumerate(sentences):  
            move \= await self.\_classify\_move(sentence, source, context)  
            move\['sequence\_number'\] \= i  
            move\['timestamp'\] \= datetime.now().isoformat()  
            moves.append(move)  
          
        return moves  
      
    async def \_parse\_ai\_response\_for\_moves(self,  
                                         ai\_response: str,  
                                         user\_input: str,  
                                         context: Dict\[str, Any\]) \-\> List\[Dict\[str, Any\]\]:  
        """Parse AI response with special handling for different response types"""  
          
        doc \= self.nlp(ai\_response)  
        sentences \= \[sent.text.strip() for sent in doc.sents if sent.text.strip()\]  
          
        moves \= \[\]  
        for i, sentence in enumerate(sentences):  
            \# Determine AI response type  
            ai\_response\_type \= await self.\_classify\_ai\_response\_type(sentence, user\_input)  
              
            move \= await self.\_classify\_move(sentence, ai\_response\_type, context)  
            move\['sequence\_number'\] \= i \+ 1000  \# Offset to distinguish from user moves  
            move\['timestamp'\] \= datetime.now().isoformat()  
            move\['ai\_specific\_metrics'\] \= await self.\_calculate\_ai\_move\_metrics(sentence, user\_input)  
              
            moves.append(move)  
          
        return moves  
      
    async def \_classify\_move(self,   
                           sentence: str,   
                           source: str,  
                           context: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Classify individual move with enhanced attributes"""  
          
        sentence\_lower \= sentence.lower()  
          
        \# Determine move type  
        move\_type \= 'analysis'  \# default  
        for type\_name, patterns in self.move\_patterns.items():  
            if any(re.search(pattern, sentence\_lower) for pattern in patterns):  
                move\_type \= type\_name  
                break  
          
        \# Determine design focus  
        design\_focus \= 'function'  \# default  
        for focus\_name, patterns in self.design\_focus\_patterns.items():  
            if any(re.search(pattern, sentence\_lower) for pattern in patterns):  
                design\_focus \= focus\_name  
                break  
          
        \# Calculate complexity metrics  
        complexity\_metrics \= await self.\_calculate\_complexity\_metrics(sentence)  
          
        \# Determine cognitive operation  
        cognitive\_operation \= await self.\_determine\_cognitive\_operation(sentence, source)  
          
        move \= {  
            'move\_id': f"move\_{datetime.now().timestamp()}\_{hash(sentence) % 10000}",  
            'content': sentence,  
            'move\_type': move\_type,  
            'phase': context.get('phase', 'ideation'),  
            'modality': context.get('modality', 'text'),  
            'cognitive\_operation': cognitive\_operation,  
            'design\_focus': design\_focus,  
            'move\_source': source,  
            'cognitive\_load': complexity\_metrics\['cognitive\_load'\],  
            'directness\_level': complexity\_metrics\['directness\_level'\],  
            'linguistic\_complexity': complexity\_metrics\['linguistic\_complexity'\],  
            'technical\_depth': complexity\_metrics\['technical\_depth'\],  
            'context': context  
        }  
          
        return move  
      
    async def \_classify\_ai\_response\_type(self, sentence: str, user\_input: str) \-\> str:  
        """Classify AI response type for source attribution"""  
          
        sentence\_lower \= sentence.lower()  
          
        \# Check for direct solution provision  
        for pattern in self.ai\_influence\_patterns\['direct\_provision'\]:  
            if re.search(pattern, sentence\_lower):  
                return 'ai\_provided'  
          
        \# Check for guided discovery  
        for pattern in self.ai\_influence\_patterns\['guided\_discovery'\]:  
            if re.search(pattern, sentence\_lower):  
                return 'ai\_prompted'  
          
        \# Check for information delivery  
        for pattern in self.ai\_influence\_patterns\['information\_delivery'\]:  
            if re.search(pattern, sentence\_lower):  
                return 'ai\_provided'  
          
        \# Default to AI provided if unclear  
        return 'ai\_provided'  
      
    async def \_calculate\_complexity\_metrics(self, sentence: str) \-\> Dict\[str, Any\]:  
        """Calculate complexity and quality metrics for move"""  
          
        import textstat  
          
        \# Linguistic complexity  
        flesch\_score \= textstat.flesch\_reading\_ease(sentence)  
        linguistic\_complexity \= max(0, (100 \- flesch\_score) / 100\)  
          
        \# Determine cognitive load based on complexity indicators  
        cognitive\_load \= 'low'  
        if any(word in sentence.lower() for word in \['analyze', 'synthesize', 'evaluate', 'integrate', 'optimize'\]):  
            cognitive\_load \= 'high'  
        elif any(word in sentence.lower() for word in \['consider', 'compare', 'assess', 'relate'\]):  
            cognitive\_load \= 'medium'  
          
        \# Determine directness level  
        directness\_level \= 'exploratory'  
        if any(phrase in sentence.lower() for phrase in \['should', 'must', 'need to', 'have to'\]):  
            directness\_level \= 'direct\_answer'  
        elif any(phrase in sentence.lower() for phrase in \['consider', 'might', 'could', 'what if'\]):  
            directness\_level \= 'guided\_discovery'  
          
        \# Technical depth based on domain-specific terms  
        technical\_terms \= \['structural', 'mechanical', 'environmental', 'sustainable', 'energy', 'load', 'thermal'\]  
        technical\_depth \= sum(1 for term in technical\_terms if term in sentence.lower()) / len(technical\_terms)  
          
        return {  
            'cognitive\_load': cognitive\_load,  
            'directness\_level': directness\_level,  
            'linguistic\_complexity': linguistic\_complexity,  
            'technical\_depth': technical\_depth  
        }  
      
    async def \_determine\_cognitive\_operation(self, sentence: str, source: str) \-\> str:  
        """Determine the cognitive operation of the move"""  
          
        sentence\_lower \= sentence.lower()  
          
        if any(word in sentence\_lower for word in \['propose', 'suggest', 'design', 'create'\]):  
            return 'proposal'  
        elif any(word in sentence\_lower for word in \['clarify', 'explain', 'define', 'describe'\]):  
            return 'clarification'  
        elif any(word in sentence\_lower for word in \['assess', 'evaluate', 'judge', 'critique'\]):  
            return 'assessment'  
        elif any(word in sentence\_lower for word in \['support', 'evidence', 'because', 'therefore'\]):  
            return 'support'  
        elif any(word in sentence\_lower for word in \['reference', 'example', 'precedent', 'case'\]):  
            return 'reference'  
        else:  
            return 'proposal'  \# default  
      
    async def \_calculate\_ai\_move\_metrics(self, sentence: str, user\_input: str) \-\> Dict\[str, Any\]:  
        """Calculate AI-specific metrics for moves"""  
          
        \# Response directness  
        directness\_indicators \= \['should', 'must', 'recommend', 'best', 'optimal'\]  
        response\_directness \= sum(1 for indicator in directness\_indicators if indicator in sentence.lower()) / len(sentence.split())  
          
        \# Information density  
        info\_indicators \= \['research', 'study', 'data', 'standard', 'guideline', 'typically'\]  
        information\_density \= sum(1 for indicator in info\_indicators if indicator in sentence.lower()) / len(sentence.split())  
          
        \# Solution completeness  
        solution\_indicators \= \['solution', 'answer', 'approach', 'method', 'strategy'\]  
        solution\_completeness \= min(1.0, sum(1 for indicator in solution\_indicators if indicator in sentence.lower()) / 2\)  
          
        \# Scaffolding presence (questions, prompts)  
        scaffolding\_present \= '?' in sentence or any(word in sentence.lower() for word in \['consider', 'think about', 'what if'\])  
          
        return {  
            'response\_directness': response\_directness,  
            'information\_density': information\_density,  
            'solution\_completeness': solution\_completeness,  
            'scaffolding\_presence': scaffolding\_present,  
            'cognitive\_offloading\_risk': response\_directness \* solution\_completeness  
        }  
      
    async def \_analyze\_ai\_influence(self,   
                                  moves: List\[Dict\[str, Any\]\],   
                                  user\_input: str,   
                                  ai\_response: str) \-\> List\[Dict\[str, Any\]\]:  
        """Analyze AI influence patterns across moves"""  
          
        \# Calculate influence strength for each move  
        for move in moves:  
            if move\['move\_source'\].startswith('ai\_'):  
                move\['ai\_influence\_strength'\] \= 1.0  
            else:  
                \# Calculate based on how much AI response influenced user move  
                semantic\_similarity \= await self.\_calculate\_semantic\_similarity(  
                    move\['content'\], ai\_response  
                )  
                move\['ai\_influence\_strength'\] \= semantic\_similarity  
          
        return moves  
      
    async def \_calculate\_semantic\_similarity(self, text1: str, text2: str) \-\> float:  
        """Calculate semantic similarity between texts"""  
          
        embeddings \= self.sentence\_model.encode(\[text1, text2\])  
        similarity \= np.dot(embeddings\[0\], embeddings\[1\]) / (  
            np.linalg.norm(embeddings\[0\]) \* np.linalg.norm(embeddings\[1\])  
        )  
        return float(similarity)

### **3\. Linkographic Analysis Engine**

**File: `src/linkography/linkographic_analyzer.py`**

"""  
Linkographic Analysis Engine for Generic AI  
Calculates semantic and temporal links between design moves  
"""

import numpy as np  
import networkx as nx  
from typing import List, Dict, Any, Tuple  
from sentence\_transformers import SentenceTransformer  
from sklearn.metrics.pairwise import cosine\_similarity  
from datetime import datetime, timedelta  
import json  
import logging

logger \= logging.getLogger(\_\_name\_\_)

class LinkographicAnalyzer:  
    """Advanced linkographic analysis with AI influence tracking"""  
      
    def \_\_init\_\_(self):  
        self.sentence\_model \= SentenceTransformer('all-MiniLM-L6-v2')  
        self.semantic\_threshold \= 0.3  \# Minimum similarity for semantic links  
        self.temporal\_window \= 10  \# Maximum temporal distance for links  
          
    async def analyze\_session\_linkography(self,   
                                        moves: List\[Dict\[str, Any\]\],   
                                        session\_context: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Complete linkographic analysis for a session"""  
          
        \# Calculate all possible links  
        links \= await self.\_calculate\_all\_links(moves)  
          
        \# Build linkographic network  
        network \= await self.\_build\_linkographic\_network(moves, links)  
          
        \# Calculate network metrics  
        network\_metrics \= await self.\_calculate\_network\_metrics(network, moves, links)  
          
        \# Analyze AI influence patterns  
        ai\_patterns \= await self.\_analyze\_ai\_influence\_patterns(moves, links, network)  
          
        \# Identify critical moves  
        critical\_moves \= await self.\_identify\_critical\_moves(moves, links, network)  
          
        \# Calculate cognitive development indicators  
        cognitive\_indicators \= await self.\_calculate\_cognitive\_development\_indicators(  
            moves, links, network, session\_context  
        )  
          
        return {  
            'moves': moves,  
            'links': links,  
            'network': network,  
            'network\_metrics': network\_metrics,  
            'ai\_influence\_patterns': ai\_patterns,  
            'critical\_moves': critical\_moves,  
            'cognitive\_development\_indicators': cognitive\_indicators,  
            'session\_summary': await self.\_generate\_session\_summary(  
                moves, links, network\_metrics, ai\_patterns  
            )  
        }  
      
    async def \_calculate\_all\_links(self, moves: List\[Dict\[str, Any\]\]) \-\> List\[Dict\[str, Any\]\]:  
        """Calculate semantic and temporal links between all moves"""  
          
        links \= \[\]  
          
        \# Extract move contents and embeddings  
        move\_contents \= \[move\['content'\] for move in moves\]  
        embeddings \= self.sentence\_model.encode(move\_contents)  
          
        \# Calculate pairwise similarities  
        similarity\_matrix \= cosine\_similarity(embeddings)  
          
        for i, move\_i in enumerate(moves):  
            for j, move\_j in enumerate(moves):  
                if i \>= j:  \# Only consider forward links  
                    continue  
                  
                \# Calculate semantic similarity  
                semantic\_sim \= similarity\_matrix\[i\]\[j\]  
                  
                \# Calculate temporal distance  
                time\_i \= datetime.fromisoformat(move\_i\['timestamp'\])  
                time\_j \= datetime.fromisoformat(move\_j\['timestamp'\])  
                temporal\_distance \= abs((time\_j \- time\_i).total\_seconds())  
                temporal\_distance\_moves \= j \- i  
                  
                \# Determine if link should be created  
                if semantic\_sim \>= self.semantic\_threshold or temporal\_distance\_moves \<= 3:  
                    link \= await self.\_create\_link(  
                        move\_i, move\_j, semantic\_sim, temporal\_distance\_moves  
                    )  
                    links.append(link)  
          
        return links  
      
    async def \_create\_link(self,   
                         source\_move: Dict\[str, Any\],   
                         target\_move: Dict\[str, Any\],  
                         semantic\_similarity: float,  
                         temporal\_distance: int) \-\> Dict\[str, Any\]:  
        """Create a link between two moves"""  
          
        \# Determine link type  
        link\_type \= await self.\_classify\_link\_type(source\_move, target\_move, semantic\_similarity)  
          
        \# Calculate link strength  
        link\_strength \= await self.\_calculate\_link\_strength(  
            source\_move, target\_move, semantic\_similarity, temporal\_distance  
        )  
          
        \# Analyze AI mediation  
        ai\_mediated \= await self.\_is\_ai\_mediated\_link(source\_move, target\_move)  
          
        \# Check for cross-modal and phase-bridging links  
        cross\_modal \= source\_move\['modality'\] \!= target\_move\['modality'\]  
        phase\_bridging \= source\_move\['phase'\] \!= target\_move\['phase'\]  
          
        link \= {  
            'link\_id': f"link\_{source\_move\['move\_id'\]}\_{target\_move\['move\_id'\]}",  
            'source\_move\_id': source\_move\['move\_id'\],  
            'target\_move\_id': target\_move\['move\_id'\],  
            'link\_type': link\_type,  
            'link\_strength': link\_strength,  
            'semantic\_similarity': semantic\_similarity,  
            'temporal\_distance': temporal\_distance,  
            'conceptual\_distance': 1.0 \- semantic\_similarity,  
            'ai\_mediated': ai\_mediated,  
            'cross\_modal': cross\_modal,  
            'phase\_bridging': phase\_bridging,  
            'detection\_method': 'automated',  
            'confidence\_score': semantic\_similarity,  
            'created\_at': datetime.now().isoformat()  
        }  
          
        return link  
      
    async def \_classify\_link\_type(self,   
                                source\_move: Dict\[str, Any\],   
                                target\_move: Dict\[str, Any\],  
                                semantic\_similarity: float) \-\> str:  
        """Classify the type of link between moves"""  
          
        \# Temporal links (adjacent moves)  
        if abs(int(target\_move\['sequence\_number'\]) \- int(source\_move\['sequence\_number'\])) \<= 1:  
            return 'temporal'  
          
        \# High semantic similarity  
        if semantic\_similarity \> 0.7:  
            \# Check for refinement patterns  
            if (source\_move\['move\_type'\] \== target\_move\['move\_type'\] and  
                source\_move\['design\_focus'\] \== target\_move\['design\_focus'\]):  
                return 'refinement'  
            else:  
                return 'semantic'  
          
        \# Medium semantic similarity  
        elif semantic\_similarity \> 0.4:  
            \# Check for causal relationships  
            if (source\_move\['cognitive\_operation'\] \== 'assessment' and  
                target\_move\['cognitive\_operation'\] \== 'proposal'):  
                return 'causal'  
            else:  
                return 'semantic'  
          
        \# Lower similarity but still connected  
        else:  
            return 'semantic'  
      
    async def \_calculate\_link\_strength(self,   
                                     source\_move: Dict\[str, Any\],   
                                     target\_move: Dict\[str, Any\],  
                                     semantic\_similarity: float,  
                                     temporal\_distance: int) \-\> float:  
        """Calculate the strength of a link"""  
          
        \# Base strength from semantic similarity  
        base\_strength \= semantic\_similarity  
          
        \# Temporal proximity bonus  
        temporal\_bonus \= max(0, (10 \- temporal\_distance) / 10\) \* 0.2  
          
        \# Same design focus bonus  
        focus\_bonus \= 0.1 if source\_move\['design\_focus'\] \== target\_move\['design\_focus'\] else 0  
          
        \# Cognitive operation compatibility bonus  
        operation\_bonus \= await self.\_calculate\_operation\_compatibility\_bonus(  
            source\_move\['cognitive\_operation'\], target\_move\['cognitive\_operation'\]  
        )  
          
        \# AI influence penalty/bonus  
        ai\_adjustment \= await self.\_calculate\_ai\_influence\_adjustment(source\_move, target\_move)  
          
        total\_strength \= min(1.0, base\_strength \+ temporal\_bonus \+ focus\_bonus \+ operation\_bonus \+ ai\_adjustment)  
          
        return total\_strength  
      
    async def \_calculate\_operation\_compatibility\_bonus(self, source\_op: str, target\_op: str) \-\> float:  
        """Calculate bonus for compatible cognitive operations"""  
          
        compatibility\_matrix \= {  
            ('proposal', 'assessment'): 0.15,  
            ('assessment', 'proposal'): 0.10,  
            ('proposal', 'support'): 0.12,  
            ('clarification', 'proposal'): 0.08,  
            ('reference', 'proposal'): 0.10,  
            ('assessment', 'support'): 0.08  
        }  
          
        return compatibility\_matrix.get((source\_op, target\_op), 0.0)  
      
    async def \_calculate\_ai\_influence\_adjustment(self, source\_move: Dict\[str, Any\], target\_move: Dict\[str, Any\]) \-\> float:  
        """Calculate adjustment based on AI influence"""  
          
        \# If AI provides direct solution that user adopts, strengthen link  
        if (source\_move\['move\_source'\].startswith('ai\_') and   
            target\_move\['move\_source'\] \== 'user\_generated' and  
            source\_move.get('ai\_specific\_metrics', {}).get('solution\_completeness', 0\) \> 0.7):  
            return \-0.1  \# Penalty for potential cognitive offloading  
          
        \# If AI guides discovery that leads to user insight, bonus  
        if (source\_move\['move\_source'\] \== 'ai\_prompted' and  
            target\_move\['move\_source'\] \== 'user\_generated' and  
            target\_move\['move\_type'\] in \['synthesis', 'evaluation'\]):  
            return 0.1  \# Bonus for guided discovery  
          
        return 0.0  
      
    async def \_is\_ai\_mediated\_link(self, source\_move: Dict\[str, Any\], target\_move: Dict\[str, Any\]) \-\> bool:  
        """Determine if link involves AI mediation"""  
          
        return (source\_move\['move\_source'\].startswith('ai\_') or   
                target\_move\['move\_source'\].startswith('ai\_') or  
                (source\_move\['move\_source'\] \== 'user\_generated' and   
                 target\_move\['move\_source'\] \== 'user\_generated' and  
                 \# Check if there's an AI move between them  
                 abs(int(target\_move\['sequence\_number'\]) \- int(source\_move\['sequence\_number'\])) \> 2))  
      
    async def \_build\_linkographic\_network(self,   
                                        moves: List\[Dict\[str, Any\]\],   
                                        links: List\[Dict\[str, Any\]\]) \-\> nx.DiGraph:  
        """Build NetworkX graph for analysis"""  
          
        G \= nx.DiGraph()  
          
        \# Add nodes (moves)  
        for move in moves:  
            G.add\_node(move\['move\_id'\], \*\*move)  
          
        \# Add edges (links)  
        for link in links:  
            G.add\_edge(  
                link\['source\_move\_id'\],   
                link\['target\_move\_id'\],  
                \*\*link  
            )  
          
        return G  
      
    async def \_calculate\_network\_metrics(self,   
                                       network: nx.DiGraph,   
                                       moves: List\[Dict\[str, Any\]\],   
                                       links: List\[Dict\[str, Any\]\]) \-\> Dict\[str, Any\]:  
        """Calculate comprehensive network metrics"""  
          
        total\_moves \= len(moves)  
        total\_links \= len(links)  
          
        \# Basic metrics  
        link\_index \= total\_links / total\_moves if total\_moves \> 0 else 0  
          
        \# Move distribution  
        move\_sources \= \[move\['move\_source'\] for move in moves\]  
        user\_generated \= sum(1 for source in move\_sources if source \== 'user\_generated')  
        ai\_provided \= sum(1 for source in move\_sources if source \== 'ai\_provided')  
        ai\_prompted \= sum(1 for source in move\_sources if source \== 'ai\_prompted')  
        hybrid\_moves \= sum(1 for source in move\_sources if source \== 'hybrid')  
          
        \# AI influence metrics  
        ai\_dependency\_ratio \= (ai\_provided \+ ai\_prompted) / total\_moves if total\_moves \> 0 else 0  
          
        \# Network connectivity metrics  
        if total\_moves \> 1:  
            try:  
                avg\_clustering \= nx.average\_clustering(network.to\_undirected())  
                density \= nx.density(network)  
                  
                \# Centrality measures  
                in\_degree\_centrality \= nx.in\_degree\_centrality(network)  
                out\_degree\_centrality \= nx.out\_degree\_centrality(network)  
                betweenness\_centrality \= nx.betweenness\_centrality(network)  
                  
                \# Identify highly connected nodes  
                high\_centrality\_nodes \= \[  
                    node for node, centrality in betweenness\_centrality.items()   
                    if centrality \> 0.1  
                \]  
                  
            except:  
                avg\_clustering \= 0  
                density \= 0  
                high\_centrality\_nodes \= \[\]  
        else:  
            avg\_clustering \= 0  
            density \= 0  
            high\_centrality\_nodes \= \[\]  
          
        \# Link type distribution  
        link\_types \= \[link\['link\_type'\] for link in links\]  
        semantic\_links \= sum(1 for lt in link\_types if lt \== 'semantic')  
        temporal\_links \= sum(1 for lt in link\_types if lt \== 'temporal')  
        causal\_links \= sum(1 for lt in link\_types if lt \== 'causal')  
        refinement\_links \= sum(1 for lt in link\_types if lt \== 'refinement')  
          
        \# AI-mediated link analysis  
        ai\_mediated\_links \= sum(1 for link in links if link\['ai\_mediated'\])  
        cross\_modal\_links \= sum(1 for link in links if link\['cross\_modal'\])  
        phase\_bridging\_links \= sum(1 for link in links if link\['phase\_bridging'\])  
          
        return {  
            'total\_moves': total\_moves,  
            'total\_links': total\_links,  
            'link\_index': link\_index,  
            'user\_generated\_moves': user\_generated,  
            'ai\_provided\_moves': ai\_provided,  
            'ai\_prompted\_moves': ai\_prompted,  
            'hybrid\_moves': hybrid\_moves,  
            'ai\_dependency\_ratio': ai\_dependency\_ratio,  
            'average\_clustering': avg\_clustering,  
            'network\_density': density,  
            'high\_centrality\_nodes': high\_centrality\_nodes,  
            'semantic\_links': semantic\_links,  
            'temporal\_links': temporal\_links,  
            'causal\_links': causal\_links,  
            'refinement\_links': refinement\_links,  
            'ai\_mediated\_links': ai\_mediated\_links,  
            'cross\_modal\_links': cross\_modal\_links,  
            'phase\_bridging\_links': phase\_bridging\_links,  
            'semantic\_link\_density': semantic\_links / total\_moves if total\_moves \> 0 else 0,  
            'temporal\_link\_density': temporal\_links / total\_moves if total\_moves \> 0 else 0  
        }  
      
    async def \_analyze\_ai\_influence\_patterns(self,   
                                           moves: List\[Dict\[str, Any\]\],   
                                           links: List\[Dict\[str, Any\]\],   
                                           network: nx.DiGraph) \-\> Dict\[str, Any\]:  
        """Analyze patterns of AI influence in the design process"""  
          
        ai\_patterns \= {  
            'direct\_solution\_provision': 0,  
            'guided\_discovery\_instances': 0,  
            'information\_scaffolding': 0,  
            'cognitive\_offloading\_risks': \[\],  
            'innovation\_catalysts': \[\],  
            'reflection\_stimulators': \[\]  
        }  
          
        for move in moves:  
            if move\['move\_source'\].startswith('ai\_'):  
                ai\_metrics \= move.get('ai\_specific\_metrics', {})  
                  
                \# Count direct solution provision  
                if ai\_metrics.get('solution\_completeness', 0\) \> 0.7:  
                    ai\_patterns\['direct\_solution\_provision'\] \+= 1  
                  
                \# Count guided discovery  
                if move\['move\_source'\] \== 'ai\_prompted':  
                    ai\_patterns\['guided\_discovery\_instances'\] \+= 1  
                  
                \# Count information scaffolding  
                if ai\_metrics.get('information\_density', 0\) \> 0.5:  
                    ai\_patterns\['information\_scaffolding'\] \+= 1  
                  
                \# Identify cognitive offloading risks  
                if ai\_metrics.get('cognitive\_offloading\_risk', 0\) \> 0.6:  
                    ai\_patterns\['cognitive\_offloading\_risks'\].append(move\['move\_id'\])  
          
        \# Analyze follow-up moves after AI interactions  
        for link in links:  
            if link\['ai\_mediated'\]:  
                source\_move \= next((m for m in moves if m\['move\_id'\] \== link\['source\_move\_id'\]), None)  
                target\_move \= next((m for m in moves if m\['move\_id'\] \== link\['target\_move\_id'\]), None)  
                  
                if source\_move and target\_move:  
                    \# Check for innovation catalysis  
                    if (source\_move\['move\_source'\].startswith('ai\_') and  
                        target\_move\['move\_source'\] \== 'user\_generated' and  
                        target\_move\['move\_type'\] \== 'synthesis'):  
                        ai\_patterns\['innovation\_catalysts'\].append(link\['link\_id'\])  
                      
                    \# Check for reflection stimulation  
                    if (source\_move\['move\_source'\] \== 'ai\_prompted' and  
                        target\_move\['move\_type'\] \== 'reflection'):  
                        ai\_patterns\['reflection\_stimulators'\].append(link\['link\_id'\])  
          
        return ai\_patterns  
      
    async def \_identify\_critical\_moves(self,   
                                     moves: List\[Dict\[str, Any\]\],   
                                     links: List\[Dict\[str, Any\]\],   
                                     network: nx.DiGraph) \-\> List\[Dict\[str, Any\]\]:  
        """Identify critical moves with high impact on design process"""  
          
        critical\_moves \= \[\]  
          
        \# Calculate centrality measures  
        try:  
            betweenness\_centrality \= nx.betweenness\_centrality(network)  
            degree\_centrality \= nx.degree\_centrality(network.to\_undirected())  
              
            \# Identify moves with high centrality  
            for move in moves:  
                move\_id \= move\['move\_id'\]  
                  
                betweenness \= betweenness\_centrality.get(move\_id, 0\)  
                degree \= degree\_centrality.get(move\_id, 0\)  
                  
                \# Count forward and backward links  
                forward\_links \= len(\[l for l in links if l\['source\_move\_id'\] \== move\_id\])  
                backward\_links \= len(\[l for l in links if l\['target\_move\_id'\] \== move\_id\])  
                  
                \# Determine criticality  
                is\_critical \= (  
                    betweenness \> 0.1 or   
                    degree \> 0.3 or   
                    forward\_links \> 3 or  
                    (move\['move\_type'\] \== 'synthesis' and forward\_links \> 1\)  
                )  
                  
                if is\_critical:  
                    critical\_move \= {  
                        'move\_id': move\_id,  
                        'move': move,  
                        'betweenness\_centrality': betweenness,  
                        'degree\_centrality': degree,  
                        'forward\_links': forward\_links,  
                        'backward\_links': backward\_links,  
                        'criticality\_score': betweenness \+ degree \* 0.5 \+ forward\_links \* 0.1  
                    }  
                    critical\_moves.append(critical\_move)  
          
        except Exception as e:  
            logger.warning(f"Error calculating critical moves: {e}")  
          
        \# Sort by criticality score  
        critical\_moves.sort(key=lambda x: x\['criticality\_score'\], reverse=True)  
          
        return critical\_moves  
      
    async def \_calculate\_cognitive\_development\_indicators(self,   
                                                        moves: List\[Dict\[str, Any\]\],   
                                                        links: List\[Dict\[str, Any\]\],   
                                                        network: nx.DiGraph,  
                                                        session\_context: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Calculate indicators of cognitive development through the session"""  
          
        \# Group moves by phase  
        phases \= \['ideation', 'visualization', 'materialization'\]  
        phase\_moves \= {phase: \[m for m in moves if m\['phase'\] \== phase\] for phase in phases}  
          
        indicators \= {}  
          
        \# Concept synthesis evolution  
        synthesis\_moves\_by\_phase \= {  
            phase: \[m for m in phase\_moves\[phase\] if m\['move\_type'\] \== 'synthesis'\]  
            for phase in phases  
        }  
          
        concept\_synthesis\_evolution \= \[\]  
        for phase in phases:  
            synthesis\_count \= len(synthesis\_moves\_by\_phase\[phase\])  
            avg\_complexity \= np.mean(\[  
                m.get('linguistic\_complexity', 0\) for m in synthesis\_moves\_by\_phase\[phase\]  
            \]) if synthesis\_moves\_by\_phase\[phase\] else 0  
              
            concept\_synthesis\_evolution.append({  
                'phase': phase,  
                'synthesis\_count': synthesis\_count,  
                'average\_complexity': avg\_complexity  
            })  
          
        indicators\['concept\_synthesis\_evolution'\] \= concept\_synthesis\_evolution  
          
        \# Evaluation sophistication  
        evaluation\_moves \= \[m for m in moves if m\['move\_type'\] \== 'evaluation'\]  
        evaluation\_sophistication \= np.mean(\[  
            m.get('technical\_depth', 0\) \+ m.get('linguistic\_complexity', 0\)  
            for m in evaluation\_moves  
        \]) if evaluation\_moves else 0  
          
        indicators\['evaluation\_sophistication'\] \= evaluation\_sophistication  
          
        \# Metacognitive awareness  
        reflection\_moves \= \[m for m in moves if m\['move\_type'\] \== 'reflection'\]  
        metacognitive\_awareness \= len(reflection\_moves) / len(moves) if moves else 0  
          
        indicators\['metacognitive\_awareness'\] \= metacognitive\_awareness  
          
        \# Knowledge integration depth  
        cross\_focus\_links \= \[  
            l for l in links   
            if any(m\['move\_id'\] \== l\['source\_move\_id'\] and   
                   any(m2\['move\_id'\] \== l\['target\_move\_id'\] and   
                       m\['design\_focus'\] \!= m2\['design\_focus'\] for m2 in moves)  
                   for m in moves)  
        \]  
          
        knowledge\_integration\_depth \= len(cross\_focus\_links) / len(links) if links else 0  
          
        indicators\['knowledge\_integration\_depth'\] \= knowledge\_integration\_depth  
          
        return indicators  
      
    async def \_generate\_session\_summary(self,   
                                       moves: List\[Dict\[str, Any\]\],   
                                       links: List\[Dict\[str, Any\]\],   
                                       network\_metrics: Dict\[str, Any\],  
                                       ai\_patterns: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
        """Generate comprehensive session summary"""  
          
        return {  
            'session\_overview': {  
                'total\_moves': len(moves),  
                'total\_links': len(links),  
                'link\_index': network\_metrics\['link\_index'\],  
                'ai\_dependency\_ratio': network\_metrics\['ai\_dependency\_ratio'\]  
            },  
            'process\_characteristics': {  
                'dominant\_move\_types': self.\_get\_dominant\_move\_types(moves),  
                'primary\_design\_focuses': self.\_get\_primary\_design\_focuses(moves),  
                'cognitive\_load\_distribution': self.\_get\_cognitive\_load\_distribution(moves)  
            },  
            'ai\_interaction\_summary': {  
                'direct\_solutions\_provided': ai\_patterns\['direct\_solution\_provision'\],  
                'guided\_discoveries': ai\_patterns\['guided\_discovery\_instances'\],  
                'cognitive\_offloading\_risks': len(ai\_patterns\['cognitive\_offloading\_risks'\]),  
                'innovation\_catalysts': len(ai\_patterns\['innovation\_catalysts'\])  
            },  
            'network\_characteristics': {  
                'connectivity\_level': 'high' if network\_metrics\['network\_density'\] \> 0.3 else 'medium' if network\_metrics\['network\_density'\] \> 0.1 else 'low',  
                'critical\_move\_count': len(network\_metrics.get('high\_centrality\_nodes', \[\])),  
                'cross\_phase\_integration': network\_metrics\['phase\_bridging\_links'\]  
            }  
        }  
      
    def \_get\_dominant\_move\_types(self, moves: List\[Dict\[str, Any\]\]) \-\> List\[Tuple\[str, int\]\]:  
        """Get dominant move types in order of frequency"""  
        move\_type\_counts \= {}  
        for move in moves:  
            move\_type \= move\['move\_type'\]  
            move\_type\_counts\[move\_type\] \= move\_type\_counts.get(move\_type, 0\) \+ 1  
          
        return sorted(move\_type\_counts.items(), key=lambda x: x\[1\], reverse=True)  
      
    def \_get\_primary\_design\_focuses(self, moves: List\[Dict\[str, Any\]\]) \-\> List\[Tuple\[str, int\]\]:  
        """Get primary design focuses in order of frequency"""  
        focus\_counts \= {}  
        for move in moves:  
            focus \= move\['design\_focus'\]  
            focus\_counts\[focus\] \= focus\_counts.get(focus, 0\) \+ 1  
          
        return sorted(focus\_counts.items(), key=lambda x: x\[1\], reverse=True)  
      
    def \_get\_cognitive\_load\_distribution(self, moves: List\[Dict\[str, Any\]\]) \-\> Dict\[str, int\]:  
        """Get distribution of cognitive load levels"""  
        load\_counts \= {'high': 0, 'medium': 0, 'low': 0}  
        for move in moves:  
            load \= move.get('cognitive\_load', 'low')  
            load\_counts\[load\] \+= 1  
          
        return load\_counts

### **4\. Enhanced Generic AI Router with Linkographic Integration**

**File: `src/ai_integrations/enhanced_generic_ai_router.py`**

"""  
Enhanced Generic AI Router with Linkography Integration  
Handles AI interactions while capturing linkographic data  
"""

import asyncio  
import logging  
from typing import Dict, List, Any, Optional  
from datetime import datetime  
import json  
from anthropic import AsyncAnthropic  
import openai

from ..linkography.enhanced\_move\_parser import EnhancedMoveParser  
from ..linkography.linkographic\_analyzer import LinkographicAnalyzer

logger \= logging.getLogger(\_\_name\_\_)

class EnhancedGenericAIRouter:  
    """Enhanced router with linkographic analysis"""  
      
    def \_\_init\_\_(self, config: Dict\[str, Any\]):  
        self.config \= config  
        self.anthropic\_client \= AsyncAnthropic(api\_key=config\['anthropic'\]\['api\_key'\])  
        self.openai\_client \= openai.AsyncOpenAI(api\_key=config\['openai'\]\['api\_key'\])  
        self.current\_model \= config.get('default\_model', 'claude')  
          
        \# Initialize linkographic components  
        self.move\_parser \= EnhancedMoveParser()  
        self.linkographic\_analyzer \= LinkographicAnalyzer()  
          
        \# Enhanced system prompts for linkographic capture  
        self.system\_prompts \= {  
            'claude': self.\_get\_enhanced\_claude\_prompt(),  
            'chatgpt': self.\_get\_enhanced\_chatgpt\_prompt()  
        }  
          
        \# Session move tracking  
        self.session\_moves \= {}  \# session\_id \-\> List\[moves\]  
      
    async def process\_interaction\_with\_linkography(self,   
                                                 session\_id: str,  
                                                 user\_input: str,   
                                                 session\_context: Dict\[str, Any\],  
                                                 phase: str) \-\> Dict\[str, Any\]:  
        """Process interaction with full linkographic analysis"""  
          
        start\_time \= datetime.now()  
          
        try:  
            \# Get AI response  
            ai\_response \= await self.\_get\_ai\_response(user\_input, session\_context, phase)  
              
            \# Extract moves from interaction  
            interaction\_moves \= await self.move\_parser.extract\_moves\_from\_interaction(  
                user\_input=user\_input,  
                ai\_response=ai\_response\['content'\],  
                interaction\_context={  
                    'session\_id': session\_id,  
                    'phase': phase,  
                    'model\_used': self.current\_model,  
                    'timestamp': start\_time.isoformat()  
                }  
            )  
              
            \# Update session move history  
            if session\_id not in self.session\_moves:  
                self.session\_moves\[session\_id\] \= \[\]  
            self.session\_moves\[session\_id\].extend(interaction\_moves)  
              
            \# Perform incremental linkographic analysis  
            linkographic\_update \= await self.\_perform\_incremental\_analysis(  
                session\_id, interaction\_moves  
            )  
              
            \# Calculate interaction-specific metrics  
            interaction\_metrics \= await self.\_calculate\_interaction\_metrics(  
                user\_input, ai\_response, interaction\_moves  
            )  
              
            processing\_time \= (datetime.now() \- start\_time).total\_seconds()  
              
            return {  
                'ai\_response': ai\_response\['content'\],  
                'model\_used': self.current\_model,  
                'processing\_time': processing\_time,  
                'interaction\_moves': interaction\_moves,  
                'linkographic\_update': linkographic\_update,  
                'interaction\_metrics': interaction\_metrics,  
                'timestamp': datetime.now().isoformat(),  
                'metadata': {  
                    'phase': phase,  
                    'approach': 'generic\_ai\_with\_linkography',  
                    'total\_session\_moves': len(self.session\_moves.get(session\_id, \[\]))  
                }  
            }  
              
        except Exception as e:  
            logger.error(f"Enhanced Generic AI processing error: {e}")  
            return {  
                'ai\_response': "I apologize, but I'm having difficulty processing your request right now.",  
                'model\_used': self.current\_model,  
                'processing\_time': 0,  
                'error': str(e),  
                'interaction\_moves': \[\],  
                'linkographic\_update': {},  
                'interaction\_metrics': {}  
            }  
      
    async def \_get\_ai\_response(self, user\_input: str, session\_context: Dict\[str, Any\], phase: str) \-\> Dict\[str, Any\]:  
        """Get response from AI service"""  
          
        \# Prepare context-aware prompt  
        system\_prompt \= self.system\_prompts\[self.current\_model\]  
          
        \# Add phase-specific guidance  
        phase\_guidance \= self.\_get\_phase\_guidance(phase)  
        full\_prompt \= f"{system\_prompt}\\n\\n{phase\_guidance}\\n\\nUser: {user\_input}"  
          
        try:  
            if self.current\_model \== 'claude':  
                response \= await self.anthropic\_client.messages.create(  
                    model="claude-3-sonnet-20240229",  
                    max\_tokens=1000,  
                    temperature=0.7,  
                    system=system\_prompt,  
                    messages=\[{"role": "user", "content": user\_input}\]  
                )  
                return {  
                    'content': response.content\[0\].text,  
                    'usage': {  
                        'input\_tokens': response.usage.input\_tokens,  
                        'output\_tokens': response.usage.output\_tokens  
                    }  
                }  
              
            elif self.current\_model \== 'chatgpt':  
                response \= await self.openai\_client.chat.completions.create(  
                    model="gpt-4",  
                    max\_tokens=1000,  
                    temperature=0.7,  
                    messages=\[  
                        {"role": "system", "content": system\_prompt},  
                        {"role": "user", "content": user\_input}  
                    \]  
                )  
                return {  
                    'content': response.choices\[0\].message.content,  
                    'usage': {  
                        'input\_tokens': response.usage.prompt\_tokens,  
                        'output\_tokens': response.usage.completion\_tokens  
                    }  
                }  
          
        except Exception as e:  
            logger.error(f"AI API error: {e}")  
            raise  
      
    async def \_perform\_incremental\_analysis(self,   
                                          session\_id: str,   
                                          new\_moves: List\[Dict\[str, Any\]\]) \-\> Dict\[str, Any\]:  
        """Perform incremental linkographic analysis with new moves"""  
          
        \# Get all session moves  
        all\_moves \= self.session\_moves.get(session\_id, \[\])  
          
        if len(all\_moves) \< 2:  
            return {'status': 'insufficient\_moves', 'total\_moves': len(all\_moves)}  
          
        \# Calculate new links involving the new moves  
        new\_links \= \[\]  
        for new\_move in new\_moves:  
            \# Check links with existing moves  
            for existing\_move in all\_moves\[:-len(new\_moves)\]:  \# Exclude the new moves themselves  
                if await self.\_should\_create\_link(existing\_move, new\_move):  
                    link \= await self.linkographic\_analyzer.\_create\_link(  
                        existing\_move, new\_move,   
                        await self.\_calculate\_semantic\_similarity(existing\_move\['content'\], new\_move\['content'\]),  
                        abs(int(new\_move\['sequence\_number'\]) \- int(existing\_move\['sequence\_number'\]))  
                    )  
                    new\_links.append(link)  
          
        \# Update session metrics  
        session\_metrics \= await self.\_calculate\_incremental\_session\_metrics(  
            session\_id, new\_moves, new\_links  
        )  
          
        return {  
            'status': 'updated',  
            'new\_moves\_count': len(new\_moves),  
            'new\_links\_count': len(new\_links),  
            'session\_metrics': session\_metrics,  
            'total\_moves': len(all\_moves),  
            'total\_estimated\_links': session\_metrics.get('estimated\_total\_links', 0\)  
        }  
      
    async def \_should\_create\_link(self, move1: Dict\[str, Any\], move2: Dict\[str, Any\]) \-\> bool:  
        """Determine if a link should be created between moves"""  
          
        semantic\_sim \= await self.\_calculate\_semantic\_similarity(move1\['content'\], move2\['content'\])  
        temporal\_distance \= abs(int(move2\['sequence\_number'\]) \- int(move1\['sequence\_number'\]))  
          
        return semantic\_sim \>= 0.3 or temporal\_distance \<= 3  
      
    async def \_calculate\_semantic\_similarity(self, text1: str, text2: str) \-\> float:  
        """Calculate semantic similarity between texts"""  
          
        embeddings \= self.move\_parser.sentence\_model.encode(\[text1, text2\])  
        similarity \= np.dot(embeddings\[0\], embeddings\[1\]) / (  
            np.linalg.norm(embeddings\[0\]) \* np.linalg.norm(embeddings\[1\])  
        )  
        return float(similarity)  
      
    async def \_calculate\_interaction\_metrics(self,   
                                           user\_input: str,   
                                           ai\_response: Dict\[str, Any\],   
                                           moves: List\[Dict\[str, Any\]\]) \-\> Dict\[str, Any\]:  
        """Calculate metrics for this specific interaction"""  
          
        ai\_content \= ai\_response\['content'\]  
          
        \# Basic response metrics  
        response\_directness \= await self.\_calculate\_response\_directness(ai\_content)  
        information\_density \= await self.\_calculate\_information\_density(ai\_content)  
        scaffolding\_presence \= await self.\_detect\_scaffolding\_presence(ai\_content)  
          
        \# Move-level metrics  
        user\_moves \= \[m for m in moves if m\['move\_source'\] \== 'user\_generated'\]  
        ai\_moves \= \[m for m in moves if m\['move\_source'\].startswith('ai\_')\]  
          
        \# Cognitive engagement indicators  
        user\_move\_complexity \= np.mean(\[  
            m.get('linguistic\_complexity', 0\) for m in user\_moves  
        \]) if user\_moves else 0  
          
        ai\_influence\_strength \= np.mean(\[  
            m.get('ai\_influence\_strength', 0\) for m in moves  
        \]) if moves else 0  
          
        return {  
            'response\_directness': response\_directness,  
            'information\_density': information\_density,  
            'scaffolding\_presence': scaffolding\_presence,  
            'user\_moves\_count': len(user\_moves),  
            'ai\_moves\_count': len(ai\_moves),  
            'user\_move\_complexity': user\_move\_complexity,  
            'ai\_influence\_strength': ai\_influence\_strength,  
            'token\_usage': ai\_response.get('usage', {}),  
            'cognitive\_offloading\_risk': response\_directness \* (len(ai\_moves) / max(len(moves), 1))  
        }  
      
    async def \_calculate\_response\_directness(self, ai\_response: str) \-\> float:  
        """Calculate how directly AI responded"""  
          
        direct\_indicators \= \[  
            'should', 'must', 'need to', 'have to', 'recommend', 'suggest',  
            'best approach', 'optimal', 'solution is', 'answer is'  
        \]  
          
        response\_lower \= ai\_response.lower()  
        direct\_count \= sum(1 for indicator in direct\_indicators if indicator in response\_lower)  
          
        \# Normalize by response length  
        words \= len(ai\_response.split())  
        directness\_score \= min(1.0, direct\_count / max(1, words / 50))  
          
        return directness\_score  
      
    async def \_calculate\_information\_density(self, ai\_response: str) \-\> float:  
        """Calculate information density of response"""  
          
        info\_indicators \= \[  
            'research shows', 'studies indicate', 'according to', 'typically',  
            'generally', 'commonly', 'standards', 'guidelines', 'example',  
            'precedent', 'case study', 'data', 'percentage', 'metric'  
        \]  
          
        response\_lower \= ai\_response.lower()  
        info\_count \= sum(1 for indicator in info\_indicators if indicator in response\_lower)  
          
        \# Factor in numerical references  
        import re  
        numeric\_refs \= len(re.findall(r'\\d+', ai\_response))  
          
        total\_words \= len(ai\_response.split())  
        density \= (info\_count \+ numeric\_refs \* 0.5) / max(total\_words / 100, 1\)  
          
        return min(1.0, density)  
      
    async def \_detect\_scaffolding\_presence(self, ai\_response: str) \-\> bool:  
        """Detect if AI response contains scaffolding elements"""  
          
        scaffolding\_indicators \= \[  
            '?', 'consider', 'think about', 'what if', 'have you',  
            'might you', 'could you', 'what do you think',  
            'how might', 'why do you think'  
        \]  
          
        response\_lower \= ai\_response.lower()  
        return any(indicator in response\_lower for indicator in scaffolding\_indicators)  
      
    async def \_calculate\_incremental\_session\_metrics(self,   
                                                   session\_id: str,   
                                                   new\_moves: List\[Dict\[str, Any\]\],   
                                                   new\_links: List\[Dict\[str, Any\]\]) \-\> Dict\[str, Any\]:  
        """Calculate updated session-level metrics"""  
          
        all\_moves \= self.session\_moves.get(session\_id, \[\])  
          
        \# Move source distribution  
        move\_sources \= \[m\['move\_source'\] for m in all\_moves\]  
        user\_generated \= sum(1 for s in move\_sources if s \== 'user\_generated')  
        ai\_provided \= sum(1 for s in move\_sources if s \== 'ai\_provided')  
        ai\_prompted \= sum(1 for s in move\_sources if s \== 'ai\_prompted')  
          
        \# AI dependency calculation  
        ai\_dependency\_ratio \= (ai\_provided \+ ai\_prompted) / len(all\_moves) if all\_moves else 0  
          
        \# Cognitive load distribution  
        cognitive\_loads \= \[m.get('cognitive\_load', 'low') for m in all\_moves\]  
        high\_load\_ratio \= sum(1 for load in cognitive\_loads if load \== 'high') / len(cognitive\_loads) if cognitive\_loads else 0  
          
        \# Estimated total links (for performance)  
        estimated\_total\_links \= len(all\_moves) \* 2  \# Rough estimate  
          
        return {  
            'total\_moves': len(all\_moves),  
            'user\_generated\_moves': user\_generated,  
            'ai\_provided\_moves': ai\_provided,  
            'ai\_prompted\_moves': ai\_prompted,  
            'ai\_dependency\_ratio': ai\_dependency\_ratio,  
            'high\_cognitive\_load\_ratio': high\_load\_ratio,  
            'estimated\_total\_links': estimated\_total\_links,  
            'session\_complexity\_trend': await self.\_calculate\_complexity\_trend(all\_moves)  
        }  
      
    async def \_calculate\_complexity\_trend(self, moves: List\[Dict\[str, Any\]\]) \-\> str:  
        """Calculate trend in move complexity over time"""  
          
        if len(moves) \< 4:  
            return 'insufficient\_data'  
          
        \# Split into early and recent moves  
        mid\_point \= len(moves) // 2  
        early\_moves \= moves\[:mid\_point\]  
        recent\_moves \= moves\[mid\_point:\]  
          
        early\_complexity \= np.mean(\[  
            m.get('linguistic\_complexity', 0\) for m in early\_moves  
        \])  
        recent\_complexity \= np.mean(\[  
            m.get('linguistic\_complexity', 0\) for m in recent\_moves  
        \])  
          
        if recent\_complexity \> early\_complexity \+ 0.1:  
            return 'increasing'  
        elif recent\_complexity \< early\_complexity \- 0.1:  
            return 'decreasing'  
        else:  
            return 'stable'  
      
    async def get\_complete\_session\_analysis(self, session\_id: str) \-\> Dict\[str, Any\]:  
        """Get complete linkographic analysis for session"""  
          
        all\_moves \= self.session\_moves.get(session\_id, \[\])  
          
        if not all\_moves:  
            return {'error': 'No moves found for session'}  
          
        \# Perform complete linkographic analysis  
        complete\_analysis \= await self.linkographic\_analyzer.analyze\_session\_linkography(  
            moves=all\_moves,  
            session\_context={'session\_id': session\_id}  
        )  
          
        return complete\_analysis  
      
    def \_get\_enhanced\_claude\_prompt(self) \-\> str:  
        """Enhanced system prompt for Claude with linkographic considerations"""  
          
        return """You are an AI assistant helping with architectural design tasks. Your role is to provide helpful information and guidance while allowing the user to develop their own design thinking.

Key principles:  
\- Provide clear, informative responses to direct questions  
\- Offer relevant examples and precedents when appropriate  
\- Break down complex problems into manageable components  
\- Encourage consideration of multiple design factors  
\- Be specific and actionable in your suggestions

Focus areas to address:  
\- Functional requirements and spatial relationships  
\- Structural and technical considerations    
\- Environmental and sustainability factors  
\- Cultural and community context  
\- Design process and methodology

Respond naturally and conversationally while maintaining professional expertise."""  
      
    def \_get\_enhanced\_chatgpt\_prompt(self) \-\> str:  
        """Enhanced system prompt for ChatGPT with linkographic considerations"""  
          
        return """You are an experienced architectural consultant providing design assistance. Help users develop comprehensive design solutions through clear guidance and expert knowledge.

Approach:  
\- Answer questions directly with specific, actionable advice  
\- Provide relevant examples and case studies  
\- Address technical, functional, and contextual design aspects  
\- Offer structured approaches to complex design challenges  
\- Share professional insights and best practices

Key areas to cover:  
\- Space planning and functional organization  
\- Building systems and technical integration  
\- Sustainable design strategies  
\- Community and user needs  
\- Construction and material considerations

Maintain a professional, helpful tone while providing substantive design guidance."""  
      
    def \_get\_phase\_guidance(self, phase: str) \-\> str:  
        """Get phase-specific guidance for AI responses"""  
          
        phase\_prompts \= {  
            'ideation': "Focus on conceptual development, exploring different approaches, and identifying key design drivers and constraints.",  
            'visualization': "Emphasize spatial relationships, design representation, and translation of concepts into visual/spatial solutions.",  
            'materialization': "Address technical implementation, construction considerations, and detailed design resolution."  
        }  
          
        return phase\_prompts.get(phase, "")

### **5\. Enhanced API Routes with Linkographic Endpoints**

**File: `src/api/enhanced_generic_ai_routes.py`**

"""  
Enhanced API Routes with Linkographic Analysis  
Provides comprehensive endpoints for Generic AI testing with linkography  
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks  
from typing import Dict, Any, List  
import logging  
from datetime import datetime

from ..ai\_integrations.enhanced\_generic\_ai\_router import EnhancedGenericAIRouter  
from ..data\_processing.enhanced\_session\_manager import EnhancedSessionManager  
from ..benchmarking.enhanced\_cognitive\_metrics import EnhancedCognitiveMetrics  
from ..utils.config import settings

logger \= logging.getLogger(\_\_name\_\_)  
router \= APIRouter(prefix="/enhanced-generic-ai", tags=\["enhanced-generic-ai"\])

\# Initialize enhanced components  
enhanced\_ai\_router \= EnhancedGenericAIRouter(settings.ai\_models)  
enhanced\_session\_manager \= EnhancedSessionManager()  
enhanced\_cognitive\_metrics \= EnhancedCognitiveMetrics()

@router.post("/sessions/{session\_id}/interact-with-linkography")  
async def process\_enhanced\_interaction(  
    session\_id: str,   
    interaction\_data: Dict\[str, Any\],  
    background\_tasks: BackgroundTasks  
):  
    """Process interaction with full linkographic analysis"""  
      
    try:  
        \# Validate input  
        required\_fields \= \['user\_input', 'participant\_id', 'phase'\]  
        for field in required\_fields:  
            if field not in interaction\_data:  
                raise ValueError(f"Missing required field: {field}")  
          
        \# Get session context  
        session\_context \= await enhanced\_session\_manager.get\_enhanced\_session\_context(session\_id)  
          
        \# Process through enhanced AI router  
        ai\_result \= await enhanced\_ai\_router.process\_interaction\_with\_linkography(  
            session\_id=session\_id,  
            user\_input=interaction\_data\['user\_input'\],  
            session\_context=session\_context,  
            phase=interaction\_data\['phase'\]  
        )  
          
        \# Calculate enhanced cognitive metrics  
        cognitive\_metrics \= await enhanced\_cognitive\_metrics.calculate\_enhanced\_metrics(  
            session\_id=session\_id,  
            interaction\_data=interaction\_data,  
            ai\_result=ai\_result,  
            linkographic\_data=ai\_result.get('linkographic\_update', {})  
        )  
          
        \# Store interaction and linkographic data  
        interaction\_record \= await enhanced\_session\_manager.store\_enhanced\_interaction(  
            session\_id=session\_id,  
            interaction\_data=interaction\_data,  
            ai\_result=ai\_result,  
            cognitive\_metrics=cognitive\_metrics  
        )  
          
        \# Schedule background analysis updates  
        background\_tasks.add\_task(  
            update\_session\_linkographic\_analysis,  
            session\_id,  
            ai\_result.get('interaction\_moves', \[\])  
        )  
          
        return {  
            'interaction\_id': interaction\_record\['interaction\_id'\],  
            'ai\_response': ai\_result\['ai\_response'\],  
            'model\_used': ai\_result\['model\_used'\],  
            'processing\_time': ai\_result\['processing\_time'\],  
            'interaction\_moves': ai\_result\['interaction\_moves'\],  
            'linkographic\_update': ai\_result\['linkographic\_update'\],  
            'cognitive\_metrics': cognitive\_metrics,  
            'session\_progress': {  
                'total\_moves': ai\_result\['metadata'\]\['total\_session\_moves'\],  
                'current\_phase': interaction\_data\['phase'\],  
                'linkographic\_status': ai\_result\['linkographic\_update'\].get('status', 'unknown')  
            },  
            'recommendations': await generate\_real\_time\_recommendations(  
                ai\_result, cognitive\_metrics  
            ),  
            'alerts': await check\_for\_cognitive\_alerts(  
                cognitive\_metrics, ai\_result  
            )  
        }  
          
    except Exception as e:  
        logger.error(f"Error in enhanced interaction processing: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/sessions/{session\_id}/linkographic-analysis")  
async def get\_complete\_linkographic\_analysis(session\_id: str):  
    """Get complete linkographic analysis for session"""  
      
    try:  
        \# Get complete analysis from AI router  
        complete\_analysis \= await enhanced\_ai\_router.get\_complete\_session\_analysis(session\_id)  
          
        if 'error' in complete\_analysis:  
            raise HTTPException(status\_code=404, detail=complete\_analysis\['error'\])  
          
        \# Enhance with comparative metrics  
        comparative\_metrics \= await enhanced\_cognitive\_metrics.calculate\_comparative\_linkographic\_metrics(  
            session\_id, complete\_analysis  
        )  
          
        return {  
            'session\_id': session\_id,  
            'linkographic\_analysis': complete\_analysis,  
            'comparative\_metrics': comparative\_metrics,  
            'analysis\_timestamp': datetime.now().isoformat(),  
            'summary': complete\_analysis.get('session\_summary', {}),  
            'visualizations': await generate\_linkographic\_visualizations(complete\_analysis)  
        }  
          
    except Exception as e:  
        logger.error(f"Error retrieving linkographic analysis: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/sessions/{session\_id}/cognitive-development-trajectory")  
async def get\_cognitive\_development\_trajectory(session\_id: str):  
    """Get cognitive development trajectory with linkographic insights"""  
      
    try:  
        \# Get session moves and analysis  
        complete\_analysis \= await enhanced\_ai\_router.get\_complete\_session\_analysis(session\_id)  
          
        if 'error' in complete\_analysis:  
            raise HTTPException(status\_code=404, detail=complete\_analysis\['error'\])  
          
        \# Calculate developmental trajectory  
        trajectory \= await enhanced\_cognitive\_metrics.calculate\_cognitive\_trajectory(  
            complete\_analysis\['moves'\],  
            complete\_analysis\['links'\],  
            complete\_analysis\['network\_metrics'\]  
        )  
          
        return {  
            'session\_id': session\_id,  
            'cognitive\_trajectory': trajectory,  
            'key\_milestones': trajectory.get('milestones', \[\]),  
            'development\_indicators': complete\_analysis.get('cognitive\_development\_indicators', {}),  
            'ai\_influence\_analysis': complete\_analysis.get('ai\_influence\_patterns', {}),  
            'recommendations': await generate\_development\_recommendations(trajectory)  
        }  
          
    except Exception as e:  
        logger.error(f"Error calculating cognitive trajectory: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.post("/sessions/{session\_id}/comparative-analysis")  
async def run\_comparative\_analysis(  
    session\_id: str,  
    comparison\_data: Dict\[str, Any\]  
):  
    """Run comparative analysis against MENTOR or baseline"""  
      
    try:  
        comparison\_type \= comparison\_data.get('comparison\_type', 'mentor')  
          
        \# Get session linkographic analysis  
        session\_analysis \= await enhanced\_ai\_router.get\_complete\_session\_analysis(session\_id)  
          
        if 'error' in session\_analysis:  
            raise HTTPException(status\_code=404, detail=session\_analysis\['error'\])  
          
        \# Perform comparative analysis  
        if comparison\_type \== 'mentor':  
            comparative\_results \= await enhanced\_cognitive\_metrics.compare\_with\_mentor\_baseline(  
                session\_analysis, comparison\_data.get('mentor\_baseline\_data')  
            )  
        elif comparison\_type \== 'baseline':  
            comparative\_results \= await enhanced\_cognitive\_metrics.compare\_with\_no\_ai\_baseline(  
                session\_analysis, comparison\_data.get('baseline\_data')  
            )  
        else:  
            raise ValueError(f"Unsupported comparison type: {comparison\_type}")  
          
        return {  
            'session\_id': session\_id,  
            'comparison\_type': comparison\_type,  
            'comparative\_results': comparative\_results,  
            'key\_differences': comparative\_results.get('key\_differences', \[\]),  
            'statistical\_significance': comparative\_results.get('statistical\_tests', {}),  
            'interpretation': comparative\_results.get('interpretation', ''),  
            'recommendations': comparative\_results.get('recommendations', \[\])  
        }  
          
    except Exception as e:  
        logger.error(f"Error in comparative analysis: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

@router.get("/sessions/{session\_id}/export-linkographic-data")  
async def export\_linkographic\_data(  
    session\_id: str,  
    format: str \= "json",  
    include\_visualizations: bool \= False  
):  
    """Export complete linkographic data for external analysis"""  
      
    try:  
        \# Get complete analysis  
        complete\_analysis \= await enhanced\_ai\_router.get\_complete\_session\_analysis(session\_id)  
          
        if 'error' in complete\_analysis:  
            raise HTTPException(status\_code=404, detail=complete\_analysis\['error'\])  
          
        \# Prepare export data  
        export\_data \= {  
            'session\_metadata': {  
                'session\_id': session\_id,  
                'export\_timestamp': datetime.now().isoformat(),  
                'analysis\_version': '2.0',  
                'format': format  
            },  
            'moves': complete\_analysis\['moves'\],  
            'links': complete\_analysis\['links'\],  
            'network\_metrics': complete\_analysis\['network\_metrics'\],  
            'ai\_influence\_patterns': complete\_analysis\['ai\_influence\_patterns'\],  
            'critical\_moves': complete\_analysis\['critical\_moves'\],  
            'cognitive\_development\_indicators': complete\_analysis\['cognitive\_development\_indicators'\],  
            'session\_summary': complete\_analysis\['session\_summary'\]  
        }  
          
        \# Add visualizations if requested  
        if include\_visualizations:  
            export\_data\['visualizations'\] \= await generate\_linkographic\_visualizations(  
                complete\_analysis  
            )  
          
        \# Format according to request  
        if format.lower() \== 'csv':  
            return await convert\_to\_csv\_format(export\_data)  
        elif format.lower() \== 'graphml':  
            return await convert\_to\_graphml\_format(export\_data)  
        else:  \# Default to JSON  
            return export\_data  
          
    except Exception as e:  
        logger.error(f"Error exporting linkographic data: {e}")  
        raise HTTPException(status\_code=500, detail=str(e))

\# Background task functions  
async def update\_session\_linkographic\_analysis(session\_id: str, new\_moves: List\[Dict\[str, Any\]\]):  
    """Background task to update session linkographic analysis"""  
      
    try:  
        \# Update database with new moves  
        await enhanced\_session\_manager.update\_session\_moves(session\_id, new\_moves)  
          
        \# Trigger incremental analysis update  
        await enhanced\_session\_manager.update\_session\_linkographic\_metrics(session\_id)  
          
        logger.info(f"Updated linkographic analysis for session {session\_id}")  
          
    except Exception as e:  
        logger.error(f"Error updating session linkographic analysis: {e}")

\# Helper functions  
async def generate\_real\_time\_recommendations(  
    ai\_result: Dict\[str, Any\],   
    cognitive\_metrics: Dict\[str, Any\]  
) \-\> List\[Dict\[str, Any\]\]:  
    """Generate real-time recommendations based on analysis"""  
      
    recommendations \= \[\]  
      
    \# Check for high AI dependency  
    if ai\_result.get('interaction\_metrics', {}).get('cognitive\_offloading\_risk', 0\) \> 0.7:  
        recommendations.append({  
            'type': 'cognitive\_offloading\_warning',  
            'priority': 'high',  
            'message': 'Consider asking follow-up questions to deepen your understanding',  
            'action': 'encourage\_elaboration'  
        })  
      
    \# Check for low engagement  
    user\_moves \= len(\[m for m in ai\_result.get('interaction\_moves', \[\])   
                     if m.get('move\_source') \== 'user\_generated'\])  
    if user\_moves \< 2:  
        recommendations.append({  
            'type': 'engagement\_encouragement',  
            'priority': 'medium',  
            'message': 'Try explaining your reasoning or exploring alternatives',  
            'action': 'encourage\_reflection'  
        })  
      
    return recommendations

async def check\_for\_cognitive\_alerts(  
    cognitive\_metrics: Dict\[str, Any\],   
    ai\_result: Dict\[str, Any\]  
) \-\> List\[Dict\[str, Any\]\]:  
    """Check for cognitive development alerts"""  
      
    alerts \= \[\]  
      
    \# Low complexity alert  
    if cognitive\_metrics.get('average\_user\_move\_complexity', 0\) \< 0.3:  
        alerts.append({  
            'type': 'low\_complexity',  
            'severity': 'warning',  
            'message': 'Consider exploring more complex design relationships',  
            'suggestion': 'Ask about integration between different design systems'  
        })  
      
    return alerts

async def generate\_linkographic\_visualizations(analysis: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
    """Generate visualization data for linkographic networks"""  
      
    \# This would generate data for frontend visualization  
    \# Implementation would depend on your visualization library  
      
    return {  
        'network\_graph': {  
            'nodes': \[{'id': m\['move\_id'\], 'label': m\['content'\]\[:50\] \+ '...',   
                      'type': m\['move\_type'\], 'source': m\['move\_source'\]}   
                     for m in analysis\['moves'\]\],  
            'edges': \[{'source': l\['source\_move\_id'\], 'target': l\['target\_move\_id'\],  
                      'weight': l\['link\_strength'\], 'type': l\['link\_type'\]}  
                     for l in analysis\['links'\]\]  
        },  
        'temporal\_flow': await generate\_temporal\_flow\_data(analysis),  
        'ai\_influence\_heatmap': await generate\_ai\_influence\_heatmap(analysis)  
    }

async def generate\_temporal\_flow\_data(analysis: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
    """Generate temporal flow visualization data"""  
      
    moves \= analysis\['moves'\]  
      
    \# Group moves by time windows  
    time\_windows \= \[\]  
    window\_size \= 5  \# 5 moves per window  
      
    for i in range(0, len(moves), window\_size):  
        window\_moves \= moves\[i:i+window\_size\]  
        time\_windows.append({  
            'window\_id': i // window\_size,  
            'moves': len(window\_moves),  
            'user\_generated': len(\[m for m in window\_moves if m\['move\_source'\] \== 'user\_generated'\]),  
            'ai\_influenced': len(\[m for m in window\_moves if m\['move\_source'\].startswith('ai\_')\]),  
            'complexity': sum(m.get('linguistic\_complexity', 0\) for m in window\_moves) / len(window\_moves)  
        })  
      
    return {'time\_windows': time\_windows}

async def generate\_ai\_influence\_heatmap(analysis: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
    """Generate AI influence heatmap data"""  
      
    moves \= analysis\['moves'\]  
    links \= analysis\['links'\]  
      
    \# Create influence matrix  
    influence\_data \= \[\]  
      
    for move in moves:  
        if move\['move\_source'\].startswith('ai\_'):  
            \# Find moves influenced by this AI move  
            influenced\_moves \= \[  
                l\['target\_move\_id'\] for l in links   
                if l\['source\_move\_id'\] \== move\['move\_id'\] and l\['ai\_mediated'\]  
            \]  
              
            influence\_data.append({  
                'ai\_move\_id': move\['move\_id'\],  
                'ai\_move\_type': move\['move\_type'\],  
                'influenced\_count': len(influenced\_moves),  
                'influence\_strength': move.get('ai\_influence\_strength', 0\)  
            })  
      
    return {'influence\_data': influence\_data}

async def generate\_development\_recommendations(trajectory: Dict\[str, Any\]) \-\> List\[str\]:  
    """Generate recommendations based on cognitive development trajectory"""  
      
    recommendations \= \[\]  
      
    \# Analyze trajectory trends  
    if trajectory.get('complexity\_trend') \== 'decreasing':  
        recommendations.append(  
            "Consider exploring more complex design relationships and interdependencies"  
        )  
      
    if trajectory.get('ai\_dependency\_trend') \== 'increasing':  
        recommendations.append(  
            "Try developing initial ideas independently before seeking AI assistance"  
        )  
      
    if trajectory.get('synthesis\_frequency') \< 0.2:  
        recommendations.append(  
            "Focus on connecting and integrating different design concepts"  
        )  
      
    return recommendations

async def convert\_to\_csv\_format(export\_data: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
    """Convert linkographic data to CSV format"""  
      
    \# Implementation would convert the data to CSV-friendly format  
    \# This is a placeholder for the actual conversion logic  
      
    return {  
        'format': 'csv',  
        'files': {  
            'moves.csv': 'moves\_csv\_content',  
            'links.csv': 'links\_csv\_content',  
            'metrics.csv': 'metrics\_csv\_content'  
        }  
    }

async def convert\_to\_graphml\_format(export\_data: Dict\[str, Any\]) \-\> Dict\[str, Any\]:  
    """Convert linkographic data to GraphML format for network analysis tools"""  
      
    \# Implementation would convert to GraphML format  
    \# This is a placeholder for the actual conversion logic  
      
    return {  
        'format': 'graphml',  
        'content': 'graphml\_content\_here'  
    }

