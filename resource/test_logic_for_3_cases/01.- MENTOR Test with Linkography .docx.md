# **MENTOR Tool Comprehensive B-Tests Suite**

## **Executive Summary**

This comprehensive testing suite is designed to evaluate the Multimodal AI Mentor across three phases (Ideation, Visualization, Materialization) with integrated cognitive benchmarking. The tests are engineered to maximize the potential of the cognitive benchmarking tool by incorporating the six key metrics: Cognitive Offloading Prevention (COP), Deep Thinking Engagement (DTE), Scaffolding Effectiveness (SE), Knowledge Integration (KI), Learning Progression (LP), and Metacognitive Awareness (MA).

---

## **Test Structure Overview**

### **Core Testing Framework**

* **Phase 1**: Ideation Phase Tests  
* **Phase 2**: Visualization Phase Tests  
* **Phase 3**: Materialization Phase Tests  
* **Cross-Phase**: Integration & Progression Tests  
* **Benchmarking**: Cognitive Metrics Validation

### **Participant Groups**

* **Group A**: MENTOR Tool Users (Experimental)  
* **Group B**: Generic AI Tool Users (ChatGPT/Claude \- Control)  
* **Group C**: No AI Assistance (Traditional Control)

---

## **Phase 1: Ideation Phase Tests**

### **Test 1.1: Architectural Concept Development**

**Duration**: 15 minutes  
 **Scenario**: Urban Community Center Design

#### **Pre-Test Assessment**

* Critical Thinking Assessment (Halpern CTDA \- abbreviated 8 questions)  
* Architectural Knowledge Baseline (12 questions)  
* Spatial Reasoning Test (5 questions)

#### **Main Task**

**Prompt**: "You are tasked with designing a community center for a diverse urban neighborhood of 15,000 residents. The site is a former industrial warehouse (150m x 80m x 12m height). Consider: community needs, cultural sensitivity, sustainability, and adaptive reuse principles."

#### **Phase-Specific Interactions**

**Group A (MENTOR) \- Expected Socratic Dialogue Pattern**:

1. **Initial Context Reasoning**: "Before we begin designing, what do you think are the most important questions we should ask about this community?"  
2. **Knowledge Synthesis Trigger**: "What are some successful examples of warehouse-to-community transformations you're aware of?"  
3. **Socratic Questioning**: "Why might the existing industrial character be valuable to preserve? What would be lost if we completely transformed it?"  
4. **Metacognitive Prompt**: "How are you approaching this problem differently than a typical new-build community center?"

**Group B (Generic AI)**: Standard prompt with ability to ask direct questions **Group C (No AI)**: Research materials and traditional resources only

#### **Measured Outputs**

* **Design Concept Quality** (Expert evaluation 1-10 scale)  
* **Process Documentation** (thinking progression)  
* **Cognitive Engagement Metrics**:  
  * Question depth and frequency  
  * Iterative refinement cycles  
  * Assumption questioning behavior  
  * Cultural consideration integration

#### **Benchmarking Integration**

* **COP Score**: Tracks ratio of exploratory vs. direct answer-seeking queries  
* **DTE Score**: Measures reflection pauses, reasoning chains, complexity of responses  
* **SE Score**: Evaluates appropriateness of guidance to user proficiency level  
* **KI Score**: Assesses connection of architectural principles to design decisions

---

### **Test 1.2: Spatial Program Development**

**Duration**: 10 minutes  
 **Scenario**: Functional Space Allocation

#### **Task Description**

"Based on your community center concept, develop a detailed spatial program. Consider: circulation patterns, adjacency requirements, flexibility needs, and community input integration."

#### **Multi-Agent Interaction Triggers**

* **Context Reasoning Agent**: "How do the functional relationships between spaces reflect community social patterns?"  
* **Knowledge Synthesis Agent**: "What precedents can inform your adjacency decisions?"  
* **Socratic Dialogue Agent**: "What assumptions are you making about how this community gathers and interacts?"  
* **Metacognitive Agent**: "How is your programming methodology evolving as you think through this problem?"

#### **Assessment Criteria**

*  (1-10)**Spatial Logic Quality**  
* **Community Needs Integration** (1-10)  
* **Flexibility & Adaptability** (1-10)  
* **Justification Depth** (1-10)

---

## **Phase 2: Visualization Phase Tests**

### **Test 2.1: 2D Design Development & Analysis**

**Duration**: 20 minutes  
 **Scenario**: Schematic Design with Computer Vision Integration

#### **Task Setup**

Participants upload hand sketches or CAD drawings of their community center concept for AI analysis and critique.

#### **Technical Integration**

* **Computer Vision Processing**: Automated analysis of spatial proportions, circulation patterns, and design elements  
* **Region Segmentation**: Identification of functional zones  
* **Vision-Language Analysis**: Semantic understanding of design intent

#### **MENTOR Tool Response Pattern**

1. **Spatial Analysis**: "I notice your main gathering space represents 40% of the total area. How does this proportion relate to your intended community capacity?"  
2. **Circulation Critique**: "Your circulation pattern creates a linear progression through spaces. What are the implications for spontaneous community interaction?"  
3. **Proportion Questioning**: "The scale relationship between your entrance and main hall suggests a particular hierarchy. Was this intentional?"  
4. **Design Principle Integration**: "How do your proportional decisions reflect principles of inclusive community design?"

#### **Measured Outputs**

* **Design Quality Improvement** (Pre/Post upload comparison)  
* **Spatial Reasoning Development** (Measured through dialogue depth)  
* **Visual Analysis Comprehension** (Response to AI feedback quality)  
* **Design Iteration Cycles** (Number and sophistication of revisions)

#### **Benchmarking Metrics**

* **MA Score**: Self-reflection on design decisions after AI feedback  
* **LP Score**: Skill progression from initial concept to refined design  
* **KI Score**: Integration of feedback into design evolution

---

### **Test 2.2: Environmental & Contextual Integration**

**Duration**: 10 minutes  
 **Scenario**: Site Responsiveness and Environmental Design

#### **Task Description**

"Integrate your community center design with environmental factors: natural lighting, ventilation, solar orientation, and urban context. Consider how the building responds to its surroundings."

#### **MENTOR Dialogue Framework with Linkography Integration**

* **Context Reasoning Agent**: "How might the industrial windows influence your natural lighting strategy throughout the day?"

  * *Captured Moves*:  
    * Move 1: "Consider existing industrial windows" (analysis, visualization, text)  
    * Move 2: "Evaluate natural lighting potential" (evaluation, visualization, text)  
    * Move 3: "Plan lighting strategy throughout day" (synthesis, visualization, text)  
* **Cultural Context Agent**: "What elements of the surrounding neighborhood architecture should your design respond to or contrast with?"

  * *Captured Moves*:  
    * Move 4: "Analyze neighborhood architectural context" (analysis, visualization, text)  
    * Move 5: "Identify design response strategies" (synthesis, visualization, text)  
    * Move 6: "Balance response vs. contrast approaches" (evaluation, visualization, text)  
* **Sustainability Integration**: "How do your environmental strategies support community activities while honoring the building's industrial heritage?"

  * *Captured Moves*:  
    * Move 7: "Connect environmental strategies to program" (synthesis, visualization, text)  
    * Move 8: "Preserve industrial heritage character" (transformation, visualization, text)  
    * Move 9: "Integrate sustainability with community use" (synthesis, visualization, text)

#### **Linkography-Enhanced Assessment Dimensions**

* **Environmental Responsiveness** (1-10) \+ **Link Density Score** (moves per concept)  
* **Cultural Sensitivity** (1-10) \+ **Conceptual Bridge Count** (cross-cultural connections)  
* **Technical Integration** (1-10) \+ **Solution Synthesis Frequency** (combining moves)  
* **Holistic Thinking** (1-10) \+ **Inter-phase Move Links** (connections across design phases)

---

## **Phase 3: Materialization Phase Tests**

### **Test 3.1: 3D Spatial Analysis & Material Systems**

**Duration**: 20 minutes  
 **Scenario**: Detailed Design Development with 3D Analysis

#### **Task Components**

1. **3D Model Development**: Create detailed spatial model of community center  
2. **Material Selection**: Choose appropriate materials for adaptive reuse  
3. **Structural Integration**: Consider existing structural systems  
4. **Construction Methodology**: Plan for community involvement in construction

#### **3D Analysis Integration with Linkography**

* **Scene Graph Parsing**: Automated analysis of 3D geometry and spatial relationships  
  * *Each identified element becomes a design move*: "Identify double-height main space" (analysis, materialization, 3d\_model)  
* **Spatial Analysis**: Volumetric studies, circulation flow analysis  
  * *Flow analysis generates moves*: "Optimize circulation through central spine" (transformation, materialization, 3d\_model)  
* **Semantic Labeling**: Identification of functional zones and their relationships  
  * *Zone relationships create linked moves*: "Connect library to quiet courtyard" (synthesis, materialization, 3d\_model)  
* **Material Properties Integration**: Analysis of proposed material choices  
  * *Material decisions spawn move sequences*: "Select exposed steel for authenticity" → "Consider acoustic treatment" → "Integrate warmth through wood accents" (3 linked moves)

#### **MENTOR 3D Interface Interactions with Move Parsing**

* **Spatial Reasoning Challenges**: "Your double-height spaces create opportunities for visual connection. How do you envision this affecting community interaction patterns?"

  * *Parsed into moves*:  
    * Move N: "Recognize double-height spatial opportunities" (analysis, materialization, text)  
    * Move N+1: "Envision visual connections between levels" (synthesis, materialization, text)  
    * Move N+2: "Predict community interaction behaviors" (evaluation, materialization, text)  
* **Material Logic Questioning**: "You've chosen to expose the existing steel structure. How does this decision support both structural efficiency and community identity?"

  * *Parsed into moves*:  
    * Move N+3: "Justify exposed steel structural choice" (evaluation, materialization, text)  
    * Move N+4: "Connect material to structural efficiency" (analysis, materialization, text)  
    * Move N+5: "Link material expression to community identity" (synthesis, materialization, text)

#### **Complex Cognitive Challenges with Move Tracking**

* **Structural Engineering Integration**: "How do your design modifications work with the existing structural grid?"  
  * *Generates linked move sequence analyzing structural constraints and design adaptations*  
* **Building Systems Coordination**: "Where will your new HVAC systems integrate with the preserved industrial elements?"  
  * *Creates moves linking technical systems with heritage preservation*  
* **Accessibility & Universal Design**: "How does your vertical circulation strategy ensure inclusive access for all community members?"  
  * *Produces moves connecting accessibility requirements with spatial design decisions*

#### **Measured Outputs**

* **3D Spatial Sophistication** (Expert evaluation)  
* **Material System Logic** (Technical assessment)  
* **Constructability & Feasibility** (Professional review)  
* **Community Engagement Integration** (Social sustainability assessment)

#### **Advanced Benchmarking**

* **SE Score**: Adaptive scaffolding effectiveness across increasing complexity  
* **KI Score**: Integration of technical, social, and cultural knowledge domains  
* **DTE Score**: Deep thinking engagement with complex multi-variable problems

---

### **Test 3.2: Realization & Implementation Strategy**

**Duration**: 15 minutes  
 **Scenario**: Project Implementation and Community Engagement

#### **Task Description**

"Develop a comprehensive implementation strategy for your community center, including: phased construction, community engagement process, funding strategies, and long-term stewardship plans."

#### **Real-World Integration Challenges**

* **Stakeholder Analysis**: Who are the key community stakeholders?  
* **Phasing Strategy**: How can the building serve the community during construction?  
* **Resource Allocation**: How do you balance community desires with budget constraints?  
* **Long-term Adaptability**: How will the design evolve with changing community needs?

#### **MENTOR Strategic Guidance**

* **Implementation Reality**: "Your design proposals are ambitious. How would you prioritize elements if budget was reduced by 30%?"  
* **Community Process**: "What methods will you use to ensure diverse community voices are heard in the design refinement process?"  
* **Temporal Considerations**: "How might this community center need to adapt over the next 20 years?"

---

## **Cross-Phase Integration Tests**

### **Test 4.1: Design Evolution Analysis**

**Duration**: 10 minutes  
 **Scenario**: Reflective Analysis of Design Journey

#### **Cognitive Progression Assessment with Linkography Metrics**

Participants review their complete design process from initial concept through final implementation strategy, now enhanced with linkographic analysis.

#### **Reflection Prompts with Move-Level Analysis**

1. "How did your understanding of the design problem evolve throughout the three phases?"

   * *System analyzes*: **Move evolution patterns**, **concept development chains**, **phase transition links**  
2. "What were the most significant learning moments in your design process?"

   * *System identifies*: **Critical moves with high link density**, **breakthrough moments**, **conceptual pivots**  
3. "How did the AI mentor influence your thinking patterns?"

   * *System tracks*: **AI-prompted moves vs. self-generated moves**, **scaffolding effectiveness through move analysis**  
4. "What would you approach differently in future projects?"

   * *System evaluates*: **Move sequences leading to dead ends**, **successful solution paths**, **reflection-action loops**

#### **Linkography-Enhanced Benchmarking Integration**

* **MA Score**: Metacognitive awareness measured through reflection move frequency and depth  
* **LP Score**: Learning progression tracked through move sophistication evolution across phases  
* **Overall Cognitive Development**: Integrated assessment using:  
  * **Link Index (L.I.)**: Ratio of links to moves indicating synthesis activity  
  * **Critical Move Identification**: High-impact decisions with multiple forward links  
  * **Phase Transition Analysis**: How concepts bridge between ideation, visualization, and materialization  
  * **Move Type Distribution**: Balance of analysis, synthesis, evaluation, transformation, and reflection

---

### **Test 4.2: Knowledge Transfer Challenge**

**Duration**: 15 minutes  
 **Scenario**: Application to New Design Problem

#### **Transfer Task**

"Apply the principles and methodologies you've developed to a new scenario: Adaptive reuse of a former shopping mall into a mixed-use community hub."

#### **Assessment Focus**

* **Principle Transfer**: Application of learned design methodologies  
* **Cognitive Strategy Transfer**: Use of questioning and analytical approaches  
* **Knowledge Integration**: Synthesis of architectural, social, and technical knowledge  
* **Independent Problem-Solving**: Ability to work without AI assistance

---

## **Specialized Cognitive Assessment Instruments**

### **Pre/Post Critical Thinking Assessment**

**Modified Halpern Critical Thinking Assessment for Design**

#### **Sample Questions**

1. **Verbal Reasoning**: "A community center design includes a large open space that can be divided. This flexibility is important because: \[multiple choice with reasoning required\]"

2. **Argument Analysis**: "Evaluate this design rationale: 'The entrance should be monumental because community centers need to make a strong civic statement.' Identify assumptions and evaluate the logic."

3. **Hypothesis Testing**: "If your community center design aims to promote intergenerational interaction, what specific design features would test this hypothesis and how would you measure success?"

4. **Likelihood & Uncertainty**: "Given limited community input data, how would you approach making design decisions about program allocation? Discuss your reasoning process."

5. **Problem Solving**: "A community group wants both a quiet library space and an active children's area in limited square footage. Describe your approach to resolving this apparent conflict."

### **Spatial Reasoning Assessment**

**3D Mental Rotation and Spatial Relationship Tasks**

#### **Sample Tasks**

1. **Mental Rotation**: Identify matching 3D architectural forms from different viewpoints  
2. **Spatial Relationships**: Analyze circulation patterns and spatial adjacencies  
3. **Scale Perception**: Evaluate proportional relationships in architectural spaces  
4. **Transformation Visualization**: Predict how spaces change with different configurations

### **Architectural Knowledge Assessment**

**Domain-Specific Knowledge Evaluation**

#### **Content Areas**

1. **Building Types & Precedents** (25%)  
2. **Structural Systems & Technology** (25%)  
3. **Environmental Design & Sustainability** (25%)  
4. **Social Architecture & Community Design** (25%)

#### **Sample Questions**

1. "Compare the spatial organization strategies of three notable community centers. How do their designs reflect different approaches to community interaction?"

2. "Explain how adaptive reuse strategies differ from new construction in terms of structural, environmental, and social considerations."

3. "Describe the relationship between building orientation, window placement, and natural lighting in community spaces."

---

## **Data Collection & Analysis Framework**

### **Enhanced Data Structure for Linkography Integration**

#### **Design Move Logging**

design\_move\_data \= {  
    'move\_id': unique\_move\_identifier,  
    'session\_id': session\_identifier,  
    'timestamp': precise\_timestamp\_microseconds,  
    'sequence\_number': move\_order\_in\_session,  
    'design\_move': {  
        'content': "The actual design action/decision/thought",  
        'move\_type': 'analysis|synthesis|evaluation|transformation|reflection',  
        'phase': 'ideation|visualization|materialization',  
        'modality': 'text|sketch|image|voice|upload',  
        'cognitive\_operation': 'proposal|clarification|assessment|support|reference',  
        'design\_focus': 'function|form|structure|material|environment|culture'  
    },  
    'linkography\_context': {  
        'previous\_move\_id': last\_move\_reference,  
        'tool\_used': 'mentor\_agent|vision\_analysis|3d\_processor',  
        'interaction\_trigger': 'user\_initiated|ai\_prompted|system\_generated',  
        'semantic\_links': \[list\_of\_related\_move\_ids\],  
        'temporal\_links': \[chronologically\_adjacent\_moves\],  
        'conceptual\_distance': calculated\_similarity\_score  
    },  
    'cognitive\_load\_indicators': {  
        'pause\_duration': seconds\_before\_move,  
        'revision\_count': number\_of\_corrections,  
        'complexity\_score': linguistic\_complexity\_measure,  
        'uncertainty\_markers': hesitation\_language\_detected  
    },  
    'multimodal\_context': {  
        'concurrent\_sketch': sketch\_file\_reference,  
        'image\_annotations': vision\_analysis\_results,  
        'spatial\_coordinates': 3d\_interaction\_points,  
        'gesture\_data': interface\_interaction\_patterns  
    }  
}

#### **Real-Time Linkography Analysis**

linkography\_analyzer \= {  
    'move\_parsing': 'Break AI responses and user inputs into discrete design moves',  
    'semantic\_linking': 'Calculate conceptual relationships between moves',  
    'temporal\_sequencing': 'Track precise chronological ordering',  
    'phase\_identification': 'Explicit phase tagging for each move',  
    'link\_strength\_calculation': 'Measure connection intensity between moves',  
    'critical\_move\_detection': 'Identify high-impact design decisions'  
}

#### **Cognitive Metrics Calculation**

def calculate\_realtime\_metrics(session\_data):  
    cop\_score \= calculate\_cop(session\_data)  
    dte\_score \= calculate\_dte(session\_data)  
    se\_score \= calculate\_se(session\_data, user\_profile)  
    ki\_score \= calculate\_ki(session\_data)  
    lp\_score \= calculate\_lp(session\_data)  
    ma\_score \= calculate\_ma(session\_data)  
      
    return {  
        'cop': cop\_score,  
        'dte': dte\_score,  
        'se': se\_score,  
        'ki': ki\_score,  
        'lp': lp\_score,  
        'ma': ma\_score,  
        'composite': weighted\_average(all\_scores)  
    }

### **Expert Evaluation Protocols**

#### **Design Quality Assessment Rubric**

**Creativity & Innovation** (25%)

* Originality of approach (1-4)  
* Creative problem-solving (1-4)  
* Innovative use of existing structures (1-4)

**Technical Competence** (25%)

* Structural understanding (1-4)  
* Environmental integration (1-4)  
* Material selection appropriateness (1-4)

**Community Responsiveness** (25%)

* Cultural sensitivity (1-4)  
* Accessibility & inclusion (1-4)  
* Community engagement integration (1-4)

**Design Process Quality** (25%)

* Iterative development evidence (1-4)  
* Justification depth (1-4)  
* Self-reflection quality (1-4)

### **Statistical Analysis Plan**

#### **Primary Outcomes**

* **Between-group comparisons**: ANOVA across three groups  
* **Pre/post improvements**: Paired t-tests within groups  
* **Cognitive metric correlations**: Pearson correlations between benchmarking scores and design quality

#### **Secondary Analyses**

* **Learning trajectory analysis**: Growth curve modeling over time  
* **Interaction pattern analysis**: Sequence analysis of dialogue patterns  
* **Transfer effectiveness**: Comparison of performance on novel tasks

---

## **Implementation Instructions for Claude Code**

### **Development Environment Setup**

\# Project structure  
mentor-btests/  
├── src/  
│   ├── data\_collection/  
│   │   ├── interaction\_logger.py  
│   │   ├── metrics\_calculator.py  
│   │   └── real\_time\_analyzer.py  
│   ├── assessment\_tools/  
│   │   ├── critical\_thinking\_test.py  
│   │   ├── spatial\_reasoning\_test.py  
│   │   └── architecture\_knowledge\_test.py  
│   ├── benchmarking/  
│   │   ├── cognitive\_metrics.py  
│   │   ├── benchmark\_engine.py  
│   │   └── analysis\_pipeline.py  
│   ├── ui\_components/  
│   │   ├── test\_interface.py  
│   │   ├── upload\_handler.py  
│   │   └── progress\_tracker.py  
│   └── utils/  
│       ├── data\_processing.py  
│       ├── export\_handlers.py  
│       └── validation.py  
├── tests/  
├── data/  
│   ├── sessions/  
│   ├── assessments/  
│   └── exports/  
├── docs/  
└── requirements.txt

### **Core Dependencies**

\# AI & ML  
anthropic==0.25.0  
openai==1.14.0  
torch==2.2.0  
transformers==4.38.0  
sentence-transformers==2.5.1

\# Data Processing  
pandas==2.2.0  
numpy==1.26.4  
scipy==1.12.0  
scikit-learn==1.4.0

\# Computer Vision  
opencv-python==4.9.0  
Pillow==10.2.0

\# Web Framework  
fastapi==0.109.0  
uvicorn==0.27.0  
streamlit==1.31.0

\# Database  
sqlalchemy==2.0.25  
redis==5.0.1

\# Analysis & Visualization  
matplotlib==3.8.3  
seaborn==0.13.2  
plotly==5.18.0

\# Testing  
pytest==8.0.0  
pytest-asyncio==0.23.5

### **Database Schema**

\-- Sessions table  
CREATE TABLE sessions (  
    id UUID PRIMARY KEY,  
    participant\_id VARCHAR(50),  
    group\_assignment VARCHAR(20),  
    start\_time TIMESTAMP,  
    end\_time TIMESTAMP,  
    phase VARCHAR(20),  
    completed BOOLEAN DEFAULT FALSE  
);

\-- Interactions table  
CREATE TABLE interactions (  
    id UUID PRIMARY KEY,  
    session\_id UUID REFERENCES sessions(id),  
    timestamp TIMESTAMP,  
    interaction\_type VARCHAR(50),  
    user\_input TEXT,  
    ai\_response TEXT,  
    cognitive\_metrics JSONB,  
    processing\_time INTEGER  
);

\-- Assessments table  
CREATE TABLE assessments (  
    id UUID PRIMARY KEY,  
    session\_id UUID REFERENCES sessions(id),  
    assessment\_type VARCHAR(50),  
    questions JSONB,  
    responses JSONB,  
    scores JSONB,  
    completion\_time INTEGER  
);

\-- Design outputs table  
CREATE TABLE design\_outputs (  
    id UUID PRIMARY KEY,  
    session\_id UUID REFERENCES sessions(id),  
    phase VARCHAR(20),  
    output\_type VARCHAR(50), \-- 'text', 'image', '3d\_model'  
    content TEXT,  
    file\_path VARCHAR(255),  
    expert\_scores JSONB,  
    created\_at TIMESTAMP  
);

### **Key Implementation Classes**

#### **1\. Cognitive Metrics Calculator**

\# src/benchmarking/cognitive\_metrics.py  
class CognitiveMetricsCalculator:  
    def \_\_init\_\_(self):  
        self.nlp\_processor \= SentenceTransformer('all-MiniLM-L6-v2')  
        self.complexity\_analyzer \= TextComplexityAnalyzer()  
          
    def calculate\_cop\_score(self, session\_data):  
        """Calculate Cognitive Offloading Prevention score"""  
        direct\_queries \= self.count\_direct\_queries(session\_data)  
        exploratory\_queries \= self.count\_exploratory\_queries(session\_data)  
        inquiry\_depth \= self.analyze\_inquiry\_depth(session\_data)  
          
        if direct\_queries \+ exploratory\_queries \== 0:  
            return 0  
              
        cop\_ratio \= exploratory\_queries / (direct\_queries \+ exploratory\_queries)  
        weighted\_score \= cop\_ratio \* inquiry\_depth  
          
        return self.normalize\_score(weighted\_score)  
      
    def calculate\_dte\_score(self, session\_data):  
        """Calculate Deep Thinking Engagement score"""  
        response\_complexity \= self.analyze\_response\_complexity(session\_data)  
        reasoning\_chains \= self.extract\_reasoning\_patterns(session\_data)  
        reflection\_markers \= self.count\_reflection\_language(session\_data)  
        pause\_patterns \= self.analyze\_thinking\_pauses(session\_data)  
          
        dte\_score \= (  
            response\_complexity \* 0.3 \+  
            reasoning\_chains \* 0.3 \+  
            reflection\_markers \* 0.2 \+  
            pause\_patterns \* 0.2  
        )  
          
        return self.normalize\_score(dte\_score)

#### **2\. Test Interface Controller**

\# src/ui\_components/test\_interface.py  
class TestInterface:  
    def \_\_init\_\_(self, db\_connection, metrics\_calculator):  
        self.db \= db\_connection  
        self.metrics \= metrics\_calculator  
        self.current\_session \= None  
          
    async def start\_test\_session(self, participant\_id, group\_assignment):  
        """Initialize new test session"""  
        session \= Session(  
            id=uuid4(),  
            participant\_id=participant\_id,  
            group\_assignment=group\_assignment,  
            start\_time=datetime.now()  
        )  
          
        await self.db.save\_session(session)  
        self.current\_session \= session  
          
        return session.id  
      
    async def process\_user\_interaction(self, user\_input, phase):  
        """Process user interaction and calculate real-time metrics"""  
        interaction \= Interaction(  
            session\_id=self.current\_session.id,  
            timestamp=datetime.now(),  
            user\_input=user\_input,  
            phase=phase  
        )  
          
        \# Generate AI response based on group assignment  
        ai\_response \= await self.generate\_ai\_response(  
            user\_input,   
            self.current\_session.group\_assignment,  
            phase  
        )  
          
        interaction.ai\_response \= ai\_response  
          
        \# Calculate real-time cognitive metrics  
        metrics \= self.metrics.calculate\_realtime\_metrics(interaction)  
        interaction.cognitive\_metrics \= metrics  
          
        await self.db.save\_interaction(interaction)  
          
        return ai\_response, metrics

#### **3\. Assessment Engine**

\# src/assessment\_tools/critical\_thinking\_test.py  
class CriticalThinkingAssessment:  
    def \_\_init\_\_(self):  
        self.questions \= self.load\_questions()  
          
    def generate\_adaptive\_test(self, participant\_level='intermediate'):  
        """Generate adaptive critical thinking test"""  
        base\_questions \= self.questions\[participant\_level\]  
        randomized\_questions \= random.sample(base\_questions, 15\)  
          
        return {  
            'test\_id': uuid4(),  
            'questions': randomized\_questions,  
            'time\_limit': 1800,  \# 30 minutes  
            'adaptive': True  
        }  
      
    def score\_responses(self, responses):  
        """Score critical thinking responses"""  
        total\_score \= 0  
        detailed\_scores \= {}  
          
        for question\_id, response in responses.items():  
            question \= self.get\_question(question\_id)  
            score \= self.evaluate\_response(question, response)  
            detailed\_scores\[question\_id\] \= score  
            total\_score \+= score  
              
        return {  
            'total\_score': total\_score,  
            'percentage': (total\_score / len(responses)) \* 100,  
            'detailed\_scores': detailed\_scores,  
            'skill\_areas': self.analyze\_skill\_areas(detailed\_scores)  
        }

### **Data Export & Analysis Scripts**

#### **1\. Benchmarking Results Export**

\# src/utils/export\_handlers.py  
class BenchmarkingExporter:  
    def \_\_init\_\_(self, db\_connection):  
        self.db \= db\_connection  
          
    async def export\_session\_data(self, session\_id, format='json'):  
        """Export complete session data for analysis"""  
        session \= await self.db.get\_session(session\_id)  
        interactions \= await self.db.get\_interactions(session\_id)  
        assessments \= await self.db.get\_assessments(session\_id)  
        design\_outputs \= await self.db.get\_design\_outputs(session\_id)  
          
        export\_data \= {  
            'session': session.to\_dict(),  
            'interactions': \[i.to\_dict() for i in interactions\],  
            'assessments': \[a.to\_dict() for a in assessments\],  
            'design\_outputs': \[d.to\_dict() for d in design\_outputs\],  
            'cognitive\_progression': self.calculate\_progression(interactions),  
            'benchmark\_summary': self.generate\_benchmark\_summary(interactions)  
        }  
          
        if format \== 'json':  
            return json.dumps(export\_data, indent=2)  
        elif format \== 'csv':  
            return self.convert\_to\_csv(export\_data)  
        elif format \== 'xlsx':  
            return self.convert\_to\_excel(export\_data)

#### **2\. Statistical Analysis Pipeline**

\# src/benchmarking/analysis\_pipeline.py  
class StatisticalAnalysisPipeline:  
    def \_\_init\_\_(self):  
        self.stats\_engine \= StatisticsEngine()  
          
    def run\_between\_groups\_analysis(self, group\_a\_data, group\_b\_data, group\_c\_data):  
        """Run ANOVA and post-hoc tests between groups"""  
        results \= {}  
          
        \# Design quality scores  
        design\_scores \= \[  
            group\_a\_data\['design\_quality'\],  
            group\_b\_data\['design\_quality'\],  
            group\_c\_data\['design\_quality'\]  
        \]  
          
        f\_stat, p\_value \= stats.f\_oneway(\*design\_scores)  
        results\['design\_quality\_anova'\] \= {'f\_stat': f\_stat, 'p\_value': p\_value}  
          
        \# Cognitive metrics  
        for metric in \['cop', 'dte', 'se', 'ki', 'lp', 'ma'\]:  
            metric\_scores \= \[  
                group\_a\_data\[metric\],  
                group\_b\_data\[metric\],  
                group\_c\_data\[metric\]  
            \]  
            f\_stat, p\_value \= stats.f\_oneway(\*metric\_scores)  
            results\[f'{metric}\_anova'\] \= {'f\_stat': f\_stat, 'p\_value': p\_value}  
              
        return results  
      
    def analyze\_learning\_progression(self, longitudinal\_data):  
        """Analyze learning progression over time"""  
        progression\_results \= {}  
          
        for participant in longitudinal\_data:  
            participant\_progression \= \[\]  
            for session in participant\['sessions'\]:  
                composite\_score \= session\['cognitive\_metrics'\]\['composite'\]  
                participant\_progression.append(composite\_score)  
              
            \# Calculate learning velocity  
            velocity \= self.calculate\_learning\_velocity(participant\_progression)  
            progression\_results\[participant\['id'\]\] \= {  
                'progression': participant\_progression,  
                'velocity': velocity,  
                'trend': self.analyze\_trend(participant\_progression)  
            }  
              
        return progression\_results

### **Deployment Instructions**

#### **1\. Environment Setup**

\# Create virtual environment  
python \-m venv mentor-btests-env  
source mentor-btests-env/bin/activate  \# On Windows: mentor-btests-env\\Scripts\\activate

\# Install dependencies  
pip install \-r requirements.txt

\# Set up environment variables  
cp .env.example .env  
\# Edit .env with your API keys and database connections

#### **2\. Database Setup**

\# Run database migrations  
python \-m alembic upgrade head

\# Initialize test data  
python scripts/initialize\_test\_data.py

\# Verify setup  
python scripts/verify\_installation.py

#### **3\. Application Launch**

\# Start the API server  
uvicorn src.main:app \--reload \--port 8000

\# Start the Streamlit interface (in another terminal)  
streamlit run src/ui\_components/streamlit\_app.py \--server.port 8501

\# Run the test suite  
pytest tests/ \-v

#### **4\. Configuration Files**

**config.yaml**

\# Application configuration  
app:  
  name: "MENTOR B-Tests"  
  version: "1.0.0"  
  debug: false

\# Database configuration  
database:  
  url: "postgresql://user:password@localhost:5432/mentor\_btests"  
  pool\_size: 10  
  echo: false

\# AI Models configuration  
ai\_models:  
  anthropic:  
    api\_key: "${ANTHROPIC\_API\_KEY}"  
    model: "claude-3-sonnet-20240229"  
    
  openai:  
    api\_key: "${OPENAI\_API\_KEY}"  
    model: "gpt-4-turbo-preview"

\# Benchmarking configuration  
benchmarking:  
  metrics:  
    cop\_weight: 0.20  
    dte\_weight: 0.20  
    se\_weight: 0.15  
    ki\_weight: 0.15  
    lp\_weight: 0.15  
    ma\_weight: 0.15  
    
  thresholds:  
    excellent: 0.85  
    good: 0.70  
    adequate: 0.55  
    needs\_improvement: 0.40

\# Testing configuration  
testing:  
  session\_timeout: 7200  \# 2 hours  
  auto\_save\_interval: 30  \# 30 seconds  
  max\_file\_size: 10485760  \# 10MB  
  allowed\_file\_types: \['.jpg', '.png', '.pdf', '.dwg', '.3dm'\]

### **Validation & Quality Assurance**

#### **1\. Test Validation Protocol**

* **Content Validity**: Expert review by 3 architectural educators and 2 cognitive scientists  
* **Construct Validity**: Factor analysis of cognitive metrics  
* **Reliability Testing**: Test-retest reliability with subset of participants  
* **Inter-rater Reliability**: Multiple expert evaluations of design outputs

#### **2\. Data Quality Checks**

* **Completeness Validation**: Ensure all required data points are collected  
* **Consistency Checks**: Validate logical consistency across test phases  
* **Outlier Detection**: Identify and investigate unusual response patterns  
* **Missing Data Handling**: Implement appropriate imputation strategies

#### **3\. Ethical Considerations**

* **Informed Consent**: Comprehensive consent process for data collection  
* **Privacy Protection**: Data anonymization and secure storage  
* **Participant Wellbeing**: Monitoring for fatigue or stress indicators  
* **Right to Withdraw**: Clear procedures for participant withdrawal

---

## **Expected Outcomes & Success Metrics**

### **Primary Hypotheses**

1. **MENTOR group will show higher COP scores** (reduced cognitive offloading)  
2. **Enhanced DTE scores** in MENTOR group (deeper thinking engagement)  
3. **Improved design quality** with maintained cognitive engagement  
4. **Better knowledge transfer** to novel design problems

### **Success Criteria**

* **Effect size \> 0.8** for cognitive metrics improvement  
* **Statistically significant differences** (p \< 0.05) between MENTOR and control groups  
* **Qualitative evidence** of enhanced thinking processes  
* **Expert validation** of improved design outcomes

### **Long-term Impact Assessment**

* **6-month follow-up** to assess retention of cognitive strategies  
* **Transfer study** with different design problems  
* **Peer evaluation** of design thinking approaches  
* **Self-reported behavioral changes** in design practice

This comprehensive testing suite provides the foundation for rigorous evaluation of the MENTOR tool while maximizing the potential of your cognitive benchmarking system. The integration of real-time metrics, expert evaluation, and longitudinal analysis ensures robust data collection for both immediate assessment and future research applications.