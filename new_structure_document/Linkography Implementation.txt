Linkography Implementation

The current session data structure doesn't contain the specific information needed for linkography analysis. Here's what's missing and what needs to be tracked:


  Current Issue


The linkography analyzer is trying to extract "design moves" from your session data, but the current interaction logging format doesn't capture the necessary details for proper linkography analysis.


  What Linkography Needs


  1. Design Moves as Discrete Units
    - Currently: Our sessions log general interactions (user_message + ai_response)
    - Needed: Each design-related action/thought should be captured as a separate "move"
    - Example: "I want to add a window here" → "Let me consider the lighting" → "This affects the facade composition"
  2. Temporal Sequencing
    - Currently: Interactions have timestamps but aren't fine-grained enough
    - Needed: Precise timestamps for each design decision/action
    - The order and timing between moves is crucial for link formation
  3. Design Phase Identification
    - Currently: The analyzer tries to guess phases from keywords
    - Needed: Explicit tracking of which design phase the user is in:
      - Ideation: Conceptual exploration, brainstorming
      - Visualization: Sketching, form development
      - Materialization: Technical details, construction
  4. Move Type Classification
    - Currently: Not tracked
    - Needed: Each move should be classified as:
      - Analysis (examining existing conditions)
      - Synthesis (combining ideas)
      - Evaluation (judging solutions)
      - Transformation (modifying designs)
      - Reflection (thinking about process)
  5. Modality Information
    - Currently: Only text interactions
    - Needed: Track whether the move came from:
       - Text input
      - Sketch/drawing action
      - Image upload/annotation
      - Voice command


  What Should Be Logged


  Our interaction logger should capture:


  {
      "design_move": {
          "content": "The actual design action/decision",
          "timestamp": precise_timestamp,
          "phase": "ideation|visualization|materialization",
          "move_type": "analysis|synthesis|evaluation|transformation|reflection",
          "modality": "text|sketch|image|voice",
          "context": {
              "previous_move": "what came before",
              "tool_used": "which feature was active",
              "cognitive_load": estimated_value
          }
      }
  }


  Integration Points


  For linkography to work with our MEGA system:


  1. When using GPT-4 Vision + SAM: Each identified architectural element or design decision should be logged as a move
  2. During Multi-Agent conversations: Break down responses into discrete design-relevant statements
  3. In Socratic dialogue: Each question-answer pair might contain multiple moves
  4. Visual annotations: Each markup or selection is a potential move


  Example Session Structure for Linkography


  Instead of:
  User: "I want to design a sustainable building"
  AI: "Let's explore sustainable features..."


  We need:
  Move 1: "Identify sustainability as design goal" (synthesis, ideation)
  Move 2: "Consider solar orientation" (analysis, ideation)
  Move 3: "Sketch south-facing facade" (transformation, visualization)
  Move 4: "Evaluate daylight penetration" (evaluation, visualization)


The linkography engine then analyzes these moves for semantic similarity and creates links between related concepts, revealing the design thinking process.